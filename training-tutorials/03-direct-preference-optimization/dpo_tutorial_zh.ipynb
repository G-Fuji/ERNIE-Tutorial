{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 ERNIE-4.5-0.3B 的大语言模型直接偏好优化 (DPO) 教程\n",
    "\n",
    "## 1. 引言\n",
    "\n",
    "欢迎来到这篇关于使用 ERNIE Kit 对 ERNIE-4.5-0.3B 模型进行直接偏好优化 (Direct Preference Optimization, DPO) 的高级教程！在完成了模型的预训练 (PT) 和监督微调 (SFT) 之后，我们已经拥有了一个具备强大通用语言能力且能够精准遵循指令的 ERNIE 模型。然而，为了让模型生成的回答不仅正确，更能符合人类细致入微的偏好——例如，更具帮助性、更安全无害、风格更受欢迎——我们通常需要进行进一步的“对齐”(Alignment) 训练。本教程将深入探讨如何通过 DPO 技术，让您的 ERNIE 模型实现更高层次的对齐。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 什么是直接偏好优化 (DPO)？\n",
    "\n",
    "**直接偏好优化 (DPO)** 是一种革命性的、相较于传统方法更简单高效的模型对齐技术。在 DPO 出现之前，业界主流的对齐方法是基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback, RLHF)，该方法通常涉及多个复杂且资源密集型的阶段：\n",
    "\n",
    "1.  **监督微调 (SFT)** ：首先，对预训练好的大语言模型进行指令微调，使其学会理解并执行各种指令，这是对齐的第一步，赋予模型基础的“能力”。\n",
    "2.  **训练奖励模型 (Reward Model, RM)** ：接下来，需要收集大量人类对模型多种输出的偏好排序数据。例如，针对同一个问题，模型生成了回答A、B、C，标注者会指出哪个最好、哪个次之、哪个最差。利用这些排序数据，训练一个独立的“奖励模型”，这个模型的任务是模仿人类的判断力，为任何一个给定的回答打分，分数高低代表了人类的偏好程度。\n",
    "3.  **强化学习 (RL)** ：最后，将 SFT 阶段的模型作为“智能体”，在奖励模型的指导下进行强化学习训练（通常使用近端策略优化 PPO 算法）。智能体尝试生成新的回答，奖励模型则对其进行打分，智能体根据得分不断调整自身策略，以期获得更高的奖励，从而使其输出越来越符合人类偏好。\n",
    "\n",
    "尽管 RLHF 被证明非常有效，但其流程冗长、实现复杂、训练过程不稳定，并且需要同时维护和协调多个模型（策略模型、奖励模型、价值模型、参考模型等），对技术和计算资源都提出了很高的要求。\n",
    "\n",
    "**DPO 的核心思想在于其“直接性”——它巧妙地绕过了显式训练奖励模型和复杂的强化学习步骤。** DPO 认为，语言模型本身就可以隐式地代表一个奖励函数。它直接利用成对的人类偏好数据（例如，给定一个提示，回答A比回答B更好），通过设计一个独特的损失函数，直接在语言模型上进行优化。这个损失函数的目标是，让模型提高其生成“偏好”回答的概率，同时降低其生成“不偏好”回答的概率。本质上，DPO 将复杂的强化学习问题转化为了一个更易于处理的、带有特定目标的有监督学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DPO 与 RLHF/SFT 的核心区别对比：**\n",
    "\n",
    "| 特性 | 监督微调 (SFT) | 传统 RLHF (RM+PPO) | 直接偏好优化 (DPO) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **核心目标** | 学习遵循指令，掌握特定任务的解决方法 | 通过奖励模型信号，学习生成符合人类偏好的回答 | 直接基于偏好数据，学习生成符合人类偏好的回答 |\n",
    "| **所需数据** | “指令-理想输出”对 | “指令-多个输出-人类排序” (用于RM)，以及SFT数据 | “指令-偏好输出-非偏好输出”三元组 |\n",
    "| **学习范式** | 有监督学习 | RM:有监督学习; PPO:强化学习 | 有监督学习 (通过特殊损失函数体现偏好) |\n",
    "| **实现复杂性** | 相对简单、直接 | 非常高 (多阶段训练，涉及复杂的RL算法和超参) | 中等 (单阶段策略学习，但数据格式要求特定) |\n",
    "| **所需模型数量** | 1 (策略模型) | 至少2个核心模型 (策略模型, 奖励模型)，常伴有价值模型和参考模型 | 1个核心模型 (策略模型)，通常会用到一个冻结的SFT模型作为参考 |\n",
    "\n",
    "简而言之，如果说 SFT 教会了模型“能做什么”，那么 DPO (和 RLHF) 则是教会模型“如何做得更好、更符合人类期望、更安全可靠”。DPO 为实现这一目标提供了一条更直接、更简洁、更稳定的技术路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 DPO 的应用场景与价值\n",
    "\n",
    "DPO 作为对齐技术的前沿方法，其应用场景和意义极为广泛和深远，我们可以从以下几个维度来理解：\n",
    "\n",
    "|  | 应用领域 | 核心价值与深远意义 |\n",
    "| :---: | :--- | :--- |\n",
    "| 💡 | **提升回答质量 (Helpfulness)** | 通过精细学习人类偏好，模型能生成更准确、信息量更丰富、逻辑更严谨的回答，从“指令执行者”蜕变为真正的“智能助手”。 |\n",
    "| 🛡️ | **增强模型安全 (Harmlessness)** | 通过学习无害/有害内容的边界，有效抑制模型生成不当、有偏见或危险的言论，使其输出更符合社会规范和伦理要求。 |\n",
    "| 🎨 | **精准控制风格 (Style Control)** | 可根据特定偏好数据，引导模型采用特定沟通风格，如客服场景下的礼貌同理、代码场景下的简洁清晰、创作场景下的天马行空。 |\n",
    "| 🚀 | **简化对齐流程 (Efficiency)** | 作为 RLHF 的高效替代方案，DPO 降低了实现高质量对齐的技术门槛和计算成本，适合资源有限的团队和个人快速迭代。 |\n",
    "| 🔬 | **驱动前沿创新 (Innovation)** | DPO 的成功激发了 IPO, KTO, SimPO, ORPO 等一系列新算法的诞生，是理解和跟进大模型技术最新发展趋势的基础。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 为什么选择 ERNIE-4.5-0.3B 进行 DPO？\n",
    "\n",
    "|  | 理由 | 详细说明 |\n",
    "| :---: | :--- | :--- |\n",
    "| 🏆 | **卓越的 SFT 基础** | `ERNIE-4.5-0.3B` 基于文心4.5的先进技术，并经过高质量SFT，具备强大的指令遵循能力和世界知识，为DPO的“画龙点睛”提供了坚实基础。 |\n",
    "| 💻 | **合适的参数规模** | 0.3B的参数量在保持强大能力的同时，对计算资源要求可控，非常适合个人开发者、学术研究者和中小企业进行DPO实验和快速迭代。 |\n",
    "| 🛠️ | **ERNIE Kit 全面支持** | ERNIE Kit 提供了从数据处理到训练执行的端到端DPO解决方案 (`run_dpo.py`)，让开发者无需从头实现复杂算法，即可高效应用DPO。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 本教程的目标与收获\n",
    "\n",
    "本教程专为希望进阶模型对齐的开发者和研究者设计，通过学习，您将获得：\n",
    "\n",
    "|  | 学习成果 | 描述 |\n",
    "| :---: | :--- | :--- |\n",
    "| 🧠 | **深刻理解 DPO 原理** | 透彻理解 DPO 的核心思想、数学原理、应用价值，以及与 SFT/RLHF 的本质区别。 |\n",
    "| 🛠️ | **熟练掌握 ERNIE Kit DPO 全流程** | 完整掌握从环境配置、数据准备到模型训练、评估的全过程。 |\n",
    "| 📊 | **精通 DPO 数据处理** | 学会处理符合 ERNIE Kit 要求的偏好数据格式 (prompt, chosen, rejected)。 |\n",
    "| ⚙️ | **灵活配置与实践** | 学会修改 DPO 配置文件，启动、监控、调试训练任务，并对优化后的模型进行评估。 |\n",
    "\n",
    "现在，就让我们一起踏上这段激动人心的旅程，探索如何通过 DPO 技术，将强大的 ERNIE 模型雕琢成一个更加智能、更加安全、更懂人类心意的顶尖 AI 吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 环境准备\n",
    "\n",
    "进行直接偏好优化 (DPO) 与进行 SFT 类似，都需要一个配置完善的开发环境。这包括安装核心的深度学习框架 PaddlePaddle、专门的 ERNIE Kit 开发套件，并获取其最新的源代码以使用其中最前沿的 DPO 功能。我们假设您已经具备 SFT 的实践基础，因此对环境配置流程会比较熟悉，但本节将提供更详尽的指导，确保万无一失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 安装 PaddlePaddle 和 ERNIE Kit\n",
    "\n",
    "首先，我们需要确保安装了正确版本的 PaddlePaddle 和 ERNIE Kit。为了获得最佳的训练性能和兼容性，强烈推荐使用 GPU 版本的 PaddlePaddle。\n",
    "\n",
    "*如果在AI studio运行的同学不需要运行下方代码块*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 步骤 1: 确保您的 pip 工具是最新版本，这可以避免很多潜在的安装问题\r\n",
    "!python -m pip install --upgrade pip\r\n",
    "\r\n",
    "# 步骤 2: 安装 PaddlePaddle 的 GPU 版本\r\n",
    "# 以下命令适用于 CUDA 11.8 或更高版本的环境。如果您的 CUDA 版本不同，\r\n",
    "# 请务必访问 PaddlePaddle 官方网站获取完全匹配的安装指令：https://www.paddlepaddle.org.cn/install/quick\r\n",
    "!python -m pip install paddlepaddle-gpu -i https://mirror.baidu.com/pypi/simple\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 步骤 3: 安装 ERNIE Kit\r\n",
    "# ERNIE Kit 提供了两种安装方式，我们推荐从源码安装，以确保能使用到最新的 DPO 功能和 bug 修复。\r\n",
    "# 首先，需要从 GitHub 克隆 ERNIE Kit 的仓库。\r\n",
    "# !git clone https://github.com/PaddlePaddle/ERNIE.git\r\n",
    "# 然后，通过 pip 以可编辑模式 (-e) 安装，这样您对代码的任何修改都能立刻生效。\r\n",
    "# 请将下面的路径替换为您实际克隆 ERNIE Kit 的路径。\r\n",
    "!python -m pip install -e  ./ERNIE-develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**安装后验证**：\n",
    "\n",
    "执行以下代码，检查 PaddlePaddle 和 ERNIE Kit 是否成功安装，并确认 GPU 环境是否可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T01:54:48.277759Z",
     "iopub.status.busy": "2025-07-18T01:54:48.277437Z",
     "iopub.status.idle": "2025-07-18T01:54:48.591045Z",
     "shell.execute_reply": "2025-07-18T01:54:48.590312Z",
     "shell.execute_reply.started": "2025-07-18T01:54:48.277740Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddlePaddle Version: 3.1.0\r\n",
      "Running verify PaddlePaddle program ... \r\n",
      "PaddlePaddle works well on 1 GPU.\r\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\r\n",
      "[SUCCESS] PaddlePaddle GPU is available! Found 1 GPU(s).\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0718 09:54:48.481559  9362 pir_interpreter.cc:1524] New Executor is Running ...\r\n",
      "W0718 09:54:48.483019  9362 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.8, Runtime API Version: 12.6\r\n",
      "I0718 09:54:48.483794  9362 pir_interpreter.cc:1547] pir interpreter is running by multi-thread mode ...\r\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "\r\n",
    "print(f\"PaddlePaddle Version: {paddle.__version__}\")\r\n",
    "\r\n",
    "# 运行 PaddlePaddle 的内置检查工具，它会提供详细的环境信息\r\n",
    "try:\r\n",
    "    paddle.utils.run_check()\r\n",
    "    if paddle.device.cuda.device_count() > 0:\r\n",
    "        print(f\"[SUCCESS] PaddlePaddle GPU is available! Found {paddle.device.cuda.device_count()} GPU(s).\")\r\n",
    "    else:\r\n",
    "        print(\"[WARNING] PaddlePaddle GPU check passed, but no GPU found. Training will proceed on CPU, which will be very slow.\")\r\n",
    "except Exception as e:\r\n",
    "    print(f\"[ERROR] PaddlePaddle GPU check failed: {e}\")\r\n",
    "    print(\"If you intended to use GPU, please carefully check your CUDA environment, NVIDIA driver, and the PaddlePaddle version you installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 下载 ERNIE Kit 仓库代码\n",
    "\n",
    "ERNIE Kit 的核心 DPO 脚本 (`examples/long_text/run_dpo.py` 或类似路径) 和所有相关的模型配置文件都位于其 GitHub 仓库中。即使您通过 pip 安装了 ERNIE Kit，我们也强烈建议您克隆一份完整的代码仓库，因为所有实验和训练都将围绕这份代码进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T01:56:54.494064Z",
     "iopub.status.busy": "2025-07-18T01:56:54.493729Z",
     "iopub.status.idle": "2025-07-18T01:56:54.498103Z",
     "shell.execute_reply": "2025-07-18T01:56:54.497595Z",
     "shell.execute_reply.started": "2025-07-18T01:56:54.494044Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/ERNIE-develop\r\n"
     ]
    }
   ],
   "source": [
    "# 如果您在之前的步骤中尚未克隆，请执行此命令\r\n",
    "# git clone https://github.com/PaddlePaddle/ERNIE.git\r\n",
    "\r\n",
    "# 克隆后，您会得到一个名为 ERNIE-develop (或 ERNIE) 的文件夹\r\n",
    "# 我们后续所有命令的执行，都将假设您位于这个文件夹的根目录下\r\n",
    "# 例如: cd ./ERNIE-develop\r\n",
    "%cd ./ERNIE-develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DPO 数据准备\n",
    "\n",
    "与 SFT 阶段使用“指令-理想输出”对的数据格式截然不同，直接偏好优化 (DPO) 的精髓在于利用人类对模型不同输出的**偏好判断**。因此，DPO 的训练数据直接、精确地体现了这种成对的偏好选择，这也是其“直接”优化的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 DPO 数据的特点与格式\n",
    "\n",
    "DPO 训练数据的基本单元是一个三元组，包含：**“提示 (Prompt)”**、**“被偏好的回答 (Chosen Response)”** 和 **“不被偏好的回答 (Rejected Response)”**。这样的数据结构明确地告诉模型：在接收到同一个提示时，哪一种类型的回答更符合人类的期望，哪一种则不然。\n",
    "\n",
    "**ERNIE Kit 支持的 DPO 数据格式：**\n",
    "\n",
    "根据 ERNIE Kit 的设计，DPO 脚本期望的数据是 **JSON Lines (jsonl) 文件**。这是一种非常流行的格式，文件中的每一行都是一个独立的、合法的 JSON 对象，代表一个偏好样本。一个典型的样本结构如下所示：\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"请向我解释一下什么是人工智能？\",\n",
    "    \"chosen\": \"人工智能（AI）是计算机科学的一个分支，致力于创造能够模仿、扩展和超越人类智能的机器。它涵盖了机器学习、自然语言处理、计算机视觉等多个领域，旨在让计算机能够像人一样学习、推理、感知和解决问题。例如，我们手机上的语音助手、自动驾驶汽车以及推荐系统，都是人工智能在日常生活中的应用。\",\n",
    "    \"rejected\": \"人工智能就是机器人。\"\n",
    "}\n",
    "```\n",
    "\n",
    "**字段深度解析**：\n",
    "\n",
    "*   `prompt`: `(string)`，必需。这是输入给模型的用户指令、问题或前文对话。它是偏好判断的上下文，是所有比较的起点。\n",
    "*   `chosen`: `(string)`，必需。这是针对 `prompt` 的两个回答中，被人类标注为**更偏好**、**质量更高**的那个。它代表了模型应该学习和模仿的方向。\n",
    "*   `rejected`: `(string)`，必需。这是针对 `prompt` 的两个回答中，被人类标注为**相对不偏好**、**质量较低**的那个。它代表了模型应该避免和抑制的方向。\n",
    "\n",
    "**为什么是这种“提示-选择-拒绝”的三元组格式？**\n",
    "\n",
    "DPO 的损失函数需要直接比较模型对于 `chosen` 回答和 `rejected` 回答分别赋予的概率（或隐式的偏好得分）。因此，每一条训练数据都必须清晰地提供这一对成对比较的 (chosen, rejected) 输出，以及它们所共同对应的输入提示 `prompt`。这种结构使得损失函数可以直接计算偏好差异，并据此更新模型参数。\n",
    "\n",
    "**高质量DPO数据的关键要素：**\n",
    "\n",
    "构建或选用一个高质量的DPO数据集是成功的关键。一个好的数据集应具备以下特点：\n",
    "\n",
    "*   **明确且有意义的偏好差异**：Chosen 和 Rejected 回答之间应该存在清晰、可辨别的质量差异。这种差异应该精确反映你希望模型学习到的特定偏好，例如：\n",
    "    *   **事实准确性**：Chosen 回答更真实，Rejected 回答包含事实错误。\n",
    "    *   **无害性**：Chosen 回答安全、礼貌，Rejected 回答包含有害、冒犯性或有偏见的内容。\n",
    "    *   **帮助性与详细程度**：Chosen 回答更全面、更有帮助，Rejected 回答过于简略或答非所问。\n",
    "    *   **指令遵循**：Chosen 回答严格遵循了指令中的所有约束（如格式、角色），Rejected 回答则没有。\n",
    "    *   **风格偏好**：Chosen 回答符合特定的风格（如专业、幽默），Rejected 回答则不符合。\n",
    "*   **提示的多样性**：`prompt` 应该广泛覆盖模型在实际应用中可能遇到的各种指令类型、主题和难度，以确保模型学到的偏好具有良好的泛化能力。\n",
    "*   **回答的真实性与挑战性**：`chosen` 和 `rejected` 的回答最好是模型在SFT阶段的真实输出，或者是经过精心构造、能够代表模型典型错误或潜在改进方向的例子。两者差异不应过于极端（例如一个完美答案 vs 一个完全无关的答案），适度的难度更能激发模型的学习潜力。\n",
    "*   **一致的标注标准**：如果数据由多人标注，必须建立并遵循一套统一、明确的偏好判断标准，以保证数据质量的可靠性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用 ERNIE Kit 自带的示例 DPO 数据集\n",
    "\n",
    "为了方便用户快速上手，ERNIE Kit 项目在 `examples/data/` 目录下已经内置了一份小型的 DPO 示例数据集。这份数据可以直接用于运行教程中的 DPO 训练脚本，无需从网络下载。\n",
    "\n",
    "**数据集文件：**\n",
    "\n",
    "*   `examples/data/dpo-train.jsonl`: 用于 DPO 训练的偏好数据集。\n",
    "*   `examples/data/dpo-eval.jsonl`: 用于在训练过程中评估模型性能的偏好数据集。\n",
    "\n",
    "这两个文件都遵循我们之前讨论的 JSON Lines 格式，包含了 `prompt`, `chosen`, 和 `rejected` 字段。\n",
    "\n",
    "**查看数据样本：**\n",
    "\n",
    "假设您当前的工作目录是 ERNIE Kit 的根目录，您可以使用以下代码来查看训练数据的内容，直观地了解其结构和质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:04:15.890827Z",
     "iopub.status.busy": "2025-07-18T03:04:15.890259Z",
     "iopub.status.idle": "2025-07-18T03:04:15.897453Z",
     "shell.execute_reply": "2025-07-18T03:04:15.896826Z",
     "shell.execute_reply.started": "2025-07-18T03:04:15.890798Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 查看自带的 DPO 训练数据样本: ./examples/data/dpo-train.jsonl ---\r\n",
      "\r\n",
      "--- 样本 1 ---\r\n",
      "Prompt: 请写一份关于支撑航空和飞行的物理基本原理的全面解释，包括升力、推力、重量和阻力等主题，以及伯努利定律、牛顿运动定律和空气性质等关键科学概念。使用清晰简洁的语言，并提供详细的例子和插图，以帮助读者理解这些概念。考虑塑造航空业的历史和技术发展，以及物理学在推动飞行能力方面发挥的作用。\r\n",
      "\r\n",
      "Chosen: ['飞行物理学：航空原理全面指南\\n\\n**介绍**\\n\\n理解支撑航空和飞行的物理原理对于理解飞机在空中飞行的机制至关重要。在本指南中，我们将探讨飞行的基本因素，包括升力、推力、重量和阻力，并探索关键的科学概念，如伯努利定理、牛顿运动定律和空气的特性。通过使用清晰、简明的语言，并提供详细的例子和插图，本指南旨在使航空和飞行的概念对读者易于理解和吸引。\\n\\n**1. 飞行的四个力量**\\n\\n首先，认识到在飞行过程中作用于飞机的四个关键力量是很重要的：升力、推力、重量和阻力。这些力量之间的微妙平衡是飞机保持空中飞行和维持其所需飞行路径所必需的。\\n\\n1.1. 升力：升力是抵抗飞机重量并支撑其在空中的力量。升力是通过操纵飞机机翼周围的气压分布来产生的。机翼的形状，称为翼型，是为了优化升力而设计的。升力主要是通过伯努利定理和牛顿运动定律来解释的，我们稍后将详细探讨。\\n\\n1.2. 推力：推力是由飞机发动机产生的向前的力量，推动飞机穿过空气。喷气发动机、螺旋桨发动机甚至火箭发动机都可以提供必要的推力来克服阻力，这是另一个控制飞行的关键因素。\\n\\n1.3. 重量：重量是由于飞机质量所产生的重力。它向下作用并抵抗升力。为了保持水平飞行，飞机必须产生足够的升力来抵消其重量。\\n\\n1.4. 阻力：阻力是阻碍飞机在空气中运动的力量。它主要由两种类型组成：形状阻力，由于飞机的形状而产生，以及皮肤摩擦阻力，由于空气与飞机表面之间的摩擦而产生。最小化阻力对于实现高效飞行至关重要。\\n\\n**2. 伯努利定理和升力**\\n\\n伯努利定理是流体动力学中的基本概念，对于理解升力至关重要。该定理指出，当流体（在这种情况下是空气）的速度增加时，其压力降低。飞机机翼采用翼型形状，使空气在机翼上方移动得比下方快。这导致上表面的压力降低，下表面的压力升高，产生升力。\\n\\n**3. 牛顿运动定律和飞行**\\n\\n艾萨克·牛顿爵士的运动定律在解释作用于飞机的力量方面也起着至关重要的作用。\\n\\n3.1. 牛顿第一定律（惯性）：静止的物体将保持静止，运动的物体将继续以恒定的速度运动，除非受到外力的作用。当分析升力产生时，这个定律适用于飞行，其中空气流动方向改变，导致向上的力（升力）。\\n\\n3.2. 牛顿第二定律（力=质量×加速度）：物体的加速度与施加的力成正比，与其质量成反比。在\r\n",
      "\r\n",
      "Rejected: ['基础物理原理支撑着航空和飞行，包括升力、推力、重量和阻力。这些原理受到伯努利定律、牛顿运动定律以及空气特性等多种科学概念的影响。要实现升力，飞机需要在其上方有一个比地球引力向下的力更大的向上力。这种向上力是通过机翼与空气相互作用而产生的。推力则由连接到飞机的发动机产生，造成一个向前的力，使飞机在空中移动。而重量则是一种将物体向地球中心拉的力。为了实现飞行，飞机的向上力必须大于引力的向下力。\\n\\n飞机的飞行能力通过技术创新、研究和实验的结合得到了提高。从19世纪末的莱特飞行器到21世纪的波音737 MAX，飞机的演变是由于增加速度、容量和安全性的愿望推动的。喷气发动机、涡扇发动机以及复合材料的引入使飞机的速度更快、燃油更节约，也更安全。\\n\\n此外，物理学在航空器设计中发挥了关键作用，通过空气动力学、流体力学和结构设计的进步。这些概念促进了空气动力学机翼表面、节能喷气发动机和轻量、坚固的飞机结构的发展。如今，研究正在进行中，以进一步提高飞机的性能和效率，旨在创造出速度最快、最可持续且最安全的交通方式。']\r\n",
      "--------------------\r\n",
      "\r\n",
      "--- 样本 2 ---\r\n",
      "Prompt: Part 1. Definition\r\n",
      "在这个任务中，你会得到一些推特帖子。你的任务是标记帖子中用户表达的情感，如悲伤、喜悦、爱、愤怒、恐惧或惊讶。\r\n",
      "Part 2. Example\r\n",
      "im feeling quite sad and sorry for myself but ill snap out of it soon\r\n",
      "答案：悲伤\r\n",
      "解释：帖子中的情感是悲伤，因为用户提到“感觉相当悲伤和为自己感到抱歉”。因此，标签是“悲伤”。\r\n",
      "Part 3. Exercise\r\n",
      "im feeling morose as i tend to do when im awake and writing here at almost am\r\n",
      "答案：\r\n",
      "\r\n",
      "Chosen: ['悲伤\\n解释：这篇帖子的情绪是悲伤，因为用户提到“感到忧郁”，这是悲伤或不快乐的同义词。他们在清醒的时候写作并表达这种情绪，这表明了一种消极的情绪状态。因此，标签是“悲伤”。']\r\n",
      "\r\n",
      "Rejected: ['悲伤\\n\\n解释：这篇帖子的情绪是悲伤，因为用户说自己“感到郁闷”和“几乎凌晨还清醒着在这里写”，这表明他们正在经历悲伤。']\r\n",
      "--------------------\r\n",
      "\r\n",
      "--- 样本 3 ---\r\n",
      "Prompt: 你是一个AI助理。提供详细的答案，使用户不需要在外部搜索就能理解答案。\r\n",
      "\r\n",
      "Chosen: ['标题：爱与欺骗的艺术\\n\\n类型：浪漫惊悚片\\n\\n剧情：\\n\\n哈里（一个有才华且内向的画家）和樱桃（一个有抱负的记者），他自由奔放的女友，搬进了城市波西米亚区的阁楼。他们共同踏上了实现梦想并更加亲近彼此的旅程，却不知道会面临考验他们关系和忠诚的挑战。\\n\\n第一幕：\\n\\n电影以展示这对情侣愉快的日常生活开始。哈里从樱桃的激动人心的故事和经历中获得绘画灵感，他们的关系迅速蓬勃发展。另一方面，樱桃开始在一家当地杂志上作为记者展开职业生涯，忙于多个任务。\\n\\n第二幕：\\n\\n有一天，当樱桃被指派调查有影响力的艺术大亨亚力克时，哈里对向他展示自己的画作的机会产生兴趣。哈里说服樱桃帮助他引起亚力克的注意，以在艺术界获得认可。樱桃在记者道德和爱情之间犹豫不决地同意了。\\n\\n随着这对情侣越来越深入亚力克的艺术和财富世界，哈里变得更加雄心勃勃和疏远，让樱桃感到孤独和不被重视。与此同时，樱桃发现亚力克涉足非法艺术伪造计划，并打算利用哈里的才华谋取自己的利益。\\n\\n第三幕：\\n\\n悲痛欲绝的樱桃向哈里揭示了她的发现，但他感到矛盾，因为亚力克承诺给他他一直渴望的名声和认可。樱桃在对哈里的忠诚和记者职业之间犹豫不决。她决定结束这段关系，但跟随直觉开始收集针对亚力克的证据。\\n\\n在最后一幕中，哈里看清了亚力克的真实意图，意识到樱桃在他生活中的重要性。在樱桃的帮助下，哈里最终智胜亚力克，导致他被逮捕。\\n\\n电影以哈里和樱桃和好并重新点燃他们的关系结束。哈里真正的才华在艺术界得到认可，樱桃的记者才能也受到赞扬。电影以这对情侣认识到爱和诚信的重要性而结束，证明真正的幸福永远不会被欺诈行为或满足自私欲望所妥协。']\r\n",
      "\r\n",
      "Rejected: ['当然，我很乐意帮你围绕哈里和他的女朋友樱桃构建一个电影情节！以下是一个可能的故事情节：\\n\\n哈里是一位成功的年轻专业人士，刚刚在城市里找到了他梦寐以求的工作。兴奋地开始他的新生活，他着手寻找一个住所。当他在网上浏览房源时，他看到了一个迷人的小公寓引起了他的注意。唯一的问题是它有点超出他的预算，但他决定冒险申请。\\n\\n令他惊讶的是，他被批准了这个公寓，并在同一天搬了进去。当他打开箱子开始收拾时，他遇到了他的新邻居樱桃，她是一个自由奔放的艺术家。他们展开了对话，哈里立刻被她活泼的个性和欢快的笑声所吸引。他们交换了电话号码，很快他们就开始互发短信。\\n\\n在他意识到之前，哈里和樱桃开始花费所有的空闲时间在一起，探索城市，尝试新餐厅，享受彼此的陪伴。他们都深深爱上了对方，但他们也都不愿意给他们的关系贴标签。他们满足于看看事情会发展到哪里，享受当下的时刻。\\n\\n随着时间的推移，哈里和樱桃的关系加深，他们开始谈论他们未来的计划。他们都有着宏伟的梦想和抱负，他们决心支持彼此。他们开始计划一起旅行，他们都对看到他们的旅程将带他们去哪里感到兴奋。\\n\\n但就在事情开始好转的时候，樱桃的过去却回来困扰她。她一直对哈里隐瞒着一个秘密，她不确定是否准备向他透露。她害怕失去他，也不知道他发现真相后会有什么反应。\\n\\n随着他们之间的紧张关系加剧，哈里开始注意到有些事情不对劲。他能感觉到樱桃对他隐瞒了一些事情，他决心找出真相。他质问她，她最终坦白了她的过去。\\n\\n原来，樱桃和她的家庭有着不愉快的历史。她的父母曾经虐待和忽视她，自从离开家后她一直在努力接受过去。她一直试图摆脱并开始新的生活，但她童年的记忆一直在困扰着她。\\n\\n哈里对樱桃的启示感到震惊和悲伤，但他也决心在这个困难时期支持她。他倾听她，拥抱她，并告诉她无论发生什么他都会在她身边。他让她放心，他爱她的本质，他不会离开。\\n\\n当他们一起克服问题时，哈里和樱桃对彼此的爱只会变得更加坚固。他们意识到彼此是对方的支柱，只要他们在一起，他们就可以面对任何事情。他们决定将他们的关系提升到新的水平，并开始规划他们的未来。\\n\\n电影以哈里和樱桃坐在沙发上，手牵手，望着城市天际线的温馨场景结束。他们都微笑着，知道他们在彼此身上找到了他们的幸福结局。']\r\n",
      "--------------------\r\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import json\r\n",
    "\r\n",
    "# ERNIE Kit 自带的 DPO 训练数据路径\r\n",
    "dpo_train_file = \"./examples/data/dpo-train.jsonl\"\r\n",
    "\r\n",
    "if os.path.exists(dpo_train_file):\r\n",
    "    print(f\"--- 查看自带的 DPO 训练数据样本: {dpo_train_file} ---\")\r\n",
    "    with open(dpo_train_file, 'r', encoding='utf-8') as f:\r\n",
    "        for i, line in enumerate(f):\r\n",
    "            if i < 3: # 只看前3个样本\r\n",
    "                sample = json.loads(line.strip())\r\n",
    "                print(f\"\\n--- 样本 {i+1} ---\")\r\n",
    "                # 根据文件格式，'src' 是一个列表，我们取第一个元素作为 prompt\r\n",
    "                print(f\"Prompt: {sample['src'][0]}\")\r\n",
    "                # 'response' 是一个包含两个回答的列表，'sort' 字段的 [1, 0] 表示第一个回答是 chosen，第二个是 rejected\r\n",
    "                print(f\"\\nChosen: {sample['response'][0]}\")\r\n",
    "                print(f\"\\nRejected: {sample['response'][1]}\")\r\n",
    "                print(\"-\" * 20)\r\n",
    "            else:\r\n",
    "                break\r\n",
    "else:\r\n",
    "    print(f\"[ERROR] 未找到 DPO 训练数据文件: {dpo_train_file}。请确认您的工作目录是否为 ERNIE Kit 根目录，并且该文件确实存在。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过运行以上代码，您可以清晰地看到 DPO 数据的具体形式，这对于理解后续的训练过程至关重要。现在，数据已经准备就绪，我们可以继续探索 DPO 训练的核心环节了。\n",
    "\n",
    "通过打印出的样本，您可以清晰地看到每个数据点是如何由 `prompt`、`chosen` 和 `rejected` 构成的，这有助于您更好地理解 DPO 的训练机制，并为您将来构建自己的数据集提供参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 数据预处理与Tokenization核心要点\n",
    "\n",
    "与 SFT 和预训练阶段类似，DPO 训练前也必须将文本数据转换为模型能够理解的数字序列（Token IDs）。ERNIE Kit 的 `run_dpo.py` 脚本在后台会自动处理复杂的预处理逻辑，但理解其核心机制至关重要：\n",
    "\n",
    "1.  **Tokenization**：脚本会使用与您的基础模型（ERNIE-4.5-0.3B，通常是SFT后的版本）完全匹配的 Tokenizer。对于每个样本，`prompt`、`chosen` 和 `rejected` 这三段文本都会被独立地进行 Tokenize。\n",
    "\n",
    "2.  **序列构建与损失计算**：\n",
    "    *   DPO 的核心是比较模型对 `chosen` 和 `rejected` 回答的偏好程度。这通常通过计算模型在这两个回答序列上的**对数概率 (log-probabilities)** 来实现。\n",
    "    *   为了计算这些概率，脚本会构建两个输入序列：一个是 `prompt` 与 `chosen` 的拼接 (`prompt + chosen`)，另一个是 `prompt` 与 `rejected` 的拼接 (`prompt + rejected`)。\n",
    "    *   **标签 (Labels) 的智能处理**：在计算损失时，我们只关心模型对**回答部分**的预测能力，而 `prompt` 部分只是上下文，不应计入损失。这是通过在 `labels` 中对非回答部分的 token 进行**掩码 (masking)** 来实现的（通常设置为-100）。\n",
    "        *   对于 `prompt + chosen` 序列，其对应的 `labels` 中，`prompt` 部分被 mask 掉，只有 `chosen` 部分是真实的 token ID。\n",
    "        *   同理，对于 `prompt + rejected` 序列，其 `labels` 中，`prompt` 部分被 mask 掉，只有 `rejected` 部分是真实的 token ID。\n",
    "    *   **DPO 损失函数**：最终，DPO 损失函数会利用模型在这两组序列上计算出的对数概率之差，来引导模型参数的更新，目标是让 `chosen` 回答的概率相对 `rejected` 回答的概率更高。\n",
    "\n",
    "3.  **长度控制：截断与填充**：\n",
    "    *   实际应用中，文本长度各不相同。脚本会使用 `max_length` (或类似的) 参数来统一处理。过长的序列（`prompt + response`）会被截断，不足的则会用 padding token 填充到相同长度，以便进行批处理训练。\n",
    "\n",
    "4.  **参考模型 (Reference Model) 的作用**：\n",
    "    *   标准的 DPO 算法会引入一个“参考模型”，它通常就是 SFT 阶段结束后的那个模型，并且在 DPO 训练过程中其参数**完全冻结，不进行更新**。\n",
    "    *   DPO 损失函数在计算 `chosen` 和 `rejected` 回答的对数概率时，会减去参考模型在相同回答上的对数概率。这可以被理解为一种**正则化**手段，目的是防止策略模型（即我们正在训练的模型）为了过度迎合偏好数据而偏离其在 SFT 阶段学到的通用语言能力太远，从而导致“能力灾难性遗忘”。它确保模型在学习“偏好”的同时，不会丢掉“能说人话”的基础。\n",
    "\n",
    "幸运的是，作为使用者，我们通常只需要在配置文件中正确指定数据集路径、SFT模型路径、以及相关的序列长度和 DPO 核心超参数（如 `beta`）。ERNIE Kit 的 `run_dpo.py` 脚本会为我们妥善处理上述所有复杂的 Tokenization、序列拼接、标签掩码和损失计算细节。\n",
    "\n",
    "至此，我们已经拥有了高质量、格式正确的偏好数据集，并深刻理解了其背后的处理逻辑。这是 DPO 训练成功的坚实基石。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ERNIE Kit DPO 数据格式\n",
    "\n",
    "根据我们对 `ERNIE-develop` 项目的分析，ERNIE Kit 的DPO脚本期望的数据是 **JSON Lines (jsonl) 文件**，其中每一行是一个JSON对象，代表一个偏好样本。一个典型的样本结构如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"system\": \"系统提示词\",\n",
    "    \"src\": [\"用户问题1\", \"用户问题2\"],\n",
    "    \"tgt\": [\"模型在遇到用户问题1时的回答\"],\n",
    "    \"response\": [\n",
    "        [\"针对用户问题2的偏好回答\"],\n",
    "        [\"针对用户问题2的不偏好回答\"]\n",
    "    ],\n",
    "    \"sort\": [1, 0]\n",
    "}\n",
    "```\n",
    "\n",
    "**字段解释**：\n",
    "\n",
    "*   `system`: (可选) `str`。系统级的提示词，用于设定模型的角色或行为准则。\n",
    "*   `src`: `List[str]`。用户的输入序列，在多轮对话中，这会是一个包含多条用户输入的列表。\n",
    "*   `tgt`: `List[str]`。模型在前几轮对话中的回答。\n",
    "*   `response`: `List[List[str]]`。一个包含两个列表的列表。第一个列表是**chosen response**（偏好回答），第二个列表是**rejected response**（非偏好回答）。\n",
    "*   `sort`: `List[int]`。一个包含两个整数的列表，`[1, 0]` 表示 `response` 中第一个回答是偏好的，第二个是不偏好的。\n",
    "\n",
    "ERNIE Kit 提供了一个示例数据集，我们可以参考其格式来准备自己的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 使用示例 DPO 数据集\n",
    "\n",
    "ERNIE Kit 在 `examples/data/` 目录下提供了 `dpo-train.jsonl` 和 `dpo-eval.jsonl` 作为示例。我们可以直接使用这些数据来进行DPO训练。\n",
    "\n",
    "**查看数据样本：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:06:19.154276Z",
     "iopub.status.busy": "2025-07-18T03:06:19.153922Z",
     "iopub.status.idle": "2025-07-18T03:06:19.159143Z",
     "shell.execute_reply": "2025-07-18T03:06:19.158601Z",
     "shell.execute_reply.started": "2025-07-18T03:06:19.154255Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"src\": [\r\n",
      "        \"请写一份关于支撑航空和飞行的物理基本原理的全面解释，包括升力、推力、重量和阻力等主题，以及伯努利定律、牛顿运动定律和空气性质等关键科学概念。使用清晰简洁的语言，并提供详细的例子和插图，以帮助读者理解这些概念。考虑塑造航空业的历史和技术发展，以及物理学在推动飞行能力方面发挥的作用。\"\r\n",
      "    ],\r\n",
      "    \"tgt\": [],\r\n",
      "    \"response\": [\r\n",
      "        [\r\n",
      "            \"飞行物理学：航空原理全面指南\\n\\n**介绍**\\n\\n理解支撑航空和飞行的物理原理对于理解飞机在空中飞行的机制至关重要。在本指南中，我们将探讨飞行的基本因素，包括升力、推力、重量和阻力，并探索关键的科学概念，如伯努利定理、牛顿运动定律和空气的特性。通过使用清晰、简明的语言，并提供详细的例子和插图，本指南旨在使航空和飞行的概念对读者易于理解和吸引。\\n\\n**1. 飞行的四个力量**\\n\\n首先，认识到在飞行过程中作用于飞机的四个关键力量是很重要的：升力、推力、重量和阻力。这些力量之间的微妙平衡是飞机保持空中飞行和维持其所需飞行路径所必需的。\\n\\n1.1. 升力：升力是抵抗飞机重量并支撑其在空中的力量。升力是通过操纵飞机机翼周围的气压分布来产生的。机翼的形状，称为翼型，是为了优化升力而设计的。升力主要是通过伯努利定理和牛顿运动定律来解释的，我们稍后将详细探讨。\\n\\n1.2. 推力：推力是由飞机发动机产生的向前的力量，推动飞机穿过空气。喷气发动机、螺旋桨发动机甚至火箭发动机都可以提供必要的推力来克服阻力，这是另一个控制飞行的关键因素。\\n\\n1.3. 重量：重量是由于飞机质量所产生的重力。它向下作用并抵抗升力。为了保持水平飞行，飞机必须产生足够的升力来抵消其重量。\\n\\n1.4. 阻力：阻力是阻碍飞机在空气中运动的力量。它主要由两种类型组成：形状阻力，由于飞机的形状而产生，以及皮肤摩擦阻力，由于空气与飞机表面之间的摩擦而产生。最小化阻力对于实现高效飞行至关重要。\\n\\n**2. 伯努利定理和升力**\\n\\n伯努利定理是流体动力学中的基本概念，对于理解升力至关重要。该定理指出，当流体（在这种情况下是空气）的速度增加时，其压力降低。飞机机翼采用翼型形状，使空气在机翼上方移动得比下方快。这导致上表面的压力降低，下表面的压力升高，产生升力。\\n\\n**3. 牛顿运动定律和飞行**\\n\\n艾萨克·牛顿爵士的运动定律在解释作用于飞机的力量方面也起着至关重要的作用。\\n\\n3.1. 牛顿第一定律（惯性）：静止的物体将保持静止，运动的物体将继续以恒定的速度运动，除非受到外力的作用。当分析升力产生时，这个定律适用于飞行，其中空气流动方向改变，导致向上的力（升力）。\\n\\n3.2. 牛顿第二定律（力=质量×加速度）：物体的加速度与施加的力成正比，与其质量成反\r\n",
      "        ],\r\n",
      "        [\r\n",
      "            \"基础物理原理支撑着航空和飞行，包括升力、推力、重量和阻力。这些原理受到伯努利定律、牛顿运动定律以及空气特性等多种科学概念的影响。要实现升力，飞机需要在其上方有一个比地球引力向下的力更大的向上力。这种向上力是通过机翼与空气相互作用而产生的。推力则由连接到飞机的发动机产生，造成一个向前的力，使飞机在空中移动。而重量则是一种将物体向地球中心拉的力。为了实现飞行，飞机的向上力必须大于引力的向下力。\\n\\n飞机的飞行能力通过技术创新、研究和实验的结合得到了提高。从19世纪末的莱特飞行器到21世纪的波音737 MAX，飞机的演变是由于增加速度、容量和安全性的愿望推动的。喷气发动机、涡扇发动机以及复合材料的引入使飞机的速度更快、燃油更节约，也更安全。\\n\\n此外，物理学在航空器设计中发挥了关键作用，通过空气动力学、流体力学和结构设计的进步。这些概念促进了空气动力学机翼表面、节能喷气发动机和轻量、坚固的飞机结构的发展。如今，研究正在进行中，以进一步提高飞机的性能和效率，旨在创造出速度最快、最可持续且最安全的交通方式。\"\r\n",
      "        ]\r\n",
      "    ],\r\n",
      "    \"sort\": [\r\n",
      "        1,\r\n",
      "        0\r\n",
      "    ]\r\n",
      "}\r\n",
      "{\r\n",
      "    \"src\": [\r\n",
      "        \"Part 1. Definition\\n在这个任务中，你会得到一些推特帖子。你的任务是标记帖子中用户表达的情感，如悲伤、喜悦、爱、愤怒、恐惧或惊讶。\\nPart 2. Example\\nim feeling quite sad and sorry for myself but ill snap out of it soon\\n答案：悲伤\\n解释：帖子中的情感是悲伤，因为用户提到“感觉相当悲伤和为自己感到抱歉”。因此，标签是“悲伤”。\\nPart 3. Exercise\\nim feeling morose as i tend to do when im awake and writing here at almost am\\n答案：\"\r\n",
      "    ],\r\n",
      "    \"tgt\": [],\r\n",
      "    \"response\": [\r\n",
      "        [\r\n",
      "            \"悲伤\\n解释：这篇帖子的情绪是悲伤，因为用户提到“感到忧郁”，这是悲伤或不快乐的同义词。他们在清醒的时候写作并表达这种情绪，这表明了一种消极的情绪状态。因此，标签是“悲伤”。\"\r\n",
      "        ],\r\n",
      "        [\r\n",
      "            \"悲伤\\n\\n解释：这篇帖子的情绪是悲伤，因为用户说自己“感到郁闷”和“几乎凌晨还清醒着在这里写”，这表明他们正在经历悲伤。\"\r\n",
      "        ]\r\n",
      "    ],\r\n",
      "    \"sort\": [\r\n",
      "        1,\r\n",
      "        0\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "import json\r\n",
    "\r\n",
    "# 假设您在 ERNIE 项目的根目录下\r\n",
    "dpo_train_file = \"./examples/data/dpo-train.jsonl\"\r\n",
    "\r\n",
    "with open(dpo_train_file, 'r', encoding='utf-8') as f:\r\n",
    "    for i, line in enumerate(f):\r\n",
    "        if i < 2:\r\n",
    "            sample = json.loads(line.strip())\r\n",
    "            print(json.dumps(sample, indent=4, ensure_ascii=False))\r\n",
    "        else:\r\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这将打印出数据集中的前两个样本，帮助我们理解其具体结构和内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 开始 DPO 训练\n",
    "\n",
    "现在，我们已经准备好了环境、模型和数据，是时候深入 DPO 训练的核心了。本章将首先从原理层面深入剖析 DPO，然后详细指导如何利用 ERNIE Kit 的强大功能来配置和启动训练任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 DPO 原理深度剖析\n",
    "\n",
    "理解 DPO 的工作原理，有助于我们更好地调整参数、分析结果。DPO 的核心思想是**将人类偏好直接转化为一个可供优化的损失函数**，从而绕过了 RLHF 中复杂的奖励模型训练和强化学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 从偏好到策略：DPO 的核心思想\n",
    "\n",
    "1.  **隐式奖励模型**：DPO 巧妙地假设，存在一个未知的、理想的奖励模型 `r(x, y)`，它可以为任何一个“提示-回答”对 `(x, y)` 打分。人类的偏好数据（即哪个回答更好）反映了这个奖励模型的结果。具体来说，如果对于提示 `x`，回答 `y_w` (chosen) 优于 `y_l` (rejected)，那么我们认为 `r(x, y_w) > r(x, y_l)`。\n",
    "\n",
    "2.  **Bradley-Terry 模型**：DPO 借用了 Bradley-Terry 模型来对这种偏好概率进行建模。该模型指出，人类偏好 `y_w` 超过 `y_l` 的概率 `p*(y_w > y_l | x)` 可以表示为：\n",
    "\n",
    "    $\\ p^*(y_w > y_l | x) = \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} = \\sigma(r(x, y_w) - r(x, y_l)) \\$ \n",
    "   \n",
    "\n",
    "    其中，`σ` 是 Sigmoid 函数。这个公式直观地表示：两个回答的奖励分数之差越大，人类偏好其中一个的概率就越接近1。\n",
    "\n",
    "3.  **连接语言模型与奖励**：接下来是 DPO 最为精妙的一步。它建立了一个语言模型策略 `π_θ`（即我们正在训练的模型）和上述隐式奖励模型 `r` 之间的解析关系。通过一系列推导，可以证明奖励函数可以表示为：\n",
    "\n",
    "    $ \\ r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} \\$ \n",
    "\n",
    "    这里：\n",
    "    *   `π_θ(y|x)` 是当前策略模型生成回答 `y` 的概率。\n",
    "    *   `π_ref(y|x)` 是参考模型（通常是SFT后的模型）生成回答 `y` 的概率。\n",
    "    *   `β` 是一个超参数，用于控制策略模型与参考模型之间的散度（差异程度）。它平衡了“迎合偏好”和“保持语言能力”两个目标。\n",
    "\n",
    "#### 4.1.2 DPO 损失函数：直观理解与数学表达\n",
    "\n",
    "将上述奖励函数表达式代入 Bradley-Terry 模型，我们就可以用语言模型的概率来表示人类偏好概率。然后，通过最大化这个偏好概率的对数似然，我们最终得到了 DPO 的损失函数：\n",
    "\n",
    "$-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left( \\beta \\log \\frac{\\pi_\\theta\\left(y_w|x\\right)}{\\pi_{\\text{ref}}\\left(y_w|x\\right)} - \\beta \\log \\frac{\\pi_\\theta\\left(y_l|x\\right)}{\\pi_{\\text{ref}}\\left(y_l|x\\right)} \\right) \\right] = \\mathcal{L}_{\\text{DPO}}\\left(\\pi_\\theta; \\pi_{\\text{ref}}\\right)$\n",
    "\n",
    "\n",
    "**直观理解这个复杂的公式：**\n",
    "\n",
    "*   **核心目标**：损失函数的目标是最大化“模型对 `chosen` 回答的偏好”与“模型对 `rejected` 回答的偏好”之间的差距。\n",
    "*   **偏好如何衡量**：模型对一个回答的“偏好”由 `log(π_θ / π_ref)` 这一项来衡量。它表示当前模型相对于参考模型，对这个回答的概率提升了多少。\n",
    "*   **训练过程**：在每个训练步骤中，模型会调整参数 `θ`，使得 `log(π_θ(y_w|x) / π_ref(y_w|x))` 的值尽可能增大，同时使 `log(π_θ(y_l|x) / π_ref(y_l|x))` 的值尽可能减小。这样一来，两者的差值就会变大，`log σ(...)` 的值也随之增大，最终使得整个损失 `L_DPO` 减小。\n",
    "\n",
    "**DPO vs. RLHF：为什么 DPO 更优越？**\n",
    "\n",
    "*   **简单稳定**：DPO 将复杂的 RLHF 两阶段（奖励建模 + 强化学习）合并为一个简单的、带有新损失函数的分类任务。它不需要训练一个独立的奖励模型，也避免了强化学习中采样和训练不稳定的问题。\n",
    "*   **数据高效**：DPO 直接利用偏好数据，不需要像 PPO 那样在训练中从策略模型大量采样，计算成本更低，数据利用率更高。\n",
    "*   **效果相当甚至更好**：多项研究表明，DPO 能够在比 RLHF 更简单、成本更低的情况下，达到甚至超越 RLHF 的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 启动 DPO 训练\n",
    "\n",
    "接下来，我们使用`erniekit`工具启动DPO训练。`erniekit`是ERNIE项目提供的一个便捷的命令行工具集，可以帮助我们轻松地管理模型的训练、评估和部署等任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**单卡训练命令：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过`--stage DPO`参数指定执行DPO训练任务。同时，我们还配置了`--model_name_or_path`（预训练模型路径）、`--train_dataset_path`（训练数据集）、`--eval_dataset_path`（评估数据集）和`--output_dir`（模型输出路径）等核心参数。此外，诸如`--max_seq_len`（最大序列长度）、`--learning_rate`（学习率）、`--max_steps`（最大训练步数）等超参数也被一并设置，以精确控制训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意：\n",
    "\n",
    "根据调用栈，问 ernie/modeling_moe.py 将一个包含6个元素的 labels 元组传递给了 ernie/loss/dpo.py ，而后者期望的是5个元素。 score_deltas 这个多余的参数导致了错误。因此我们需要手动删除该参数\n",
    "\n",
    "- 在/home/aistudio/ERNIE-develop/ernie/modeling_moe.py，大约（1709行）\n",
    "- 为了快速演示，训练轮数我将减少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T04:04:33.297297Z",
     "iopub.status.busy": "2025-07-18T04:04:33.296967Z",
     "iopub.status.idle": "2025-07-18T04:06:41.292412Z",
     "shell.execute_reply": "2025-07-18T04:06:41.291650Z",
     "shell.execute_reply.started": "2025-07-18T04:04:33.297276Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 -----------  Configuration  ----------------------\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 auto_cluster_config: 0\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 auto_parallel_config: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 auto_tuner_json: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 devices: 0\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 elastic_level: -1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 elastic_timeout: 30\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 enable_gpu_log: True\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 gloo_port: 6767\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 host: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 ips: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 job_id: default\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 legacy: False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 log_dir: erniekit_dist_log\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 log_level: INFO\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 log_overwrite: False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 master: 127.0.0.1:8080\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 max_restart: 3\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 nnodes: 1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 nproc_per_node: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 rank: -1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 run_mode: collective\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 server_num: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 servers: \r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 sort_ip: False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 start_port: 6070\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 trainer_num: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 trainers: \r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 training_script: /home/aistudio/ERNIE-develop/erniekit/launcher.py\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 training_script_args: ['train', '--stage', 'DPO', '--model_name_or_path', '../data/models/30656/ERNIE-4.5-0.3B-Paddle', '--train_dataset_path', './examples/data/dpo-train.jsonl', '--eval_dataset_path', './examples/data/dpo-eval.jsonl', '--output_dir', './output/dpo_tutorial_checkpoint', '--max_seq_len', '8192', '--learning_rate', '5.0e-7', '--warmup_steps', '5', '--max_steps', '10', '--save_steps', '10', '--logging_steps', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '36', '--bf16', 'True', '--do_train', '--fp16_opt_level', 'O2']\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 with_gloo: 1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 --------------------------------------------------\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 Job: default, mode collective, replicas 1[1:1], elastic False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,163 Run Pod: bhnrxg, replicas 1, status ready\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,177 Watching Pod: bhnrxg, replicas 1, status running\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "\u001b[32m[2025-07-18 12:04:41,399] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,432] [ WARNING]\u001b[0m - Dpo training requires use_sparse_head_and_loss_fn=True. Set use_sparse_head_and_loss_fn to True\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,433] [ WARNING]\u001b[0m - LoRA does not support ref_model_update_steps. Set ref_model_update_steps to -1.\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m -      Model Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - add_tail_layers               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - continue_training             : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fine_tuning                   : LoRA\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_gate_detach_matmul       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_linear                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_rms_norm                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_rope                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_softmax_mask             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_swiglu                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_alpha                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_path                     : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_plus_scale               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_rank                     : 8\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - model_name_or_path            : ../data/models/30656/ERNIE-4.5-0.3B-Paddle\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_aux_loss_lambda           : 1e-05\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_group                     : dummy\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_group_experts             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_multimodal_dispatch_use_allgather: v2-alltoall-unpad\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_orthogonal_loss_lambda    : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_use_aux_free              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_use_hard_gate             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_z_loss_lambda             : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - no_recompute_layers           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - offload_recompute_inputs      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - pp_seg_method                 : layer:Ernie4_5_DecoderLayer|EmptyLayer\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - recompute_granularity         : full\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - recompute_use_reentrant       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - rslora                        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - rslora_plus                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - stage                         : DPO\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - tensor_parallel_output        : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_attn_mask_start_row_indices: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_flash_attention           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_fused_head_and_loss_fn    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_recompute_moe             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_sparse_flash_attn         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_sparse_head_and_loss_fn   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - virtual_pp_degree             : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m -       Data Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - buffer_size                   : 500\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - dataset_type                  : iterable\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - eval_dataset_path             : ./examples/data/dpo-eval.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - eval_dataset_prob             : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - eval_dataset_type             : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - greedy_intokens               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - in_tokens_batching            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - mask_out_eos_token            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - max_prompt_len                : 2048\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - max_seq_len                   : 8192\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - num_comparisons               : 6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - num_samples_each_epoch        : 6000000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - offline_dataset_path          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - random_shuffle                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - train_dataset_path            : ./examples/data/dpo-train.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - train_dataset_prob            : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - train_dataset_type            : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - use_cls                       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m -    DPOConfig Configuration Arguments    \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - _no_sync_in_gradient_accumulation: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - actscale_moving_rate          : 0.01\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - adam_beta1                    : 0.9\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - adam_beta2                    : 0.999\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - adam_epsilon                  : 1e-08\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - amp_custom_black_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - amp_custom_white_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - amp_master_grad               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - apply_hadamard                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - apply_online_actscale_step    : 200\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - attention_probs_dropout_prob  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - auto_parallel_resume_form_hybrid_parallel: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - batch_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - beta                          : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - bf16                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - bf16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - ckpt_quant_stage              : O0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - compute_type                  : bf16\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - context_parallel_degree       : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - count_trained_tokens          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - current_device                : gpu:0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - data_parallel_config          : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - data_parallel_degree          : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - data_parallel_rank            : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataloader_drop_last          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataloader_num_workers        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataloader_shuffle            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataset_rank                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataset_world_size            : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - ddp_find_unused_parameters    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - decay_steps                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - device                        : gpu\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - disable_ckpt_quant            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - disable_tqdm                  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - distributed_dataloader        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_eval                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_export                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_predict                    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_train                      : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - dpo_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - dpop_lambda                   : 50\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - dropout_warmup_steps          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - enable_auto_parallel          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - enable_zero_cost_checkpoint   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - eval_accumulation_steps       : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - eval_batch_size               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - eval_steps                    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - evaluation_strategy           : IntervalStrategy.NO\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_max_capacity           : 4294967296\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_min_capacity           : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_tensor_parallel_degree : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - flash_device_save_steps       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - flatten_param_grads           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - force_reshard_pp              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - fp16                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - fp16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - fp16_opt_level                : O2\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - fp8_format_type               : hybrid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - fuse_sequence_parallel_allreduce: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - gradient_accumulation_steps   : 36\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - greater_is_better             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - hadamard_block_size           : 32\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - hidden_dropout_prob           : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - hybrid_parallel_topo_order    : pp_first\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - ignore_data_skip              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - ignore_load_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - ignore_save_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - label_names                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - label_smoothing               : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - layerwise_lr_decay_bound      : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - lazy_data_processing          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - learning_rate                 : 5e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - load_best_model_at_end        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - load_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - local_process_index           : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - local_rank                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - log_level                     : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - log_level_replica             : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - log_on_each_node              : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_dir                   : ./output/dpo_tutorial_checkpoint/runs/Jul18_12-04-41_jupyter-2553954-9384723\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_first_step            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_steps                 : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_strategy              : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logical_process_index         : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - loss_type                     : sigmoid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - lr_end                        : 1e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - lr_scheduler_type             : SchedulerType.LINEAR\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_estimate_samples          : 100000.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_evaluate_steps            : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_grad_norm                 : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_steps                     : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - metric_for_best_model         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - metrics_output_path           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - min_lr                        : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - minimum_eval_times            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - multi_token_pred_lambda       : 0.3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - no_cuda                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - normalize_logps               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_cycles                    : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_nextn_predict_layers      : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_of_gpus                   : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_train_epochs              : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - offload_optim                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - offset_alpha                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - optim                         : OptimizerNames.ADAMW\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - optim_shard_num               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - optimizer_name_suffix         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - ordered_save_group_size       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - output_dir                    : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - output_signal_dir             : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - overwrite_output_dir          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pad_token_id                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - past_index                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pdc_download_ckpt             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pdc_download_timeout          : 300\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - per_device_eval_batch_size    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - per_device_train_batch_size   : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pipeline_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pipeline_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pipeline_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - power                         : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - prediction_loss_only          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pref_loss_ratio               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - process_index                 : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - quant_input_grad              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - quant_weight_grad             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - recompute                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - ref_model_update_steps        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - reference_free                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - refined_recompute             : {}\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - release_grads                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - remove_unused_columns         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - report_to                     : ['visualdl']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - resume_from_checkpoint        : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - run_name                      : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_on_each_node             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_rng_states               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_sharding_stage1_model_include_freeze_params: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_steps                    : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_strategy                 : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_tokenizer                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_total_limit              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - scale_loss                    : 32768\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - seed                          : 42\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - sep_parallel_degree           : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - sequence_parallel             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sequence_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - server_tp_degree              : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sft_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sft_loss_ratio                : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding                      : []\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_comm_buffer_size_MB  : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_degree               : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_mesh_dimension: dp\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_load_dataset           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_load_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_log                    : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save_model_state       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save_model_with_tensor_fusion: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - simpo_gamma                   : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - skip_data_intervals           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - skip_memory_metrics           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - skip_profile_timer            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - split_inputs_sequence_dim     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - split_norm_comm               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensor_parallel_config        : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensor_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensor_parallel_rank          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensorwise_offload_optimizer  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - to_static                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - train_batch_size              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - unified_checkpoint            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - unified_checkpoint_config     : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_async_save                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_expert_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_hybrid_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_lowprecision_moment       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_recompute_mtp             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_sp_callback               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - wandb_api_key                 : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - wandb_http_proxy              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - warmup_ratio                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - warmup_steps                  : 5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - weight_decay                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - weight_name_suffix            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - weight_quantize_algo          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - world_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_ema_interval              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_pipeline_hooks_capacity_usage: 0.6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_save_ema_coef             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_workers_num               : 3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,445] [    INFO]\u001b[0m - The global seed is set to 42, local seed is set to 43 and random seed is set to 42.\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,445] [ WARNING]\u001b[0m - Process rank: -1, device: gpu, world_size: 1, distributed training: False, 16-bits training: True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,445] [    INFO]\u001b[0m - Start to load model ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,449] [    INFO]\u001b[0m - Loading configuration file ../data/models/30656/ERNIE-4.5-0.3B-Paddle/config.json\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,452] [ WARNING]\u001b[0m - You are using a model of type ernie4_5 to instantiate a model of type ernie4_5_moe. This is not supported for all configurations of models and can yield errors.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,454] [    INFO]\u001b[0m - Loading weights file ../data/models/30656/ERNIE-4.5-0.3B-Paddle/model.safetensors\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,748] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,749] [    INFO]\u001b[0m - change initializer-range from 0.02 to 0.018041293779826325\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,749] [    INFO]\u001b[0m - using moe-group: dummy\u001b[0m\r\n",
      "W0718 12:04:41.931790 259232 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.8, Runtime API Version: 12.6\r\n",
      "\u001b[32m[2025-07-18 12:04:41,934] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,937] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,939] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,942] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,944] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,946] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,949] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,951] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,953] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,955] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,957] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,960] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,962] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,964] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,966] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,969] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,971] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,974] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,978] [    INFO]\u001b[0m - output-weight:[103424, 1024] config.tie_word_embeddings=True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,978] [    INFO]\u001b[0m - Use fusedRMSNorm\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,068] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing Ernie4_5_MoeForCausalLM.\r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,068] [    INFO]\u001b[0m - All the weights of Ernie4_5_MoeForCausalLM were initialized from the model checkpoint at ../data/models/30656/ERNIE-4.5-0.3B-Paddle.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Ernie4_5_MoeForCausalLM for predictions without further training.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,070] [    INFO]\u001b[0m - Loading configuration file ../data/models/30656/ERNIE-4.5-0.3B-Paddle/generation_config.json\u001b[0m\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:250: UserWarning: using greedy search strategy. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:255: UserWarning: using greedy search strategy. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-18 12:04:42,082] [    INFO]\u001b[0m - Start to wrap model with LoRA config ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,092] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:42,095] [   DEBUG]\u001b[0m - Frozen parameters: 4.67e+08 || Trainable parameters:5.16e+05 || Total parameters:4.67e+08|| Trainable:0.11%\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,095] [    INFO]\u001b[0m - Wraping model with LoRA config successfully !\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,243] [    INFO]\u001b[0m - Loading model & tokenizer successfully !\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,243] [    INFO]\u001b[0m - Start to create dataset ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,248] [    INFO]\u001b[0m - ./examples/data/dpo-train.jsonl: task prob: 1.0, ori number of examples: 200, target_num_each_epoch: 6000000\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,248] [    INFO]\u001b[0m - Creating dataset successfully ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,248] [    INFO]\u001b[0m - The global seed is set to 42, local seed is set to 43 and random seed is set to 42.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,283] [    INFO]\u001b[0m - max_steps is given, it will override any value given in num_train_epochs\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,283] [    INFO]\u001b[0m - Using half precision\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m -     Training Configuration Arguments    \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - _no_sync_in_gradient_accumulation: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - actscale_moving_rate          : 0.01\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - adam_beta1                    : 0.9\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - adam_beta2                    : 0.999\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - adam_epsilon                  : 1e-08\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - amp_custom_black_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - amp_custom_white_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - amp_master_grad               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - apply_hadamard                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - apply_online_actscale_step    : 200\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - attention_probs_dropout_prob  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - auto_parallel_resume_form_hybrid_parallel: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - batch_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - beta                          : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - bf16                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - bf16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - ckpt_quant_stage              : O0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - compute_type                  : bf16\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - context_parallel_degree       : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - count_trained_tokens          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - current_device                : gpu:0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - data_parallel_config          : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - data_parallel_degree          : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - data_parallel_rank            : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataloader_drop_last          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataloader_num_workers        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataloader_shuffle            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataset_rank                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dataset_world_size            : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - ddp_find_unused_parameters    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - decay_steps                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - device                        : gpu\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - disable_ckpt_quant            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - disable_tqdm                  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - distributed_dataloader        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_eval                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_export                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_predict                    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_train                      : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dpo_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dpop_lambda                   : 50\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dropout_warmup_steps          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - enable_auto_parallel          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - enable_zero_cost_checkpoint   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - eval_accumulation_steps       : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - eval_batch_size               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - eval_steps                    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - evaluation_strategy           : IntervalStrategy.NO\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - expert_max_capacity           : 4294967296\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - expert_min_capacity           : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - expert_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - expert_tensor_parallel_degree : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - flash_device_save_steps       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - flatten_param_grads           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - force_reshard_pp              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp16                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp16_opt_level                : O2\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp8_format_type               : hybrid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fuse_sequence_parallel_allreduce: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - gradient_accumulation_steps   : 36\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - greater_is_better             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - hadamard_block_size           : 32\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - hidden_dropout_prob           : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - hybrid_parallel_topo_order    : pp_first\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - ignore_data_skip              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - ignore_load_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - ignore_save_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - label_names                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - label_smoothing               : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - layerwise_lr_decay_bound      : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - lazy_data_processing          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - learning_rate                 : 5e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - load_best_model_at_end        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - load_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - local_process_index           : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - local_rank                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - log_level                     : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - log_level_replica             : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - log_on_each_node              : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_dir                   : ./output/dpo_tutorial_checkpoint/runs/Jul18_12-04-41_jupyter-2553954-9384723\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_first_step            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_steps                 : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_strategy              : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logical_process_index         : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - loss_type                     : sigmoid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - lr_end                        : 1e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - lr_scheduler_type             : SchedulerType.LINEAR\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_estimate_samples          : 100000.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_evaluate_steps            : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_grad_norm                 : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_steps                     : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - metric_for_best_model         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - metrics_output_path           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - min_lr                        : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - minimum_eval_times            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - multi_token_pred_lambda       : 0.3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - no_cuda                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - normalize_logps               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_cycles                    : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_nextn_predict_layers      : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_of_gpus                   : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_train_epochs              : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - offload_optim                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - offset_alpha                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - optim                         : OptimizerNames.ADAMW\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - optim_shard_num               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - optimizer_name_suffix         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - ordered_save_group_size       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - output_dir                    : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - output_signal_dir             : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - overwrite_output_dir          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pad_token_id                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - past_index                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pdc_download_ckpt             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pdc_download_timeout          : 300\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - per_device_eval_batch_size    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - per_device_train_batch_size   : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pipeline_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - pipeline_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - pipeline_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - power                         : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - prediction_loss_only          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - pref_loss_ratio               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - process_index                 : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - quant_input_grad              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - quant_weight_grad             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - recompute                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - ref_model_update_steps        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - reference_free                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - refined_recompute             : {}\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - release_grads                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - remove_unused_columns         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - report_to                     : ['visualdl']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - resume_from_checkpoint        : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - run_name                      : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_on_each_node             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_rng_states               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_sharding_stage1_model_include_freeze_params: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_steps                    : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_strategy                 : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - save_tokenizer                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - save_total_limit              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - scale_loss                    : 32768\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - seed                          : 42\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sep_parallel_degree           : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sequence_parallel             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sequence_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - server_tp_degree              : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sft_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sft_loss_ratio                : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding                      : []\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_comm_buffer_size_MB  : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_degree               : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_mesh_dimension: dp\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_load_dataset           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_load_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_log                    : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_save                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_save_model_state       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - should_save_model_with_tensor_fusion: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - should_save_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - simpo_gamma                   : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - skip_data_intervals           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - skip_memory_metrics           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - skip_profile_timer            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - split_inputs_sequence_dim     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - split_norm_comm               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensor_parallel_config        : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensor_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensor_parallel_rank          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensorwise_offload_optimizer  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - to_static                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - train_batch_size              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - unified_checkpoint            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - unified_checkpoint_config     : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_async_save                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_expert_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_hybrid_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_lowprecision_moment       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_recompute_mtp             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_sp_callback               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - wandb_api_key                 : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - wandb_http_proxy              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - warmup_ratio                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - warmup_steps                  : 5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - weight_decay                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - weight_name_suffix            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - weight_quantize_algo          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - world_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_ema_interval              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_pipeline_hooks_capacity_usage: 0.6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_save_ema_coef             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_workers_num               : 3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,307] [    INFO]\u001b[0m - Starting training from resume_from_checkpoint : None\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m - [timelog] checkpoint loading time: 0.00s (2025-07-18 12:04:43) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m - ***** Running training *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Num examples = 360\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Num Epochs = 9223372036854775807\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Instantaneous batch size per device = 1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Total train batch size (w. parallel, distributed & accumulation) = 36\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Gradient Accumulation steps = 36\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Total optimization steps = 10\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Total num train samples = 360\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,310] [   DEBUG]\u001b[0m -   Number of trainable parameters = 516,096 (per device)\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:   0%|          | 0/10 [00:00<?, ?it/s]W0718 12:04:50.665277 259232 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.\r\n",
      "\r\n",
      "TrainProcess:  10%|█         | 1/10 [00:18<02:44, 18.24s/it]\u001b[32m[2025-07-18 12:05:01,567] [    INFO]\u001b[0m - loss: 0.69314718, learning_rate: 1e-07, global_step: 1, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 55.92091178894043, max_memory_allocated: 11.65785264968872, max_memory_reserved: 55.92091178894043, interval_runtime: 18.2493, interval_samples_per_second: 1.9727, interval_steps_per_second: 0.0548, rewards/chosen: 0.0, rewards/rejected: 0.0, rewards/accuracies: 0.0, rewards/margins: 0.0, logps/rejected: -593.4622802734375, logps/chosen: -558.8125610351562, sigmoid_loss: 0.6931471824645996, sft_loss: 0.0, progress_or_epoch: 0.1\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  20%|██        | 2/10 [00:28<01:49, 13.72s/it]\u001b[32m[2025-07-18 12:05:12,115] [    INFO]\u001b[0m - loss: 0.69314718, learning_rate: 2e-07, global_step: 2, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 55.92091178894043, max_memory_allocated: 11.65785264968872, max_memory_reserved: 55.92091178894043, interval_runtime: 10.5483, interval_samples_per_second: 3.4129, interval_steps_per_second: 0.0948, rewards/chosen: 0.0, rewards/rejected: 0.0, rewards/accuracies: 0.0, rewards/margins: 0.0, logps/rejected: -561.4170532226562, logps/chosen: -552.60888671875, sigmoid_loss: 0.6931471824645996, sft_loss: 0.0, progress_or_epoch: 0.2\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  30%|███       | 3/10 [00:39<01:27, 12.43s/it]\u001b[32m[2025-07-18 12:05:23,022] [    INFO]\u001b[0m - loss: 0.69692159, learning_rate: 3e-07, global_step: 3, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 55.92091178894043, max_memory_allocated: 11.65785264968872, max_memory_reserved: 55.92091178894043, interval_runtime: 10.9074, interval_samples_per_second: 3.3005, interval_steps_per_second: 0.0917, rewards/chosen: 0.0004406546941027045, rewards/rejected: -0.000714117253664881, rewards/accuracies: 0.5167573690414429, rewards/margins: 0.0011547724716365337, logps/rejected: -565.6019897460938, logps/chosen: -536.885986328125, sigmoid_loss: 0.6969215273857117, sft_loss: 0.0, progress_or_epoch: 0.3\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  40%|████      | 4/10 [00:50<01:09, 11.65s/it]\u001b[32m[2025-07-18 12:05:33,480] [    INFO]\u001b[0m - loss: 0.69113737, learning_rate: 4e-07, global_step: 4, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.4572, interval_samples_per_second: 3.4426, interval_steps_per_second: 0.0956, rewards/chosen: 0.006543044466525316, rewards/rejected: -0.004613205324858427, rewards/accuracies: 0.5185551643371582, rewards/margins: 0.011156250722706318, logps/rejected: -548.4920654296875, logps/chosen: -541.0109252929688, sigmoid_loss: 0.6911373734474182, sft_loss: 0.0, progress_or_epoch: 0.4\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  50%|█████     | 5/10 [01:00<00:56, 11.32s/it]\u001b[32m[2025-07-18 12:05:44,199] [    INFO]\u001b[0m - loss: 0.69389129, learning_rate: 5e-07, global_step: 5, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.7196, interval_samples_per_second: 3.3583, interval_steps_per_second: 0.0933, rewards/chosen: 0.005858992226421833, rewards/rejected: -0.002096320502460003, rewards/accuracies: 0.5291945338249207, rewards/margins: 0.007955312728881836, logps/rejected: -571.0379638671875, logps/chosen: -564.6283569335938, sigmoid_loss: 0.6938911080360413, sft_loss: 0.0, progress_or_epoch: 0.5\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  60%|██████    | 6/10 [01:11<00:44, 11.06s/it]\u001b[32m[2025-07-18 12:05:54,770] [    INFO]\u001b[0m - loss: 0.69585663, learning_rate: 4e-07, global_step: 6, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.5706, interval_samples_per_second: 3.4057, interval_steps_per_second: 0.0946, rewards/chosen: 0.002569297095760703, rewards/rejected: -0.001821780577301979, rewards/accuracies: 0.5273837447166443, rewards/margins: 0.004391078371554613, logps/rejected: -561.6737060546875, logps/chosen: -544.4690551757812, sigmoid_loss: 0.6958566308021545, sft_loss: 0.0, progress_or_epoch: 0.6\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  70%|███████   | 7/10 [01:22<00:32, 10.90s/it]\u001b[32m[2025-07-18 12:06:05,324] [    INFO]\u001b[0m - loss: 0.70019507, learning_rate: 3e-07, global_step: 7, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.5534, interval_samples_per_second: 3.4112, interval_steps_per_second: 0.0948, rewards/chosen: -0.006452502217143774, rewards/rejected: -0.0021812929771840572, rewards/accuracies: 0.5088156461715698, rewards/margins: -0.0042712087742984295, logps/rejected: -511.5072937011719, logps/chosen: -512.0642700195312, sigmoid_loss: 0.7001951336860657, sft_loss: 0.0, progress_or_epoch: 0.7\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  80%|████████  | 8/10 [01:32<00:21, 10.85s/it]\u001b[32m[2025-07-18 12:06:16,074] [    INFO]\u001b[0m - loss: 0.70007676, learning_rate: 2e-07, global_step: 8, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.751, interval_samples_per_second: 3.3485, interval_steps_per_second: 0.093, rewards/chosen: 0.003911859355866909, rewards/rejected: 0.009731447324156761, rewards/accuracies: 0.49071377515792847, rewards/margins: -0.005819589830935001, logps/rejected: -575.8965454101562, logps/chosen: -557.0797729492188, sigmoid_loss: 0.7000767588615417, sft_loss: 0.0, progress_or_epoch: 0.8\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  90%|█████████ | 9/10 [01:43<00:10, 10.82s/it]\u001b[32m[2025-07-18 12:06:26,816] [    INFO]\u001b[0m - loss: 0.70192665, learning_rate: 1e-07, global_step: 9, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.7402, interval_samples_per_second: 3.3519, interval_steps_per_second: 0.0931, rewards/chosen: 0.0030782907269895077, rewards/rejected: 0.013875698670744896, rewards/accuracies: 0.4840790331363678, rewards/margins: -0.010797406546771526, logps/rejected: -542.80224609375, logps/chosen: -535.5034790039062, sigmoid_loss: 0.701926589012146, sft_loss: 0.0, progress_or_epoch: 0.9\u001b[0m\r\n",
      "\r\n",
      "TrainProcess: 100%|██████████| 10/10 [01:54<00:00, 10.81s/it]\u001b[32m[2025-07-18 12:06:37,607] [    INFO]\u001b[0m - loss: 0.69396645, learning_rate: 0.0, global_step: 10, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.7926, interval_samples_per_second: 3.3356, interval_steps_per_second: 0.0927, rewards/chosen: -0.0035317859146744013, rewards/rejected: -0.01574733480811119, rewards/accuracies: 0.5068547129631042, rewards/margins: 0.012215547263622284, logps/rejected: -582.034912109375, logps/chosen: -574.9534912109375, sigmoid_loss: 0.6939664483070374, sft_loss: 0.0, progress_or_epoch: 1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,608] [    INFO]\u001b[0m - Saving model checkpoint to ./output/dpo_tutorial_checkpoint/checkpoint-10\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,611] [    INFO]\u001b[0m - tokenizer config file saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/tokenizer_config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,612] [    INFO]\u001b[0m - Special tokens file saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/special_tokens_map.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,612] [    INFO]\u001b[0m - added tokens file saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/added_tokens.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,625] [    INFO]\u001b[0m - Configuration saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,628] [    INFO]\u001b[0m - Saving optimizer files.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,646] [    INFO]\u001b[0m - [timelog] checkpoint saving time: 0.04s (2025-07-18 12:06:37) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,647] [    INFO]\u001b[0m - \r\n",
      "Training completed. \r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,647] [    INFO]\u001b[0m - train_runtime: 114.3367, train_samples_per_second: 3.1486, train_steps_per_second: 0.0875, train_loss: 0.6960266172885895, progress_or_epoch: 1.0\u001b[0m\r\n",
      "\r\n",
      "TrainProcess: 100%|██████████| 10/10 [01:54<00:00, 11.43s/it]\r\n",
      "\u001b[32m[2025-07-18 12:06:37,647] [    INFO]\u001b[0m - Saving model checkpoint to ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,651] [    INFO]\u001b[0m - tokenizer config file saved in ./output/dpo_tutorial_checkpoint/tokenizer_config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,652] [    INFO]\u001b[0m - Special tokens file saved in ./output/dpo_tutorial_checkpoint/special_tokens_map.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,653] [    INFO]\u001b[0m - added tokens file saved in ./output/dpo_tutorial_checkpoint/added_tokens.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m - Configuration saved in ./output/dpo_tutorial_checkpoint/config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m - ***** train metrics *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   progress_or_epoch        =        1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_loss               =      0.696\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_runtime            = 0:01:54.33\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_samples_per_second =     3.1486\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_steps_per_second   =     0.0875\u001b[0m\r\n",
      "LAUNCH INFO 2025-07-18 12:06:40,304 Pod completed\r\n",
      "LAUNCH INFO 2025-07-18 12:06:40,304 Exit code 0\r\n"
     ]
    }
   ],
   "source": [
    "!erniekit train \\\r\n",
    "    --stage DPO \\\r\n",
    "    --model_name_or_path ../data/models/30656/ERNIE-4.5-0.3B-Paddle \\\r\n",
    "    --train_dataset_path ./examples/data/dpo-train.jsonl \\\r\n",
    "    --eval_dataset_path ./examples/data/dpo-eval.jsonl \\\r\n",
    "    --output_dir ./output/dpo_tutorial_checkpoint \\\r\n",
    "    --max_seq_len 8192 \\\r\n",
    "    --learning_rate 5.0e-7 \\\r\n",
    "    --warmup_steps 5 \\\r\n",
    "    --max_steps 10 \\\r\n",
    "    --save_steps 10 \\\r\n",
    "    --logging_steps 1 \\\r\n",
    "    --per_device_train_batch_size 1 \\\r\n",
    "    --gradient_accumulation_steps 36 \\\r\n",
    "    --bf16 True \\\r\n",
    "    --do_train \\\r\n",
    "    --fp16_opt_level O2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**监控训练过程：**\n",
    "\n",
    "当训练开始后，您会在终端看到大量的日志输出。关注以下关键信息：\n",
    "\n",
    "*   **`loss`**: 这是 DPO 损失的值。它应该随着训练的进行而稳步下降。\n",
    "*   **`rewards/chosen` 和 `rewards/rejected`**: 这些是模型对 chosen 和 rejected 回答的平均隐式奖励得分。您应该会看到 `rewards/chosen` 的值逐渐增大，而 `rewards/rejected` 的值逐渐减小或保持较低水平。\n",
    "*   **`rewards/accuracies`**: 这表示在一个批次中，模型正确地给予 `chosen` 回答比 `rejected` 回答更高奖励的样本比例。这个值应该趋向于 1.0。\n",
    "*   **`rewards/margins`**: 这是 `rewards/chosen` 和 `rewards/rejected` 之间的平均差值。这个值越大，说明模型区分好坏回答的能力越强。\n",
    "*   **`eval_loss`**: 在每个评估步骤 (`eval_steps`)，会在验证集上计算损失。观察这个值可以帮助您判断模型是否过拟合。\n",
    "\n",
    "训练完成后，最终的模型权重和配置文件将保存在您 `output_dir` 指定的目录中（例如 `./output/ERNIE-4.5-0.3B-dpo`）。这个目录下的模型就是我们 DPO 成功的最终产物，它现在更懂得人类的偏好了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 评估与推理\n",
    "\n",
    "经过 DPO 训练，我们的模型在理论上已经更懂得人类的偏好。但“是骡子是马，拉出来遛遛”，本章将指导您如何科学地评估模型的效果，并将其部署到实际的交互式推理任务中，直观地感受其性能提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 DPO 模型评估\n",
    "\n",
    "DPO 的评估与传统的准确率、F1值等指标不同，它更侧重于**生成质量和偏好对齐度**的衡量。评估的核心是判断模型在面对相同的提示时，其生成的内容是否比 SFT 模型更符合我们的期望（例如，更安全、更详细、更具创造性等）。\n",
    "\n",
    "#### 5.1.1 评估策略\n",
    "\n",
    "1.  **构建高质量评估集**：\n",
    "    *   设计一个专门的、包含各种挑战性提示的评估集至关重要。这些提示应该能够激发模型在不同维度上的能力，例如：\n",
    "        *   **安全性**：包含一些可能诱导模型产生不安全或有偏见回答的边缘问题。\n",
    "        *   **指令遵循**：设计带有复杂、多步骤或约束性指令的提示。\n",
    "        *   **创造性与开放性问题**：提出需要模型发挥想象力或进行深入思考的问题。\n",
    "        *   **事实性与知识性**：包含需要准确知识储备才能回答的问题。\n",
    "    *   这个评估集不应该在训练或验证数据中出现过，以保证评估的公正性。\n",
    "\n",
    "2.  **对比评估 (A/B Test)** ：\n",
    "    *   最有效的评估方法是进行**头对头 (Head-to-Head)** 的比较。对于评估集中的每一个提示，分别使用 **SFT 模型** 和 **DPO 模型** 生成回答。\n",
    "    *   将两个模型生成的回答（匿名，顺序打乱）呈现给人类评估者，让他们判断哪个回答更好，或者两者相当/都差。\n",
    "    *   收集大量的评估结果，统计 DPO 模型的胜率、平局率和败率，从而量化其相对于 SFT 模型的提升。\n",
    "\n",
    "3.  **自动化评估 (使用更强的模型作为裁判)** ：\n",
    "    *   在没有足够人力进行大规模人工评估时，可以借助更强大的模型（如 GPT-4, ERNIE-4.0）作为“裁判”。\n",
    "    *   设计一个裁判提示模板，将用户提示、SFT模型的回答、DPO模型的回答一并输入给裁判模型，让它判断哪个更好并给出理由。\n",
    "    *   虽然自动化评估无法完全替代人类，但它提供了一种可扩展的、成本较低的评估方式，能够快速获得模型性能的初步印象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结与展望\n",
    "\n",
    "恭喜您！至此，您已经跟随本教程完整地走完了使用 ERNIE Kit 对 ERNIE-4.5-0.3B 模型进行直接偏好优化 (DPO) 的全过程。现在，让我们一起回顾这段旅程，并展望未来的可能性。\n",
    "\n",
    "### 6.1 本教程核心内容回顾\n",
    "\n",
    "在本教程中，我们系统性地学习并实践了以下核心内容：\n",
    "\n",
    "1.  **DPO 核心思想**：我们深入理解了 DPO 如何巧妙地将人类偏好数据直接转化为一个简单的分类损失，从而绕过 RLHF 复杂的奖励建模和强化学习流程，实现了更高效、更稳定的模型对齐。\n",
    "\n",
    "2.  **环境与代码准备**：我们搭建了基于 PaddlePaddle 和 ERNIE Kit 的开发环境，并学会了如何从源码安装 ERNIE Kit 以获取最新的功能和最大的灵活性。\n",
    "\n",
    "3.  **数据准备与剖析**：我们掌握了 DPO 所需的 `(prompt, chosen, rejected)` 三元组数据格式，并使用 `UltraFeedback Binarized` 数据集作为示例，通过代码下载、解压并查看了数据样本，深刻理解了高质量偏好数据的关键要素。\n",
    "\n",
    "4.  **DPO 训练配置与执行**：我们详细解读了 ERNIE Kit 的 DPO 配置文件，特别是对 `model_name_or_path` (必须是SFT模型)、`beta`、`learning_rate` 等关键超参数的理解和设置。我们还学会了如何通过命令行启动单卡和多卡训练，并如何解读训练日志中的关键指标来监控训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 DPO 的局限性与未来展望\n",
    "\n",
    "DPO 虽然强大，但并非银弹，它也有其局限性：\n",
    "\n",
    "*   **对偏好数据质量的依赖**：DPO 的效果高度依赖于偏好数据的质量和一致性。标注模糊、有偏差或不一致的数据会严重误导模型的学习。\n",
    "*   **Beta 值的敏感性**：超参数 `beta` 的选择对最终结果有显著影响，需要通过实验仔细调整。\n",
    "*   **模式崩溃风险**：尽管比 RLHF 稳定，但在某些情况下，DPO 仍然可能导致模型为了过度迎合偏好而牺牲生成的多样性和流畅性。\n",
    "\n",
    "**前沿的 DPO 变种与未来方向：**\n",
    "\n",
    "学术界和工业界正在不断探索比 DPO 更优的对齐算法。当您对 DPO 有了深入的理解后，不妨关注以下一些前沿方向：\n",
    "\n",
    "*   **IPO (Identity Preference Optimisation)** ：一种 DPO 的变体，通过修改损失函数来声称可以更好地防止模型在偏好数据上过拟合，ERNIE Kit 已支持此损失。\n",
    "*   **KTO (Kahneman-Tversky Optimisation)** ：受人类决策理论启发，KTO 允许使用“好”和“坏”的单一样本进行对齐，而不再严格要求成对的偏好数据，极大地降低了数据标注的门槛。\n",
    "*   **SimPO (Simple Preference Optimisation)** ：一种更简洁的损失函数设计，旨在提高训练效率和最终性能。\n",
    "\n",
    "这些新的算法正在不断推动着大模型对齐技术的发展，也预示着未来我们会有更多、更好用的工具来让 AI 更好地为人类服务。\n",
    "\n",
    "### 6.3 结语\n",
    "\n",
    "本教程为您打开了通往大模型偏好对齐世界的大门。真正的学习始于实践。我们强烈鼓励您：\n",
    "\n",
    "*   **尝试不同的超参数**：调整 `beta`、`learning_rate` 等，观察它们对模型行为的影响。\n",
    "*   **构建自己的数据集**：尝试在您自己的业务场景或感兴趣的领域，构建一个小型的偏好数据集，并用本教程的方法进行训练。\n",
    "*   **探索其他模型**：将本教程的方法应用到 ERNIE Kit 支持的其他模型上。\n",
    "\n",
    "感谢您跟随本教程的学习。愿您在探索大模型的道路上，不断创造出更智能、更可靠、更符合人类价值观的 AI。祝您探索愉快！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题反馈/与我联系： Wechat：G_Fuji"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
