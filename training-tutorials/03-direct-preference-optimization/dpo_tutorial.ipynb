{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Direct Preference Optimization (DPO) Using the ERNIE-4.5-0.3B Large Language Model\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Welcome to this advanced tutorial on using ERNIE Kit to perform Direct Preference Optimization (DPO) on the ERNIE-4.5-0.3B model! After completing the model's pre-training (PT) and supervised fine-tuning (SFT), we now have an ERNIE model with strong general language capabilities and the ability to precisely follow instructions. However, to ensure that the model's generated responses are not only correct but also align with human preferences‚Äîsuch as being more helpful, safer, and more stylistically appealing‚Äîwe typically need to perform further ‚Äúalignment‚Äù training. This tutorial will delve into how to use DPO technology to achieve a higher level of alignment for your ERNIE model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What is Direct Preference Optimization (DPO)?\n",
    "\n",
    "**Direct Preference Optimization (DPO)** is a revolutionary model alignment technique that is simpler and more efficient than traditional methods. Before DPO emerged, the mainstream alignment method in the industry was Reinforcement Learning from Human Feedback (RLHF), which typically involved multiple complex and resource-intensive stages:\n",
    "\n",
    "1.  **Supervised Fine-Tuning (SFT)**: First, the pre-trained large language model is fine-tuned using instructions to enable it to understand and execute various commands. This is the first step in alignment, endowing the model with foundational ‚Äúcapabilities.‚Äù\n",
    "2.  **Training the Reward Model (RM)**: Next, a large amount of human preference ranking data for the model's various outputs must be collected. For example, for the same question, the model generates answers A, B, and C. Annotators indicate which is best, which is second-best, and which is worst. Using this ranking data, an independent ‚Äúreward model‚Äù is trained. The model's task is to mimic human judgment, scoring any given answer, with higher scores indicating stronger human preference.\n",
    "3.  **Reinforcement Learning (RL)**: Finally, the model from the SFT stage is used as an ‚Äúagent‚Äù and trained using reinforcement learning under the guidance of the reward model (typically using the Proximal Policy Optimization (PPO) algorithm). The agent attempts to generate new answers, which are scored by the reward model. The agent continuously adjusts its strategy based on the scores to maximize rewards, thereby making its outputs increasingly aligned with human preferences.\n",
    "\n",
    "Although RLHF has proven highly effective, its process is lengthy, implementation is complex, training is unstable, and it requires maintaining and coordinating multiple models (policy model, reward model, value model, reference model, etc.), placing significant demands on both technical expertise and computational resources.\n",
    "\n",
    "**The core idea of DPO lies in its ‚Äúdirectness‚Äù‚Äîit cleverly bypasses the need for explicit training of the reward model and complex reinforcement learning steps.** DPO posits that the language model itself can implicitly represent a reward function. It directly utilizes paired human preference data (e.g., given a prompt, response A is better than response B) and optimizes the language model directly through a uniquely designed loss function. The objective of this loss function is to increase the model's probability of generating ‚Äúpreferred‚Äù responses while decreasing its probability of generating ‚Äúunpreferred‚Äù responses. Essentially, DPO transforms a complex reinforcement learning problem into a more manageable supervised learning problem with a specific objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences between DPO and RLHF/SFT:**\n",
    "\n",
    "| Feature | Supervised fine-tuning (SFT) | Traditional RLHF (RM+PPO) | Direct preference optimization (DPO) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Core Objective** | Learn to follow instructions and master specific task solutions | Learn to generate responses aligned with human preferences through reward model signals | Directly learn to generate responses aligned with human preferences based on preference data |\n",
    "| **Required Data** | ‚ÄúInstruction-Ideal Output‚Äù pairs | ‚ÄúInstruction-Multiple Outputs-Human Ranking‚Äù (for RM), plus SFT data | ‚ÄúInstruction-preferred output-non-preferred output‚Äù triplets |\n",
    "| **Learning Paradigm** | Supervised learning | RM: Supervised learning; PPO: Reinforcement learning | Supervised learning (preferences reflected through a special loss function) |\n",
    "| **Implementation Complexity** | Relatively simple and direct | Very high (multi-stage training involving complex RL algorithms and hyperparameters) | Moderate (single-stage policy learning, but data format requires specific constraints) |\n",
    "| **Number of Models Required** | 1 (policy model) | At least 2 core models (policy model, reward model), often accompanied by a value model and reference model | 1 core model (policy model), typically using a frozen SFT model as a reference |\n",
    "\n",
    "In summary, while SFT teaches the model ‚Äúwhat it can do,‚Äù DPO (and RLHF) teach the model ‚Äúhow to do it better, more in line with human expectations, and more safely and reliably.‚Äù DPO provides a more direct, concise, and stable technical path to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Application Scenarios and Value of DPO\n",
    "\n",
    "As a cutting-edge alignment technique, DPO has extremely broad and far-reaching application scenarios and significance. We can understand it from the following dimensions:\n",
    "\n",
    "|  | Application Area | Core Value and Far-Reaching Significance |\n",
    "| :---: | :--- | :--- |\n",
    "| üí° | **Improving Answer Quality (Helpfulness)** | By finely learning human preferences, the model can generate more accurate, information-rich, and logically rigorous answers, transforming from an ‚Äúinstruction executor‚Äù into a true ‚Äúintelligent assistant.‚Äù |\n",
    "| üõ°Ô∏è | **Enhancing Model Safety (Harmlessness)** | By learning the boundaries between harmless and harmful content, the model can effectively suppress the generation of inappropriate, biased, or dangerous statements, ensuring its outputs align with social norms and ethical standards. |\n",
    "| üé® | **Precise Style Control** | Based on specific preference data, the model can be guided to adopt specific communication styles, such as polite empathy in customer service scenarios, concise clarity in coding scenarios, and creative freedom in creative scenarios. |\n",
    "| üöÄ | **Streamlined Alignment Process (Efficiency)** | As an efficient alternative to RLHF, DPO lowers the technical barriers and computational costs of achieving high-quality alignment, making it suitable for teams and individuals with limited resources to iterate quickly. |\n",
    "| üî¨ | **Driving Cutting-Edge Innovation (Innovation)** | The success of DPO has inspired the development of a series of new algorithms such as IPO, KTO, SimPO, and ORPO, serving as the foundation for understanding and keeping pace with the latest trends in large-scale model technology. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Why choose ERNIE-4.5-0.3B for DPO?\n",
    "\n",
    "|  | Reason | Detailed explanation |\n",
    "| :---: | :--- | :--- |\n",
    "| üèÜ | **Exceptional SFT Foundation** | `ERNIE-4.5-0.3B` is built on the advanced technology of Wenxin 4.5 and has undergone high-quality SFT, endowing it with robust instruction following capabilities and world knowledge, providing a solid foundation for the ‚Äúcherry on top‚Äù of DPO. |\n",
    "| üíª | **Appropriate Parameter Scale** | The 0.3B parameter count maintains strong capabilities while keeping computational resource requirements manageable, making it ideal for individual developers, academic researchers, and small and medium-sized enterprises to conduct DPO experiments and rapid iterations. |\n",
    "| üõ†Ô∏è | **Full ERNIE Kit Support** | ERNIE Kit offers an end-to-end DPO solution (`run_dpo.py`) from data processing to training execution, enabling developers to efficiently apply DPO without needing to implement complex algorithms from scratch. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Objectives and Benefits of This Tutorial\n",
    "\n",
    "This tutorial is designed for developers and researchers who wish to advance their model alignment skills. By completing this tutorial, you will gain the following:\n",
    "\n",
    "|  | Learning Outcomes | Description |\n",
    "| :---: | :--- | :--- |\n",
    "| üß† | **Deep Understanding of DPO Principles** | Gain a thorough understanding of the core concepts, mathematical principles, practical value, and fundamental differences between DPO and SFT/RLHF. |\n",
    "| üõ†Ô∏è | **Proficient Mastery of the ERNIE Kit DPO Workflow** | Master the entire process from environment setup, data preparation, to model training and evaluation. |\n",
    "| üìä | **Expertise in DPO Data Processing** | Learn to process preference data formats (prompt, chosen, rejected) that meet ERNIE Kit requirements. |\n",
    "| ‚öôÔ∏è | **Flexible Configuration and Practice** | Learn to modify DPO configuration files, start, monitor, and debug training tasks, and evaluate optimized models. |\n",
    "\n",
    "Now, let‚Äôs embark on this exciting journey together and explore how to use DPO technology to refine the powerful ERNIE model into a smarter, safer, and more human-centric AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Preparation\n",
    "\n",
    "Performing Direct Preference Optimization (DPO) is similar to performing SFT in that it requires a well-configured development environment. This includes installing the core deep learning framework PaddlePaddle, the dedicated ERNIE Kit development kit, and obtaining the latest source code to use the most cutting-edge DPO features. We assume you already have practical experience with SFT, so you should be familiar with the environment setup process. However, this section provides more detailed guidance to ensure everything is set up correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Installing PaddlePaddle and ERNIE Kit\n",
    "\n",
    "First, we need to ensure that we have installed the correct versions of PaddlePaddle and ERNIE Kit. For optimal training performance and compatibility, we strongly recommend using the GPU version of PaddlePaddle.\n",
    "\n",
    "*If you are running AI Studio, you do not need to run the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Ensure that your pip tool is the latest version to avoid potential installation issues.\n",
    "!python -m pip install --upgrade pip\n",
    "\n",
    "# Step 2: Install the GPU version of PaddlePaddle.\n",
    "# The following command is applicable to environments with CUDA 11.8 or higher. If your CUDA version is different,\n",
    "# please visit the PaddlePaddle official website to obtain the exact installation instructions: https://www.paddlepaddle.org.cn/install/quick\n",
    "!python -m pip install paddlepaddle-gpu -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Install ERNIE Kit\n",
    "# ERNIE Kit offers two installation methods. We recommend installing from source code to ensure you can use the latest DPO features and bug fixes.\n",
    "# First, you need to clone the ERNIE Kit repository from GitHub.\n",
    "# !git clone https://github.com/PaddlePaddle/ERNIE.git\n",
    "# Then, install using pip in edit mode (-e) so that any changes you make to the code take effect immediately.\n",
    "# Replace the following path with the actual path where you cloned the ERNIE Kit repository.\n",
    "!python -m pip install -e  ./ERNIE-develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post-installation verification**:\n",
    "\n",
    "Execute the following code to check whether PaddlePaddle and ERNIE Kit have been successfully installed and confirm that the GPU environment is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T01:54:48.277759Z",
     "iopub.status.busy": "2025-07-18T01:54:48.277437Z",
     "iopub.status.idle": "2025-07-18T01:54:48.591045Z",
     "shell.execute_reply": "2025-07-18T01:54:48.590312Z",
     "shell.execute_reply.started": "2025-07-18T01:54:48.277740Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddlePaddle Version: 3.1.0\r\n",
      "Running verify PaddlePaddle program ... \r\n",
      "PaddlePaddle works well on 1 GPU.\r\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\r\n",
      "[SUCCESS] PaddlePaddle GPU is available! Found 1 GPU(s).\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0718 09:54:48.481559  9362 pir_interpreter.cc:1524] New Executor is Running ...\r\n",
      "W0718 09:54:48.483019  9362 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.8, Runtime API Version: 12.6\r\n",
      "I0718 09:54:48.483794  9362 pir_interpreter.cc:1547] pir interpreter is running by multi-thread mode ...\r\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "print(f\"PaddlePaddle Version: {paddle.__version__}\")\n",
    "\n",
    "# Run PaddlePaddle's built-in check tool, which will provide detailed environment information.\n",
    "try:\n",
    "    paddle.utils.run_check()\n",
    "    if paddle.device.cuda.device_count() > 0:\n",
    "        print(f\"[SUCCESS] PaddlePaddle GPU is available! Found {paddle.device.cuda.device_count()} GPU(s).\")\n",
    "    else:\n",
    "        print(\"[WARNING] PaddlePaddle GPU check passed, but no GPU found. Training will proceed on CPU, which will be very slow.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] PaddlePaddle GPU check failed: {e}\")\n",
    "    print(\"If you intended to use GPU, please carefully check your CUDA environment, NVIDIA driver, and the PaddlePaddle version you installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download the ERNIE Kit repository code\n",
    "\n",
    "The core DPO script of ERNIE Kit (`examples/long_text/run_dpo.py` or similar path) and all related model configuration files are located in its GitHub repository. Even if you have installed ERNIE Kit via pip, we strongly recommend that you clone the entire code repository, as all experiments and training will be based on this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T01:56:54.494064Z",
     "iopub.status.busy": "2025-07-18T01:56:54.493729Z",
     "iopub.status.idle": "2025-07-18T01:56:54.498103Z",
     "shell.execute_reply": "2025-07-18T01:56:54.497595Z",
     "shell.execute_reply.started": "2025-07-18T01:56:54.494044Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/ERNIE-develop\r\n"
     ]
    }
   ],
   "source": [
    "# If you haven't cloned yet, run this command\n",
    "# git clone https://github.com/PaddlePaddle/ERNIE.git\n",
    "\n",
    "# After cloning, you will have a folder named ERNIE-develop (or ERNIE)\n",
    "# All subsequent commands will assume that you are in the root directory of this folder\n",
    "# For example: cd ./ERNIE-develop\n",
    "%cd ./ERNIE-develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DPO Data Preparation\n",
    "\n",
    "Unlike the ‚Äúcommand-ideal output‚Äù data format used in the SFT stage, the essence of Direct Preference Optimization (DPO) lies in utilizing human **preference judgments** for different model outputs. Therefore, DPO training data directly and accurately reflects these paired preference choices, which is the basis for its ‚Äúdirect‚Äù optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Characteristics and Format of DPO Data\n",
    "\n",
    "The basic unit of DPO training data is a triplet consisting of: **‚ÄúPrompt‚Äù**, **‚ÄúChosen Response‚Äù**, and **‚ÄúRejected Response‚Äù**. This data structure explicitly informs the model which type of response aligns more closely with human expectations when presented with the same prompt, and which does not.\n",
    "\n",
    "**DPO Data Formats Supported by ERNIE Kit:**\n",
    "\n",
    "According to ERNIE Kit's design, the data format expected by DPO scripts is **JSON Lines (jsonl) files**. This is a widely used format where each line in the file represents an independent, valid JSON object, corresponding to a preference sample. A typical sample structure is shown below:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"Please explain what artificial intelligence is to me.\",\n",
    "    \"chosen\": \"Artificial intelligence (AI) is a branch of computer science dedicated to creating machines that can mimic, extend, and surpass human intelligence. It encompasses multiple fields such as machine learning, natural language processing, and computer vision, aiming to enable computers to learn, reason, perceive, and solve problems like humans. For example, voice assistants on our smartphones, self-driving cars, and recommendation systems are all applications of artificial intelligence in daily life.\",\n",
    "    \"rejected\": \"Artificial intelligence is just robots.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Field-by-field breakdown**:\n",
    "\n",
    "* `prompt`: `(string)`, required. This is the user instruction, question, or preceding dialogue input to the model. It serves as the context for preference judgments and the starting point for all comparisons.\n",
    "* `chosen`: `(string)`, required. This is the response to `prompt` that was labeled by humans as **more preferred** or **higher quality**. It represents the direction the model should learn and emulate.\n",
    "* `rejected`: `(string)`, required. This is the response to the `prompt` that was labeled by humans as **less preferred** or **lower quality**. It represents the direction the model should avoid and suppress.\n",
    "\n",
    "**Why this ‚Äúprompt-chosen-rejected‚Äù triplet format?**\n",
    "\n",
    "The DPO loss function requires a direct comparison of the probabilities (or implicit preference scores) assigned by the model to the `chosen` and `rejected` responses. Therefore, each training data point must clearly provide this paired comparison of (chosen, rejected) outputs, along with the input prompt `prompt` they correspond to. This structure allows the loss function to directly calculate preference differences and update model parameters accordingly.\n",
    "\n",
    "**Key elements of high-quality DPO data:**\n",
    "\n",
    "Building or selecting a high-quality DPO dataset is critical to success. A good dataset should have the following characteristics:\n",
    "\n",
    "*   **Clear and meaningful preference differences**: There should be clear, distinguishable quality differences between chosen and rejected responses. These differences should accurately reflect the specific preferences you want the model to learn, such as:\n",
    "    *   **Factual accuracy**: Chosen responses are more accurate, while Rejected responses contain factual errors.  \n",
    "    *   **Harmlessness**: Chosen responses are safe and polite, while Rejected responses contain harmful, offensive, or biased content.\n",
    "    * **Helpfulness and detail**: Chosen answers are more comprehensive and helpful, while Rejected answers are too brief or off-topic.  \n",
    "    * **Instruction adherence**: Chosen answers strictly adhere to all constraints in the instructions (e.g., format, role), while Rejected answers do not.\n",
    "    * **Style preference**: Chosen answers match a specific style (e.g., professional, humorous), while Rejected answers do not.\n",
    "* **Prompt diversity**: The `prompt` should broadly cover various types of instructions, topics, and difficulty levels that the model may encounter in real-world applications to ensure that the preferences learned by the model have good generalization capabilities.\n",
    "* **Authenticity and Challenge of Responses**: `chosen` and `rejected` responses should ideally be the model's actual outputs from the SFT phase, or carefully constructed examples that represent typical model errors or potential areas for improvement. The difference between the two should not be overly extreme (e.g., a perfect answer vs. a completely irrelevant answer); moderate difficulty better stimulates the model's learning potential.\n",
    "* **Consistent Annotation Standards**: If data is annotated by multiple people, a unified and clear set of preference judgment criteria must be established and followed to ensure the reliability of data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using the Sample DPO Dataset Included in ERNIE Kit\n",
    "\n",
    "To help users get started quickly, the ERNIE Kit project has included a small sample DPO dataset in the `examples/data/` directory. This dataset can be used directly to run the DPO training script in the tutorial without downloading it from the internet.\n",
    "\n",
    "**Dataset files:**\n",
    "\n",
    "*   `examples/data/dpo-train.jsonl`: Preference dataset used for DPO training.\n",
    "*   `examples/data/dpo-eval.jsonl`: Preference dataset used to evaluate model performance during training.\n",
    "\n",
    "Both files follow the JSON Lines format discussed earlier, containing the `prompt`, `chosen`, and `rejected` fields.\n",
    "\n",
    "**Viewing data samples:**\n",
    "\n",
    "Assuming your current working directory is the root directory of ERNIE Kit, you can use the following code to view the contents of the training data and gain an intuitive understanding of its structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:04:15.890827Z",
     "iopub.status.busy": "2025-07-18T03:04:15.890259Z",
     "iopub.status.idle": "2025-07-18T03:04:15.897453Z",
     "shell.execute_reply": "2025-07-18T03:04:15.896826Z",
     "shell.execute_reply.started": "2025-07-18T03:04:15.890798Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- View the included DPO training data samples: ./examples/data/dpo-train.jsonl ---\r\n",
      "\r\n",
      "--- sample 1 ---\r\n",
      "Prompt: ËØ∑ÂÜô‰∏Ä‰ªΩÂÖ≥‰∫éÊîØÊíëËà™Á©∫ÂíåÈ£ûË°åÁöÑÁâ©ÁêÜÂü∫Êú¨ÂéüÁêÜÁöÑÂÖ®Èù¢Ëß£ÈáäÔºåÂåÖÊã¨ÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõÁ≠â‰∏ªÈ¢òÔºå‰ª•Âèä‰ºØÂä™Âà©ÂÆöÂæã„ÄÅÁâõÈ°øËøêÂä®ÂÆöÂæãÂíåÁ©∫Ê∞îÊÄßË¥®Á≠âÂÖ≥ÈîÆÁßëÂ≠¶Ê¶ÇÂøµ„ÄÇ‰ΩøÁî®Ê∏ÖÊô∞ÁÆÄÊ¥ÅÁöÑËØ≠Ë®ÄÔºåÂπ∂Êèê‰æõËØ¶ÁªÜÁöÑ‰æãÂ≠êÂíåÊèíÂõæÔºå‰ª•Â∏ÆÂä©ËØªËÄÖÁêÜËß£Ëøô‰∫õÊ¶ÇÂøµ„ÄÇËÄÉËôëÂ°ëÈÄ†Ëà™Á©∫‰∏öÁöÑÂéÜÂè≤ÂíåÊäÄÊúØÂèëÂ±ïÔºå‰ª•ÂèäÁâ©ÁêÜÂ≠¶Âú®Êé®Âä®È£ûË°åËÉΩÂäõÊñπÈù¢ÂèëÊå•ÁöÑ‰ΩúÁî®„ÄÇ\r\n",
      "\r\n",
      "Chosen: ['È£ûË°åÁâ©ÁêÜÂ≠¶ÔºöËà™Á©∫ÂéüÁêÜÂÖ®Èù¢ÊåáÂçó\\n\\n**‰ªãÁªç**\\n\\nÁêÜËß£ÊîØÊíëËà™Á©∫ÂíåÈ£ûË°åÁöÑÁâ©ÁêÜÂéüÁêÜÂØπ‰∫éÁêÜËß£È£ûÊú∫Âú®Á©∫‰∏≠È£ûË°åÁöÑÊú∫Âà∂Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®Êú¨ÊåáÂçó‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®È£ûË°åÁöÑÂü∫Êú¨Âõ†Á¥†ÔºåÂåÖÊã¨ÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõÔºåÂπ∂Êé¢Á¥¢ÂÖ≥ÈîÆÁöÑÁßëÂ≠¶Ê¶ÇÂøµÔºåÂ¶Ç‰ºØÂä™Âà©ÂÆöÁêÜ„ÄÅÁâõÈ°øËøêÂä®ÂÆöÂæãÂíåÁ©∫Ê∞îÁöÑÁâπÊÄß„ÄÇÈÄöËøá‰ΩøÁî®Ê∏ÖÊô∞„ÄÅÁÆÄÊòéÁöÑËØ≠Ë®ÄÔºåÂπ∂Êèê‰æõËØ¶ÁªÜÁöÑ‰æãÂ≠êÂíåÊèíÂõæÔºåÊú¨ÊåáÂçóÊó®Âú®‰ΩøËà™Á©∫ÂíåÈ£ûË°åÁöÑÊ¶ÇÂøµÂØπËØªËÄÖÊòì‰∫éÁêÜËß£ÂíåÂê∏Âºï„ÄÇ\\n\\n**1. È£ûË°åÁöÑÂõõ‰∏™ÂäõÈáè**\\n\\nÈ¶ñÂÖàÔºåËÆ§ËØÜÂà∞Âú®È£ûË°åËøáÁ®ã‰∏≠‰ΩúÁî®‰∫éÈ£ûÊú∫ÁöÑÂõõ‰∏™ÂÖ≥ÈîÆÂäõÈáèÊòØÂæàÈáçË¶ÅÁöÑÔºöÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõ„ÄÇËøô‰∫õÂäõÈáè‰πãÈó¥ÁöÑÂæÆÂ¶ôÂπ≥Ë°°ÊòØÈ£ûÊú∫‰øùÊåÅÁ©∫‰∏≠È£ûË°åÂíåÁª¥ÊåÅÂÖ∂ÊâÄÈúÄÈ£ûË°åË∑ØÂæÑÊâÄÂøÖÈúÄÁöÑ„ÄÇ\\n\\n1.1. ÂçáÂäõÔºöÂçáÂäõÊòØÊäµÊäóÈ£ûÊú∫ÈáçÈáèÂπ∂ÊîØÊíëÂÖ∂Âú®Á©∫‰∏≠ÁöÑÂäõÈáè„ÄÇÂçáÂäõÊòØÈÄöËøáÊìçÁ∫µÈ£ûÊú∫Êú∫ÁøºÂë®Âõ¥ÁöÑÊ∞îÂéãÂàÜÂ∏ÉÊù•‰∫ßÁîüÁöÑ„ÄÇÊú∫ÁøºÁöÑÂΩ¢Áä∂ÔºåÁß∞‰∏∫ÁøºÂûãÔºåÊòØ‰∏∫‰∫Ü‰ºòÂåñÂçáÂäõËÄåËÆæËÆ°ÁöÑ„ÄÇÂçáÂäõ‰∏ªË¶ÅÊòØÈÄöËøá‰ºØÂä™Âà©ÂÆöÁêÜÂíåÁâõÈ°øËøêÂä®ÂÆöÂæãÊù•Ëß£ÈáäÁöÑÔºåÊàë‰ª¨Á®çÂêéÂ∞ÜËØ¶ÁªÜÊé¢ËÆ®„ÄÇ\\n\\n1.2. Êé®ÂäõÔºöÊé®ÂäõÊòØÁî±È£ûÊú∫ÂèëÂä®Êú∫‰∫ßÁîüÁöÑÂêëÂâçÁöÑÂäõÈáèÔºåÊé®Âä®È£ûÊú∫Á©øËøáÁ©∫Ê∞î„ÄÇÂñ∑Ê∞îÂèëÂä®Êú∫„ÄÅËû∫ÊóãÊ°®ÂèëÂä®Êú∫ÁîöËá≥ÁÅ´ÁÆ≠ÂèëÂä®Êú∫ÈÉΩÂèØ‰ª•Êèê‰æõÂøÖË¶ÅÁöÑÊé®ÂäõÊù•ÂÖãÊúçÈòªÂäõÔºåËøôÊòØÂè¶‰∏Ä‰∏™ÊéßÂà∂È£ûË°åÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ\\n\\n1.3. ÈáçÈáèÔºöÈáçÈáèÊòØÁî±‰∫éÈ£ûÊú∫Ë¥®ÈáèÊâÄ‰∫ßÁîüÁöÑÈáçÂäõ„ÄÇÂÆÉÂêë‰∏ã‰ΩúÁî®Âπ∂ÊäµÊäóÂçáÂäõ„ÄÇ‰∏∫‰∫Ü‰øùÊåÅÊ∞¥Âπ≥È£ûË°åÔºåÈ£ûÊú∫ÂøÖÈ°ª‰∫ßÁîüË∂≥Â§üÁöÑÂçáÂäõÊù•ÊäµÊ∂àÂÖ∂ÈáçÈáè„ÄÇ\\n\\n1.4. ÈòªÂäõÔºöÈòªÂäõÊòØÈòªÁ¢çÈ£ûÊú∫Âú®Á©∫Ê∞î‰∏≠ËøêÂä®ÁöÑÂäõÈáè„ÄÇÂÆÉ‰∏ªË¶ÅÁî±‰∏§ÁßçÁ±ªÂûãÁªÑÊàêÔºöÂΩ¢Áä∂ÈòªÂäõÔºåÁî±‰∫éÈ£ûÊú∫ÁöÑÂΩ¢Áä∂ËÄå‰∫ßÁîüÔºå‰ª•ÂèäÁöÆËÇ§Êë©Êì¶ÈòªÂäõÔºåÁî±‰∫éÁ©∫Ê∞î‰∏éÈ£ûÊú∫Ë°®Èù¢‰πãÈó¥ÁöÑÊë©Êì¶ËÄå‰∫ßÁîü„ÄÇÊúÄÂ∞èÂåñÈòªÂäõÂØπ‰∫éÂÆûÁé∞È´òÊïàÈ£ûË°åËá≥ÂÖ≥ÈáçË¶Å„ÄÇ\\n\\n**2. ‰ºØÂä™Âà©ÂÆöÁêÜÂíåÂçáÂäõ**\\n\\n‰ºØÂä™Âà©ÂÆöÁêÜÊòØÊµÅ‰ΩìÂä®ÂäõÂ≠¶‰∏≠ÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºåÂØπ‰∫éÁêÜËß£ÂçáÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇËØ•ÂÆöÁêÜÊåáÂá∫ÔºåÂΩìÊµÅ‰ΩìÔºàÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÊòØÁ©∫Ê∞îÔºâÁöÑÈÄüÂ∫¶Â¢ûÂä†Êó∂ÔºåÂÖ∂ÂéãÂäõÈôç‰Ωé„ÄÇÈ£ûÊú∫Êú∫ÁøºÈááÁî®ÁøºÂûãÂΩ¢Áä∂Ôºå‰ΩøÁ©∫Ê∞îÂú®Êú∫Áøº‰∏äÊñπÁßªÂä®ÂæóÊØî‰∏ãÊñπÂø´„ÄÇËøôÂØºËá¥‰∏äË°®Èù¢ÁöÑÂéãÂäõÈôç‰ΩéÔºå‰∏ãË°®Èù¢ÁöÑÂéãÂäõÂçáÈ´òÔºå‰∫ßÁîüÂçáÂäõ„ÄÇ\\n\\n**3. ÁâõÈ°øËøêÂä®ÂÆöÂæãÂíåÈ£ûË°å**\\n\\nËâæËê®ÂÖã¬∑ÁâõÈ°øÁàµÂ£´ÁöÑËøêÂä®ÂÆöÂæãÂú®Ëß£Èáä‰ΩúÁî®‰∫éÈ£ûÊú∫ÁöÑÂäõÈáèÊñπÈù¢‰πüËµ∑ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇ\\n\\n3.1. ÁâõÈ°øÁ¨¨‰∏ÄÂÆöÂæãÔºàÊÉØÊÄßÔºâÔºöÈùôÊ≠¢ÁöÑÁâ©‰ΩìÂ∞Ü‰øùÊåÅÈùôÊ≠¢ÔºåËøêÂä®ÁöÑÁâ©‰ΩìÂ∞ÜÁªßÁª≠‰ª•ÊÅíÂÆöÁöÑÈÄüÂ∫¶ËøêÂä®ÔºåÈô§ÈùûÂèóÂà∞Â§ñÂäõÁöÑ‰ΩúÁî®„ÄÇÂΩìÂàÜÊûêÂçáÂäõ‰∫ßÁîüÊó∂ÔºåËøô‰∏™ÂÆöÂæãÈÄÇÁî®‰∫éÈ£ûË°åÔºåÂÖ∂‰∏≠Á©∫Ê∞îÊµÅÂä®ÊñπÂêëÊîπÂèòÔºåÂØºËá¥Âêë‰∏äÁöÑÂäõÔºàÂçáÂäõÔºâ„ÄÇ\\n\\n3.2. ÁâõÈ°øÁ¨¨‰∫åÂÆöÂæãÔºàÂäõ=Ë¥®Èáè√óÂä†ÈÄüÂ∫¶ÔºâÔºöÁâ©‰ΩìÁöÑÂä†ÈÄüÂ∫¶‰∏éÊñΩÂä†ÁöÑÂäõÊàêÊ≠£ÊØîÔºå‰∏éÂÖ∂Ë¥®ÈáèÊàêÂèçÊØî„ÄÇÂú®\r\n",
      "\r\n",
      "Rejected: ['Âü∫Á°ÄÁâ©ÁêÜÂéüÁêÜÊîØÊíëÁùÄËà™Á©∫ÂíåÈ£ûË°åÔºåÂåÖÊã¨ÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõ„ÄÇËøô‰∫õÂéüÁêÜÂèóÂà∞‰ºØÂä™Âà©ÂÆöÂæã„ÄÅÁâõÈ°øËøêÂä®ÂÆöÂæã‰ª•ÂèäÁ©∫Ê∞îÁâπÊÄßÁ≠âÂ§öÁßçÁßëÂ≠¶Ê¶ÇÂøµÁöÑÂΩ±Âìç„ÄÇË¶ÅÂÆûÁé∞ÂçáÂäõÔºåÈ£ûÊú∫ÈúÄË¶ÅÂú®ÂÖ∂‰∏äÊñπÊúâ‰∏Ä‰∏™ÊØîÂú∞ÁêÉÂºïÂäõÂêë‰∏ãÁöÑÂäõÊõ¥Â§ßÁöÑÂêë‰∏äÂäõ„ÄÇËøôÁßçÂêë‰∏äÂäõÊòØÈÄöËøáÊú∫Áøº‰∏éÁ©∫Ê∞îÁõ∏‰∫í‰ΩúÁî®ËÄå‰∫ßÁîüÁöÑ„ÄÇÊé®ÂäõÂàôÁî±ËøûÊé•Âà∞È£ûÊú∫ÁöÑÂèëÂä®Êú∫‰∫ßÁîüÔºåÈÄ†Êàê‰∏Ä‰∏™ÂêëÂâçÁöÑÂäõÔºå‰ΩøÈ£ûÊú∫Âú®Á©∫‰∏≠ÁßªÂä®„ÄÇËÄåÈáçÈáèÂàôÊòØ‰∏ÄÁßçÂ∞ÜÁâ©‰ΩìÂêëÂú∞ÁêÉ‰∏≠ÂøÉÊãâÁöÑÂäõ„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞È£ûË°åÔºåÈ£ûÊú∫ÁöÑÂêë‰∏äÂäõÂøÖÈ°ªÂ§ß‰∫éÂºïÂäõÁöÑÂêë‰∏ãÂäõ„ÄÇ\\n\\nÈ£ûÊú∫ÁöÑÈ£ûË°åËÉΩÂäõÈÄöËøáÊäÄÊúØÂàõÊñ∞„ÄÅÁ†îÁ©∂ÂíåÂÆûÈ™åÁöÑÁªìÂêàÂæóÂà∞‰∫ÜÊèêÈ´ò„ÄÇ‰ªé19‰∏ñÁ∫™Êú´ÁöÑËé±ÁâπÈ£ûË°åÂô®Âà∞21‰∏ñÁ∫™ÁöÑÊ≥¢Èü≥737 MAXÔºåÈ£ûÊú∫ÁöÑÊºîÂèòÊòØÁî±‰∫éÂ¢ûÂä†ÈÄüÂ∫¶„ÄÅÂÆπÈáèÂíåÂÆâÂÖ®ÊÄßÁöÑÊÑøÊúõÊé®Âä®ÁöÑ„ÄÇÂñ∑Ê∞îÂèëÂä®Êú∫„ÄÅÊ∂°ÊâáÂèëÂä®Êú∫‰ª•ÂèäÂ§çÂêàÊùêÊñôÁöÑÂºïÂÖ•‰ΩøÈ£ûÊú∫ÁöÑÈÄüÂ∫¶Êõ¥Âø´„ÄÅÁáÉÊ≤πÊõ¥ËäÇÁ∫¶Ôºå‰πüÊõ¥ÂÆâÂÖ®„ÄÇ\\n\\nÊ≠§Â§ñÔºåÁâ©ÁêÜÂ≠¶Âú®Ëà™Á©∫Âô®ËÆæËÆ°‰∏≠ÂèëÊå•‰∫ÜÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÈÄöËøáÁ©∫Ê∞îÂä®ÂäõÂ≠¶„ÄÅÊµÅ‰ΩìÂäõÂ≠¶ÂíåÁªìÊûÑËÆæËÆ°ÁöÑËøõÊ≠•„ÄÇËøô‰∫õÊ¶ÇÂøµ‰øÉËøõ‰∫ÜÁ©∫Ê∞îÂä®ÂäõÂ≠¶Êú∫ÁøºË°®Èù¢„ÄÅËäÇËÉΩÂñ∑Ê∞îÂèëÂä®Êú∫ÂíåËΩªÈáè„ÄÅÂùöÂõ∫ÁöÑÈ£ûÊú∫ÁªìÊûÑÁöÑÂèëÂ±ï„ÄÇÂ¶Ç‰ªäÔºåÁ†îÁ©∂Ê≠£Âú®ËøõË°å‰∏≠Ôºå‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÈ£ûÊú∫ÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºåÊó®Âú®ÂàõÈÄ†Âá∫ÈÄüÂ∫¶ÊúÄÂø´„ÄÅÊúÄÂèØÊåÅÁª≠‰∏îÊúÄÂÆâÂÖ®ÁöÑ‰∫§ÈÄöÊñπÂºè„ÄÇ']\r\n",
      "--------------------\r\n",
      "\r\n",
      "--- sample 2 ---\r\n",
      "Prompt: Part 1. Definition\r\n",
      "Âú®Ëøô‰∏™‰ªªÂä°‰∏≠Ôºå‰Ω†‰ºöÂæóÂà∞‰∏Ä‰∫õÊé®ÁâπÂ∏ñÂ≠ê„ÄÇ‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†áËÆ∞Â∏ñÂ≠ê‰∏≠Áî®Êà∑Ë°®ËææÁöÑÊÉÖÊÑüÔºåÂ¶ÇÊÇ≤‰º§„ÄÅÂñúÊÇ¶„ÄÅÁà±„ÄÅÊÑ§ÊÄí„ÄÅÊÅêÊÉßÊàñÊÉäËÆ∂„ÄÇ\r\n",
      "Part 2. Example\r\n",
      "im feeling quite sad and sorry for myself but ill snap out of it soon\r\n",
      "Á≠îÊ°àÔºöÊÇ≤‰º§\r\n",
      "Ëß£ÈáäÔºöÂ∏ñÂ≠ê‰∏≠ÁöÑÊÉÖÊÑüÊòØÊÇ≤‰º§ÔºåÂõ†‰∏∫Áî®Êà∑ÊèêÂà∞‚ÄúÊÑüËßâÁõ∏ÂΩìÊÇ≤‰º§Âíå‰∏∫Ëá™Â∑±ÊÑüÂà∞Êä±Ê≠â‚Äù„ÄÇÂõ†Ê≠§ÔºåÊ†áÁ≠æÊòØ‚ÄúÊÇ≤‰º§‚Äù„ÄÇ\r\n",
      "Part 3. Exercise\r\n",
      "im feeling morose as i tend to do when im awake and writing here at almost am\r\n",
      "Á≠îÊ°àÔºö\r\n",
      "\r\n",
      "Chosen: ['ÊÇ≤‰º§\\nËß£ÈáäÔºöËøôÁØáÂ∏ñÂ≠êÁöÑÊÉÖÁª™ÊòØÊÇ≤‰º§ÔºåÂõ†‰∏∫Áî®Êà∑ÊèêÂà∞‚ÄúÊÑüÂà∞ÂøßÈÉÅ‚ÄùÔºåËøôÊòØÊÇ≤‰º§Êàñ‰∏çÂø´‰πêÁöÑÂêå‰πâËØç„ÄÇ‰ªñ‰ª¨Âú®Ê∏ÖÈÜíÁöÑÊó∂ÂÄôÂÜô‰ΩúÂπ∂Ë°®ËææËøôÁßçÊÉÖÁª™ÔºåËøôË°®Êòé‰∫Ü‰∏ÄÁßçÊ∂àÊûÅÁöÑÊÉÖÁª™Áä∂ÊÄÅ„ÄÇÂõ†Ê≠§ÔºåÊ†áÁ≠æÊòØ‚ÄúÊÇ≤‰º§‚Äù„ÄÇ']\r\n",
      "\r\n",
      "Rejected: ['ÊÇ≤‰º§\\n\\nËß£ÈáäÔºöËøôÁØáÂ∏ñÂ≠êÁöÑÊÉÖÁª™ÊòØÊÇ≤‰º§ÔºåÂõ†‰∏∫Áî®Êà∑ËØ¥Ëá™Â∑±‚ÄúÊÑüÂà∞ÈÉÅÈó∑‚ÄùÂíå‚ÄúÂá†‰πéÂáåÊô®ËøòÊ∏ÖÈÜíÁùÄÂú®ËøôÈáåÂÜô‚ÄùÔºåËøôË°®Êòé‰ªñ‰ª¨Ê≠£Âú®ÁªèÂéÜÊÇ≤‰º§„ÄÇ']\r\n",
      "--------------------\r\n",
      "\r\n",
      "--- sample 3 ---\r\n",
      "Prompt: ‰Ω†ÊòØ‰∏Ä‰∏™AIÂä©ÁêÜ„ÄÇÊèê‰æõËØ¶ÁªÜÁöÑÁ≠îÊ°àÔºå‰ΩøÁî®Êà∑‰∏çÈúÄË¶ÅÂú®Â§ñÈÉ®ÊêúÁ¥¢Â∞±ËÉΩÁêÜËß£Á≠îÊ°à„ÄÇ\r\n",
      "\r\n",
      "Chosen: ['Ê†áÈ¢òÔºöÁà±‰∏éÊ¨∫È™óÁöÑËâ∫ÊúØ\\n\\nÁ±ªÂûãÔºöÊµ™Êº´ÊÉäÊÇöÁâá\\n\\nÂâßÊÉÖÔºö\\n\\nÂìàÈáåÔºà‰∏Ä‰∏™ÊúâÊâçÂçé‰∏îÂÜÖÂêëÁöÑÁîªÂÆ∂ÔºâÂíåÊ®±Ê°ÉÔºà‰∏Ä‰∏™ÊúâÊä±Ë¥üÁöÑËÆ∞ËÄÖÔºâÔºå‰ªñËá™Áî±Â•îÊîæÁöÑÂ•≥ÂèãÔºåÊê¨Ëøõ‰∫ÜÂüéÂ∏ÇÊ≥¢Ë•øÁ±≥‰∫öÂå∫ÁöÑÈòÅÊ•º„ÄÇ‰ªñ‰ª¨ÂÖ±ÂêåË∏è‰∏ä‰∫ÜÂÆûÁé∞Ê¢¶ÊÉ≥Âπ∂Êõ¥Âä†‰∫≤ËøëÂΩºÊ≠§ÁöÑÊóÖÁ®ãÔºåÂç¥‰∏çÁü•ÈÅì‰ºöÈù¢‰∏¥ËÄÉÈ™å‰ªñ‰ª¨ÂÖ≥Á≥ªÂíåÂø†ËØöÁöÑÊåëÊàò„ÄÇ\\n\\nÁ¨¨‰∏ÄÂπïÔºö\\n\\nÁîµÂΩ±‰ª•Â±ïÁ§∫ËøôÂØπÊÉÖ‰æ£ÊÑâÂø´ÁöÑÊó•Â∏∏ÁîüÊ¥ªÂºÄÂßã„ÄÇÂìàÈáå‰ªéÊ®±Ê°ÉÁöÑÊøÄÂä®‰∫∫ÂøÉÁöÑÊïÖ‰∫ãÂíåÁªèÂéÜ‰∏≠Ëé∑ÂæóÁªòÁîªÁÅµÊÑüÔºå‰ªñ‰ª¨ÁöÑÂÖ≥Á≥ªËøÖÈÄüËì¨ÂãÉÂèëÂ±ï„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÊ®±Ê°ÉÂºÄÂßãÂú®‰∏ÄÂÆ∂ÂΩìÂú∞ÊùÇÂøó‰∏ä‰Ωú‰∏∫ËÆ∞ËÄÖÂ±ïÂºÄËÅå‰∏öÁîüÊ∂ØÔºåÂøô‰∫éÂ§ö‰∏™‰ªªÂä°„ÄÇ\\n\\nÁ¨¨‰∫åÂπïÔºö\\n\\nÊúâ‰∏ÄÂ§©ÔºåÂΩìÊ®±Ê°ÉË¢´ÊåáÊ¥æË∞ÉÊü•ÊúâÂΩ±ÂìçÂäõÁöÑËâ∫ÊúØÂ§ß‰∫®‰∫öÂäõÂÖãÊó∂ÔºåÂìàÈáåÂØπÂêë‰ªñÂ±ïÁ§∫Ëá™Â∑±ÁöÑÁîª‰ΩúÁöÑÊú∫‰ºö‰∫ßÁîüÂÖ¥Ë∂£„ÄÇÂìàÈáåËØ¥ÊúçÊ®±Ê°ÉÂ∏ÆÂä©‰ªñÂºïËµ∑‰∫öÂäõÂÖãÁöÑÊ≥®ÊÑèÔºå‰ª•Âú®Ëâ∫ÊúØÁïåËé∑ÂæóËÆ§ÂèØ„ÄÇÊ®±Ê°ÉÂú®ËÆ∞ËÄÖÈÅìÂæ∑ÂíåÁà±ÊÉÖ‰πãÈó¥ÁäπË±´‰∏çÂÜ≥Âú∞ÂêåÊÑè‰∫Ü„ÄÇ\\n\\nÈöèÁùÄËøôÂØπÊÉÖ‰æ£Ë∂äÊù•Ë∂äÊ∑±ÂÖ•‰∫öÂäõÂÖãÁöÑËâ∫ÊúØÂíåË¥¢ÂØå‰∏ñÁïåÔºåÂìàÈáåÂèòÂæóÊõ¥Âä†ÈõÑÂøÉÂãÉÂãÉÂíåÁñèËøúÔºåËÆ©Ê®±Ê°ÉÊÑüÂà∞Â≠§Áã¨Âíå‰∏çË¢´ÈáçËßÜ„ÄÇ‰∏éÊ≠§ÂêåÊó∂ÔºåÊ®±Ê°ÉÂèëÁé∞‰∫öÂäõÂÖãÊ∂âË∂≥ÈùûÊ≥ïËâ∫ÊúØ‰º™ÈÄ†ËÆ°ÂàíÔºåÂπ∂ÊâìÁÆóÂà©Áî®ÂìàÈáåÁöÑÊâçÂçéË∞ãÂèñËá™Â∑±ÁöÑÂà©Áõä„ÄÇ\\n\\nÁ¨¨‰∏âÂπïÔºö\\n\\nÊÇ≤ÁóõÊ¨≤ÁªùÁöÑÊ®±Ê°ÉÂêëÂìàÈáåÊè≠Á§∫‰∫ÜÂ•πÁöÑÂèëÁé∞Ôºå‰ΩÜ‰ªñÊÑüÂà∞ÁüõÁõæÔºåÂõ†‰∏∫‰∫öÂäõÂÖãÊâøËØ∫Áªô‰ªñ‰ªñ‰∏ÄÁõ¥Ê∏¥ÊúõÁöÑÂêçÂ£∞ÂíåËÆ§ÂèØ„ÄÇÊ®±Ê°ÉÂú®ÂØπÂìàÈáåÁöÑÂø†ËØöÂíåËÆ∞ËÄÖËÅå‰∏ö‰πãÈó¥ÁäπË±´‰∏çÂÜ≥„ÄÇÂ•πÂÜ≥ÂÆöÁªìÊùüËøôÊÆµÂÖ≥Á≥ªÔºå‰ΩÜË∑üÈöèÁõ¥ËßâÂºÄÂßãÊî∂ÈõÜÈíàÂØπ‰∫öÂäõÂÖãÁöÑËØÅÊçÆ„ÄÇ\\n\\nÂú®ÊúÄÂêé‰∏ÄÂπï‰∏≠ÔºåÂìàÈáåÁúãÊ∏Ö‰∫Ü‰∫öÂäõÂÖãÁöÑÁúüÂÆûÊÑèÂõæÔºåÊÑèËØÜÂà∞Ê®±Ê°ÉÂú®‰ªñÁîüÊ¥ª‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÂú®Ê®±Ê°ÉÁöÑÂ∏ÆÂä©‰∏ãÔºåÂìàÈáåÊúÄÁªàÊô∫ËÉú‰∫öÂäõÂÖãÔºåÂØºËá¥‰ªñË¢´ÈÄÆÊçï„ÄÇ\\n\\nÁîµÂΩ±‰ª•ÂìàÈáåÂíåÊ®±Ê°ÉÂíåÂ•ΩÂπ∂ÈáçÊñ∞ÁÇπÁáÉ‰ªñ‰ª¨ÁöÑÂÖ≥Á≥ªÁªìÊùü„ÄÇÂìàÈáåÁúüÊ≠£ÁöÑÊâçÂçéÂú®Ëâ∫ÊúØÁïåÂæóÂà∞ËÆ§ÂèØÔºåÊ®±Ê°ÉÁöÑËÆ∞ËÄÖÊâçËÉΩ‰πüÂèóÂà∞ËµûÊâ¨„ÄÇÁîµÂΩ±‰ª•ËøôÂØπÊÉÖ‰æ£ËÆ§ËØÜÂà∞Áà±ÂíåËØö‰ø°ÁöÑÈáçË¶ÅÊÄßËÄåÁªìÊùüÔºåËØÅÊòéÁúüÊ≠£ÁöÑÂπ∏Á¶èÊ∞∏Ëøú‰∏ç‰ºöË¢´Ê¨∫ËØàË°å‰∏∫ÊàñÊª°Ë∂≥Ëá™ÁßÅÊ¨≤ÊúõÊâÄÂ¶•Âçè„ÄÇ']\r\n",
      "\r\n",
      "Rejected: ['ÂΩìÁÑ∂ÔºåÊàëÂæà‰πêÊÑèÂ∏Æ‰Ω†Âõ¥ÁªïÂìàÈáåÂíå‰ªñÁöÑÂ•≥ÊúãÂèãÊ®±Ê°ÉÊûÑÂª∫‰∏Ä‰∏™ÁîµÂΩ±ÊÉÖËäÇÔºÅ‰ª•‰∏ãÊòØ‰∏Ä‰∏™ÂèØËÉΩÁöÑÊïÖ‰∫ãÊÉÖËäÇÔºö\\n\\nÂìàÈáåÊòØ‰∏Ä‰ΩçÊàêÂäüÁöÑÂπ¥ËΩª‰∏ì‰∏ö‰∫∫Â£´ÔºåÂàöÂàöÂú®ÂüéÂ∏ÇÈáåÊâæÂà∞‰∫Ü‰ªñÊ¢¶ÂØê‰ª•Ê±ÇÁöÑÂ∑•‰Ωú„ÄÇÂÖ¥Â•ãÂú∞ÂºÄÂßã‰ªñÁöÑÊñ∞ÁîüÊ¥ªÔºå‰ªñÁùÄÊâãÂØªÊâæ‰∏Ä‰∏™‰ΩèÊâÄ„ÄÇÂΩì‰ªñÂú®ÁΩë‰∏äÊµèËßàÊàøÊ∫êÊó∂Ôºå‰ªñÁúãÂà∞‰∫Ü‰∏Ä‰∏™Ëø∑‰∫∫ÁöÑÂ∞èÂÖ¨ÂØìÂºïËµ∑‰∫Ü‰ªñÁöÑÊ≥®ÊÑè„ÄÇÂîØ‰∏ÄÁöÑÈóÆÈ¢òÊòØÂÆÉÊúâÁÇπË∂ÖÂá∫‰ªñÁöÑÈ¢ÑÁÆóÔºå‰ΩÜ‰ªñÂÜ≥ÂÆöÂÜíÈô©Áî≥ËØ∑„ÄÇ\\n\\n‰ª§‰ªñÊÉäËÆ∂ÁöÑÊòØÔºå‰ªñË¢´ÊâπÂáÜ‰∫ÜËøô‰∏™ÂÖ¨ÂØìÔºåÂπ∂Âú®Âêå‰∏ÄÂ§©Êê¨‰∫ÜËøõÂéª„ÄÇÂΩì‰ªñÊâìÂºÄÁÆ±Â≠êÂºÄÂßãÊî∂ÊãæÊó∂Ôºå‰ªñÈÅáÂà∞‰∫Ü‰ªñÁöÑÊñ∞ÈÇªÂ±ÖÊ®±Ê°ÉÔºåÂ•πÊòØ‰∏Ä‰∏™Ëá™Áî±Â•îÊîæÁöÑËâ∫ÊúØÂÆ∂„ÄÇ‰ªñ‰ª¨Â±ïÂºÄ‰∫ÜÂØπËØùÔºåÂìàÈáåÁ´ãÂàªË¢´Â•πÊ¥ªÊ≥ºÁöÑ‰∏™ÊÄßÂíåÊ¨¢Âø´ÁöÑÁ¨ëÂ£∞ÊâÄÂê∏Âºï„ÄÇ‰ªñ‰ª¨‰∫§Êç¢‰∫ÜÁîµËØùÂè∑Á†ÅÔºåÂæàÂø´‰ªñ‰ª¨Â∞±ÂºÄÂßã‰∫íÂèëÁü≠‰ø°„ÄÇ\\n\\nÂú®‰ªñÊÑèËØÜÂà∞‰πãÂâçÔºåÂìàÈáåÂíåÊ®±Ê°ÉÂºÄÂßãËä±Ë¥πÊâÄÊúâÁöÑÁ©∫Èó≤Êó∂Èó¥Âú®‰∏ÄËµ∑ÔºåÊé¢Á¥¢ÂüéÂ∏ÇÔºåÂ∞ùËØïÊñ∞È§êÂéÖÔºå‰∫´ÂèóÂΩºÊ≠§ÁöÑÈô™‰º¥„ÄÇ‰ªñ‰ª¨ÈÉΩÊ∑±Ê∑±Áà±‰∏ä‰∫ÜÂØπÊñπÔºå‰ΩÜ‰ªñ‰ª¨‰πüÈÉΩ‰∏çÊÑøÊÑèÁªô‰ªñ‰ª¨ÁöÑÂÖ≥Á≥ªË¥¥Ê†áÁ≠æ„ÄÇ‰ªñ‰ª¨Êª°Ë∂≥‰∫éÁúãÁúã‰∫ãÊÉÖ‰ºöÂèëÂ±ïÂà∞Âì™ÈáåÔºå‰∫´ÂèóÂΩì‰∏ãÁöÑÊó∂Âàª„ÄÇ\\n\\nÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÔºåÂìàÈáåÂíåÊ®±Ê°ÉÁöÑÂÖ≥Á≥ªÂä†Ê∑±Ôºå‰ªñ‰ª¨ÂºÄÂßãË∞àËÆ∫‰ªñ‰ª¨Êú™Êù•ÁöÑËÆ°Âàí„ÄÇ‰ªñ‰ª¨ÈÉΩÊúâÁùÄÂÆè‰ºüÁöÑÊ¢¶ÊÉ≥ÂíåÊä±Ë¥üÔºå‰ªñ‰ª¨ÂÜ≥ÂøÉÊîØÊåÅÂΩºÊ≠§„ÄÇ‰ªñ‰ª¨ÂºÄÂßãËÆ°Âàí‰∏ÄËµ∑ÊóÖË°åÔºå‰ªñ‰ª¨ÈÉΩÂØπÁúãÂà∞‰ªñ‰ª¨ÁöÑÊóÖÁ®ãÂ∞ÜÂ∏¶‰ªñ‰ª¨ÂéªÂì™ÈáåÊÑüÂà∞ÂÖ¥Â•ã„ÄÇ\\n\\n‰ΩÜÂ∞±Âú®‰∫ãÊÉÖÂºÄÂßãÂ•ΩËΩ¨ÁöÑÊó∂ÂÄôÔºåÊ®±Ê°ÉÁöÑËøáÂéªÂç¥ÂõûÊù•Âõ∞Êâ∞Â•π„ÄÇÂ•π‰∏ÄÁõ¥ÂØπÂìàÈáåÈöêÁûíÁùÄ‰∏Ä‰∏™ÁßòÂØÜÔºåÂ•π‰∏çÁ°ÆÂÆöÊòØÂê¶ÂáÜÂ§áÂêë‰ªñÈÄèÈú≤„ÄÇÂ•πÂÆ≥ÊÄïÂ§±Âéª‰ªñÔºå‰πü‰∏çÁü•ÈÅì‰ªñÂèëÁé∞ÁúüÁõ∏Âêé‰ºöÊúâ‰ªÄ‰πàÂèçÂ∫î„ÄÇ\\n\\nÈöèÁùÄ‰ªñ‰ª¨‰πãÈó¥ÁöÑÁ¥ßÂº†ÂÖ≥Á≥ªÂä†ÂâßÔºåÂìàÈáåÂºÄÂßãÊ≥®ÊÑèÂà∞Êúâ‰∫õ‰∫ãÊÉÖ‰∏çÂØπÂä≤„ÄÇ‰ªñËÉΩÊÑüËßâÂà∞Ê®±Ê°ÉÂØπ‰ªñÈöêÁûí‰∫Ü‰∏Ä‰∫õ‰∫ãÊÉÖÔºå‰ªñÂÜ≥ÂøÉÊâæÂá∫ÁúüÁõ∏„ÄÇ‰ªñË¥®ÈóÆÂ•πÔºåÂ•πÊúÄÁªàÂù¶ÁôΩ‰∫ÜÂ•πÁöÑËøáÂéª„ÄÇ\\n\\nÂéüÊù•ÔºåÊ®±Ê°ÉÂíåÂ•πÁöÑÂÆ∂Â∫≠ÊúâÁùÄ‰∏çÊÑâÂø´ÁöÑÂéÜÂè≤„ÄÇÂ•πÁöÑÁà∂ÊØçÊõæÁªèËôêÂæÖÂíåÂøΩËßÜÂ•πÔºåËá™‰ªéÁ¶ªÂºÄÂÆ∂ÂêéÂ•π‰∏ÄÁõ¥Âú®Âä™ÂäõÊé•ÂèóËøáÂéª„ÄÇÂ•π‰∏ÄÁõ¥ËØïÂõæÊëÜËÑ±Âπ∂ÂºÄÂßãÊñ∞ÁöÑÁîüÊ¥ªÔºå‰ΩÜÂ•πÁ´•Âπ¥ÁöÑËÆ∞ÂøÜ‰∏ÄÁõ¥Âú®Âõ∞Êâ∞ÁùÄÂ•π„ÄÇ\\n\\nÂìàÈáåÂØπÊ®±Ê°ÉÁöÑÂêØÁ§∫ÊÑüÂà∞ÈúáÊÉäÂíåÊÇ≤‰º§Ôºå‰ΩÜ‰ªñ‰πüÂÜ≥ÂøÉÂú®Ëøô‰∏™Âõ∞ÈöæÊó∂ÊúüÊîØÊåÅÂ•π„ÄÇ‰ªñÂÄæÂê¨Â•πÔºåÊã•Êä±Â•πÔºåÂπ∂ÂëäËØâÂ•πÊó†ËÆ∫ÂèëÁîü‰ªÄ‰πà‰ªñÈÉΩ‰ºöÂú®Â•πË∫´Ëæπ„ÄÇ‰ªñËÆ©Â•πÊîæÂøÉÔºå‰ªñÁà±Â•πÁöÑÊú¨Ë¥®Ôºå‰ªñ‰∏ç‰ºöÁ¶ªÂºÄ„ÄÇ\\n\\nÂΩì‰ªñ‰ª¨‰∏ÄËµ∑ÂÖãÊúçÈóÆÈ¢òÊó∂ÔºåÂìàÈáåÂíåÊ®±Ê°ÉÂØπÂΩºÊ≠§ÁöÑÁà±Âè™‰ºöÂèòÂæóÊõ¥Âä†ÂùöÂõ∫„ÄÇ‰ªñ‰ª¨ÊÑèËØÜÂà∞ÂΩºÊ≠§ÊòØÂØπÊñπÁöÑÊîØÊü±ÔºåÂè™Ë¶Å‰ªñ‰ª¨Âú®‰∏ÄËµ∑Ôºå‰ªñ‰ª¨Â∞±ÂèØ‰ª•Èù¢ÂØπ‰ªª‰Ωï‰∫ãÊÉÖ„ÄÇ‰ªñ‰ª¨ÂÜ≥ÂÆöÂ∞Ü‰ªñ‰ª¨ÁöÑÂÖ≥Á≥ªÊèêÂçáÂà∞Êñ∞ÁöÑÊ∞¥Âπ≥ÔºåÂπ∂ÂºÄÂßãËßÑÂàí‰ªñ‰ª¨ÁöÑÊú™Êù•„ÄÇ\\n\\nÁîµÂΩ±‰ª•ÂìàÈáåÂíåÊ®±Ê°ÉÂùêÂú®Ê≤ôÂèë‰∏äÔºåÊâãÁâµÊâãÔºåÊúõÁùÄÂüéÂ∏ÇÂ§©ÈôÖÁ∫øÁöÑÊ∏©È¶®Âú∫ÊôØÁªìÊùü„ÄÇ‰ªñ‰ª¨ÈÉΩÂæÆÁ¨ëÁùÄÔºåÁü•ÈÅì‰ªñ‰ª¨Âú®ÂΩºÊ≠§Ë∫´‰∏äÊâæÂà∞‰∫Ü‰ªñ‰ª¨ÁöÑÂπ∏Á¶èÁªìÂ±Ä„ÄÇ']\r\n",
      "--------------------\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ERNIE Kit's built-in DPO training data path\n",
    "dpo_train_file = \"./examples/data/dpo-train.jsonl\"\n",
    "\n",
    "if os.path.exists(dpo_train_file):\n",
    "    print(f\"--- View the included DPO training data samples: {dpo_train_file} ---\")\n",
    "    with open(dpo_train_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 3: # Only view the first 3 samples\n",
    "                sample = json.loads(line.strip())\n",
    "                print(f\"\\n--- sample {i+1} ---\")\n",
    "                # Based on the file format, ‚Äòsrc‚Äô is a list, and we take the first element as the prompt.\n",
    "                print(f\"Prompt: {sample['src'][0]}\")\n",
    "                # ‚Äòresponse‚Äô is a list containing two answers, and the [1, 0] in the ‚Äòsort‚Äô field indicates that the first answer is chosen and the second is rejected.\n",
    "                print(f\"\\nChosen: {sample['response'][0]}\")\n",
    "                print(f\"\\nRejected: {sample['response'][1]}\")\n",
    "                print(\"-\" * 20)\n",
    "            else:\n",
    "                break\n",
    "else:\n",
    "    print(f\"[ERROR] DPO training data file not found: {dpo_train_file}. Please confirm that your working directory is the ERNIE Kit root directory and that the file actually exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the above code, you can clearly see the specific format of the DPO data, which is crucial for understanding the subsequent training process. Now that the data is ready, we can proceed to explore the core components of DPO training.\n",
    "\n",
    "By examining the printed samples, you can clearly see how each data point is composed of `prompt`, `chosen`, and `rejected`, which helps you better understand the DPO training mechanism and provides a reference for building your own dataset in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Key Points of Data Preprocessing and Tokenization\n",
    "\n",
    "Similar to SFT and the pretraining stage, text data must be converted into a numerical sequence (Token IDs) that the model can understand before DPO training. The `run_dpo.py` script in ERNIE Kit automatically handles complex preprocessing logic in the background, but understanding its core mechanisms is crucial:\n",
    "\n",
    "1.  **Tokenization**: The script uses a Tokenizer that is fully compatible with your base model (ERNIE-4.5-0.3B, typically the version after SFT). For each sample, the `prompt`, `chosen`, and `rejected` text segments are tokenized independently.\n",
    "\n",
    "2.  **Sequence Construction and Loss Calculation**:\n",
    "*   The core of DPO is comparing the model's preference for the `chosen` and `rejected` responses. This is typically achieved by calculating the model's **log-probabilities** on these two response sequences.\n",
    "    * To calculate these probabilities, the script constructs two input sequences: one is the concatenation of `prompt` and `chosen` (`prompt + chosen`), and the other is the concatenation of `prompt` and `rejected` (`prompt + rejected`).\n",
    "    * **Intelligent handling of labels**: When calculating the loss, we are only concerned with the model's predictive ability for the **response part**, while the `prompt` part is just context and should not be included in the loss. This is achieved by **masking** the tokens in the non-response part of the `labels` (typically set to -100).\n",
    "        * For the `prompt + chosen` sequence, the corresponding `labels` have the `prompt` part masked, with only the `chosen` part being the true token ID.\n",
    "* Similarly, for the `prompt + rejected` sequence, the `labels` have the `prompt` part masked, with only the `rejected` part being the true token ID.\n",
    "    * **DPO loss function**: Ultimately, the DPO loss function uses the difference in log probabilities calculated by the model on these two sets of sequences to guide the update of model parameters, with the goal of making the probability of the `chosen` response higher than that of the `rejected` response.\n",
    "\n",
    "3.  **Length control: truncation and padding**:\n",
    "    * In practical applications, text lengths vary. The script uses the `max_length` (or similar) parameter to handle this uniformly. Sequences that are too long (`prompt + response`) are truncated, while shorter ones are padded with padding tokens to reach the same length, enabling batch training.\n",
    "\n",
    "4.  **Role of the Reference Model**:\n",
    "    * The standard DPO algorithm introduces a ‚Äúreference model,‚Äù which is typically the model at the end of the SFT phase. During DPO training, its parameters are **completely frozen and not updated**.\n",
    "* When calculating the log probabilities of the `chosen` and `rejected` responses, the DPO loss function subtracts the log probability of the reference model for the same response. This can be understood as a form of **regularization** aimed at preventing the policy model (i.e., the model we are training) from deviating too far from the general language capabilities it learned during the SFT phase by overly accommodating preference data, thereby causing ‚Äúcatastrophic forgetting.‚Äù It ensures that the model does not lose its foundational ability to ‚Äúspeak human language‚Äù while learning ‚Äúpreferences.‚Äù\n",
    "\n",
    "Fortunately, as users, we typically only need to correctly specify the dataset path, SFT model path, and related sequence length and DPO core hyperparameters (such as `beta`) in the configuration file. The `run_dpo.py` script in ERNIE Kit handles all the complex details of tokenization, sequence concatenation, label masking, and loss calculation for us.\n",
    "\n",
    "At this point, we have a high-quality, properly formatted preference dataset and a deep understanding of the underlying processing logic. This is the solid foundation for successful DPO training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ERNIE Kit DPO Data Format\n",
    "\n",
    "Based on our analysis of the `ERNIE-develop` project, the DPO script in ERNIE Kit expects data in the form of **JSON Lines (jsonl) files**, where each line is a JSON object representing a preference sample. A typical sample structure is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"system\": \"System prompt\",\n",
    "    \"src\": [\"User Question 1\", \"User Question 2\"],\n",
    "    \"tgt\": [\"Model's response to User Question 1\"],\n",
    "    \"response\": [\n",
    "        [\"Preferred response to User Question 2\"],\n",
    "        [\"Non-preferred response to User Question 2\"]\n",
    "    ],\n",
    "    \"sort\": [1, 0]\n",
    "}\n",
    "```\n",
    "\n",
    "**Field explanations**:\n",
    "\n",
    "*   `system`: (optional) `str`. System-level prompt used to set the model's role or behavioral guidelines.\n",
    "*   `src`: `List[str]`. User input sequence. In multi-round conversations, this will be a list containing multiple user inputs.\n",
    "* `tgt`: `List[str]`. The model's responses in the previous rounds of dialogue.\n",
    "* `response`: `List[List[str]]`. A list containing two lists. The first list is the **chosen response** (preferred response), and the second list is the **rejected response** (non-preferred response).\n",
    "* `sort`: `List[int]`. A list containing two integers, `[1, 0]`, indicating that the first response in `response` is preferred and the second is not preferred.\n",
    "\n",
    "ERNIE Kit provides a sample dataset, which we can use as a reference to prepare our own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Example of Using the DPO Dataset\n",
    "\n",
    "ERNIE Kit provides `dpo-train.jsonl` and `dpo-eval.jsonl` as examples in the `examples/data/` directory. We can use these data directly for DPO training.\n",
    "\n",
    "**Viewing Data Samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:06:19.154276Z",
     "iopub.status.busy": "2025-07-18T03:06:19.153922Z",
     "iopub.status.idle": "2025-07-18T03:06:19.159143Z",
     "shell.execute_reply": "2025-07-18T03:06:19.158601Z",
     "shell.execute_reply.started": "2025-07-18T03:06:19.154255Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"src\": [\r\n",
      "        \"ËØ∑ÂÜô‰∏Ä‰ªΩÂÖ≥‰∫éÊîØÊíëËà™Á©∫ÂíåÈ£ûË°åÁöÑÁâ©ÁêÜÂü∫Êú¨ÂéüÁêÜÁöÑÂÖ®Èù¢Ëß£ÈáäÔºåÂåÖÊã¨ÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõÁ≠â‰∏ªÈ¢òÔºå‰ª•Âèä‰ºØÂä™Âà©ÂÆöÂæã„ÄÅÁâõÈ°øËøêÂä®ÂÆöÂæãÂíåÁ©∫Ê∞îÊÄßË¥®Á≠âÂÖ≥ÈîÆÁßëÂ≠¶Ê¶ÇÂøµ„ÄÇ‰ΩøÁî®Ê∏ÖÊô∞ÁÆÄÊ¥ÅÁöÑËØ≠Ë®ÄÔºåÂπ∂Êèê‰æõËØ¶ÁªÜÁöÑ‰æãÂ≠êÂíåÊèíÂõæÔºå‰ª•Â∏ÆÂä©ËØªËÄÖÁêÜËß£Ëøô‰∫õÊ¶ÇÂøµ„ÄÇËÄÉËôëÂ°ëÈÄ†Ëà™Á©∫‰∏öÁöÑÂéÜÂè≤ÂíåÊäÄÊúØÂèëÂ±ïÔºå‰ª•ÂèäÁâ©ÁêÜÂ≠¶Âú®Êé®Âä®È£ûË°åËÉΩÂäõÊñπÈù¢ÂèëÊå•ÁöÑ‰ΩúÁî®„ÄÇ\"\r\n",
      "    ],\r\n",
      "    \"tgt\": [],\r\n",
      "    \"response\": [\r\n",
      "        [\r\n",
      "            \"È£ûË°åÁâ©ÁêÜÂ≠¶ÔºöËà™Á©∫ÂéüÁêÜÂÖ®Èù¢ÊåáÂçó\\n\\n**‰ªãÁªç**\\n\\nÁêÜËß£ÊîØÊíëËà™Á©∫ÂíåÈ£ûË°åÁöÑÁâ©ÁêÜÂéüÁêÜÂØπ‰∫éÁêÜËß£È£ûÊú∫Âú®Á©∫‰∏≠È£ûË°åÁöÑÊú∫Âà∂Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®Êú¨ÊåáÂçó‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®È£ûË°åÁöÑÂü∫Êú¨Âõ†Á¥†ÔºåÂåÖÊã¨ÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõÔºåÂπ∂Êé¢Á¥¢ÂÖ≥ÈîÆÁöÑÁßëÂ≠¶Ê¶ÇÂøµÔºåÂ¶Ç‰ºØÂä™Âà©ÂÆöÁêÜ„ÄÅÁâõÈ°øËøêÂä®ÂÆöÂæãÂíåÁ©∫Ê∞îÁöÑÁâπÊÄß„ÄÇÈÄöËøá‰ΩøÁî®Ê∏ÖÊô∞„ÄÅÁÆÄÊòéÁöÑËØ≠Ë®ÄÔºåÂπ∂Êèê‰æõËØ¶ÁªÜÁöÑ‰æãÂ≠êÂíåÊèíÂõæÔºåÊú¨ÊåáÂçóÊó®Âú®‰ΩøËà™Á©∫ÂíåÈ£ûË°åÁöÑÊ¶ÇÂøµÂØπËØªËÄÖÊòì‰∫éÁêÜËß£ÂíåÂê∏Âºï„ÄÇ\\n\\n**1. È£ûË°åÁöÑÂõõ‰∏™ÂäõÈáè**\\n\\nÈ¶ñÂÖàÔºåËÆ§ËØÜÂà∞Âú®È£ûË°åËøáÁ®ã‰∏≠‰ΩúÁî®‰∫éÈ£ûÊú∫ÁöÑÂõõ‰∏™ÂÖ≥ÈîÆÂäõÈáèÊòØÂæàÈáçË¶ÅÁöÑÔºöÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõ„ÄÇËøô‰∫õÂäõÈáè‰πãÈó¥ÁöÑÂæÆÂ¶ôÂπ≥Ë°°ÊòØÈ£ûÊú∫‰øùÊåÅÁ©∫‰∏≠È£ûË°åÂíåÁª¥ÊåÅÂÖ∂ÊâÄÈúÄÈ£ûË°åË∑ØÂæÑÊâÄÂøÖÈúÄÁöÑ„ÄÇ\\n\\n1.1. ÂçáÂäõÔºöÂçáÂäõÊòØÊäµÊäóÈ£ûÊú∫ÈáçÈáèÂπ∂ÊîØÊíëÂÖ∂Âú®Á©∫‰∏≠ÁöÑÂäõÈáè„ÄÇÂçáÂäõÊòØÈÄöËøáÊìçÁ∫µÈ£ûÊú∫Êú∫ÁøºÂë®Âõ¥ÁöÑÊ∞îÂéãÂàÜÂ∏ÉÊù•‰∫ßÁîüÁöÑ„ÄÇÊú∫ÁøºÁöÑÂΩ¢Áä∂ÔºåÁß∞‰∏∫ÁøºÂûãÔºåÊòØ‰∏∫‰∫Ü‰ºòÂåñÂçáÂäõËÄåËÆæËÆ°ÁöÑ„ÄÇÂçáÂäõ‰∏ªË¶ÅÊòØÈÄöËøá‰ºØÂä™Âà©ÂÆöÁêÜÂíåÁâõÈ°øËøêÂä®ÂÆöÂæãÊù•Ëß£ÈáäÁöÑÔºåÊàë‰ª¨Á®çÂêéÂ∞ÜËØ¶ÁªÜÊé¢ËÆ®„ÄÇ\\n\\n1.2. Êé®ÂäõÔºöÊé®ÂäõÊòØÁî±È£ûÊú∫ÂèëÂä®Êú∫‰∫ßÁîüÁöÑÂêëÂâçÁöÑÂäõÈáèÔºåÊé®Âä®È£ûÊú∫Á©øËøáÁ©∫Ê∞î„ÄÇÂñ∑Ê∞îÂèëÂä®Êú∫„ÄÅËû∫ÊóãÊ°®ÂèëÂä®Êú∫ÁîöËá≥ÁÅ´ÁÆ≠ÂèëÂä®Êú∫ÈÉΩÂèØ‰ª•Êèê‰æõÂøÖË¶ÅÁöÑÊé®ÂäõÊù•ÂÖãÊúçÈòªÂäõÔºåËøôÊòØÂè¶‰∏Ä‰∏™ÊéßÂà∂È£ûË°åÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ\\n\\n1.3. ÈáçÈáèÔºöÈáçÈáèÊòØÁî±‰∫éÈ£ûÊú∫Ë¥®ÈáèÊâÄ‰∫ßÁîüÁöÑÈáçÂäõ„ÄÇÂÆÉÂêë‰∏ã‰ΩúÁî®Âπ∂ÊäµÊäóÂçáÂäõ„ÄÇ‰∏∫‰∫Ü‰øùÊåÅÊ∞¥Âπ≥È£ûË°åÔºåÈ£ûÊú∫ÂøÖÈ°ª‰∫ßÁîüË∂≥Â§üÁöÑÂçáÂäõÊù•ÊäµÊ∂àÂÖ∂ÈáçÈáè„ÄÇ\\n\\n1.4. ÈòªÂäõÔºöÈòªÂäõÊòØÈòªÁ¢çÈ£ûÊú∫Âú®Á©∫Ê∞î‰∏≠ËøêÂä®ÁöÑÂäõÈáè„ÄÇÂÆÉ‰∏ªË¶ÅÁî±‰∏§ÁßçÁ±ªÂûãÁªÑÊàêÔºöÂΩ¢Áä∂ÈòªÂäõÔºåÁî±‰∫éÈ£ûÊú∫ÁöÑÂΩ¢Áä∂ËÄå‰∫ßÁîüÔºå‰ª•ÂèäÁöÆËÇ§Êë©Êì¶ÈòªÂäõÔºåÁî±‰∫éÁ©∫Ê∞î‰∏éÈ£ûÊú∫Ë°®Èù¢‰πãÈó¥ÁöÑÊë©Êì¶ËÄå‰∫ßÁîü„ÄÇÊúÄÂ∞èÂåñÈòªÂäõÂØπ‰∫éÂÆûÁé∞È´òÊïàÈ£ûË°åËá≥ÂÖ≥ÈáçË¶Å„ÄÇ\\n\\n**2. ‰ºØÂä™Âà©ÂÆöÁêÜÂíåÂçáÂäõ**\\n\\n‰ºØÂä™Âà©ÂÆöÁêÜÊòØÊµÅ‰ΩìÂä®ÂäõÂ≠¶‰∏≠ÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºåÂØπ‰∫éÁêÜËß£ÂçáÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇËØ•ÂÆöÁêÜÊåáÂá∫ÔºåÂΩìÊµÅ‰ΩìÔºàÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÊòØÁ©∫Ê∞îÔºâÁöÑÈÄüÂ∫¶Â¢ûÂä†Êó∂ÔºåÂÖ∂ÂéãÂäõÈôç‰Ωé„ÄÇÈ£ûÊú∫Êú∫ÁøºÈááÁî®ÁøºÂûãÂΩ¢Áä∂Ôºå‰ΩøÁ©∫Ê∞îÂú®Êú∫Áøº‰∏äÊñπÁßªÂä®ÂæóÊØî‰∏ãÊñπÂø´„ÄÇËøôÂØºËá¥‰∏äË°®Èù¢ÁöÑÂéãÂäõÈôç‰ΩéÔºå‰∏ãË°®Èù¢ÁöÑÂéãÂäõÂçáÈ´òÔºå‰∫ßÁîüÂçáÂäõ„ÄÇ\\n\\n**3. ÁâõÈ°øËøêÂä®ÂÆöÂæãÂíåÈ£ûË°å**\\n\\nËâæËê®ÂÖã¬∑ÁâõÈ°øÁàµÂ£´ÁöÑËøêÂä®ÂÆöÂæãÂú®Ëß£Èáä‰ΩúÁî®‰∫éÈ£ûÊú∫ÁöÑÂäõÈáèÊñπÈù¢‰πüËµ∑ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇ\\n\\n3.1. ÁâõÈ°øÁ¨¨‰∏ÄÂÆöÂæãÔºàÊÉØÊÄßÔºâÔºöÈùôÊ≠¢ÁöÑÁâ©‰ΩìÂ∞Ü‰øùÊåÅÈùôÊ≠¢ÔºåËøêÂä®ÁöÑÁâ©‰ΩìÂ∞ÜÁªßÁª≠‰ª•ÊÅíÂÆöÁöÑÈÄüÂ∫¶ËøêÂä®ÔºåÈô§ÈùûÂèóÂà∞Â§ñÂäõÁöÑ‰ΩúÁî®„ÄÇÂΩìÂàÜÊûêÂçáÂäõ‰∫ßÁîüÊó∂ÔºåËøô‰∏™ÂÆöÂæãÈÄÇÁî®‰∫éÈ£ûË°åÔºåÂÖ∂‰∏≠Á©∫Ê∞îÊµÅÂä®ÊñπÂêëÊîπÂèòÔºåÂØºËá¥Âêë‰∏äÁöÑÂäõÔºàÂçáÂäõÔºâ„ÄÇ\\n\\n3.2. ÁâõÈ°øÁ¨¨‰∫åÂÆöÂæãÔºàÂäõ=Ë¥®Èáè√óÂä†ÈÄüÂ∫¶ÔºâÔºöÁâ©‰ΩìÁöÑÂä†ÈÄüÂ∫¶‰∏éÊñΩÂä†ÁöÑÂäõÊàêÊ≠£ÊØîÔºå‰∏éÂÖ∂Ë¥®ÈáèÊàêÂèç\r\n",
      "        ],\r\n",
      "        [\r\n",
      "            \"Âü∫Á°ÄÁâ©ÁêÜÂéüÁêÜÊîØÊíëÁùÄËà™Á©∫ÂíåÈ£ûË°åÔºåÂåÖÊã¨ÂçáÂäõ„ÄÅÊé®Âäõ„ÄÅÈáçÈáèÂíåÈòªÂäõ„ÄÇËøô‰∫õÂéüÁêÜÂèóÂà∞‰ºØÂä™Âà©ÂÆöÂæã„ÄÅÁâõÈ°øËøêÂä®ÂÆöÂæã‰ª•ÂèäÁ©∫Ê∞îÁâπÊÄßÁ≠âÂ§öÁßçÁßëÂ≠¶Ê¶ÇÂøµÁöÑÂΩ±Âìç„ÄÇË¶ÅÂÆûÁé∞ÂçáÂäõÔºåÈ£ûÊú∫ÈúÄË¶ÅÂú®ÂÖ∂‰∏äÊñπÊúâ‰∏Ä‰∏™ÊØîÂú∞ÁêÉÂºïÂäõÂêë‰∏ãÁöÑÂäõÊõ¥Â§ßÁöÑÂêë‰∏äÂäõ„ÄÇËøôÁßçÂêë‰∏äÂäõÊòØÈÄöËøáÊú∫Áøº‰∏éÁ©∫Ê∞îÁõ∏‰∫í‰ΩúÁî®ËÄå‰∫ßÁîüÁöÑ„ÄÇÊé®ÂäõÂàôÁî±ËøûÊé•Âà∞È£ûÊú∫ÁöÑÂèëÂä®Êú∫‰∫ßÁîüÔºåÈÄ†Êàê‰∏Ä‰∏™ÂêëÂâçÁöÑÂäõÔºå‰ΩøÈ£ûÊú∫Âú®Á©∫‰∏≠ÁßªÂä®„ÄÇËÄåÈáçÈáèÂàôÊòØ‰∏ÄÁßçÂ∞ÜÁâ©‰ΩìÂêëÂú∞ÁêÉ‰∏≠ÂøÉÊãâÁöÑÂäõ„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞È£ûË°åÔºåÈ£ûÊú∫ÁöÑÂêë‰∏äÂäõÂøÖÈ°ªÂ§ß‰∫éÂºïÂäõÁöÑÂêë‰∏ãÂäõ„ÄÇ\\n\\nÈ£ûÊú∫ÁöÑÈ£ûË°åËÉΩÂäõÈÄöËøáÊäÄÊúØÂàõÊñ∞„ÄÅÁ†îÁ©∂ÂíåÂÆûÈ™åÁöÑÁªìÂêàÂæóÂà∞‰∫ÜÊèêÈ´ò„ÄÇ‰ªé19‰∏ñÁ∫™Êú´ÁöÑËé±ÁâπÈ£ûË°åÂô®Âà∞21‰∏ñÁ∫™ÁöÑÊ≥¢Èü≥737 MAXÔºåÈ£ûÊú∫ÁöÑÊºîÂèòÊòØÁî±‰∫éÂ¢ûÂä†ÈÄüÂ∫¶„ÄÅÂÆπÈáèÂíåÂÆâÂÖ®ÊÄßÁöÑÊÑøÊúõÊé®Âä®ÁöÑ„ÄÇÂñ∑Ê∞îÂèëÂä®Êú∫„ÄÅÊ∂°ÊâáÂèëÂä®Êú∫‰ª•ÂèäÂ§çÂêàÊùêÊñôÁöÑÂºïÂÖ•‰ΩøÈ£ûÊú∫ÁöÑÈÄüÂ∫¶Êõ¥Âø´„ÄÅÁáÉÊ≤πÊõ¥ËäÇÁ∫¶Ôºå‰πüÊõ¥ÂÆâÂÖ®„ÄÇ\\n\\nÊ≠§Â§ñÔºåÁâ©ÁêÜÂ≠¶Âú®Ëà™Á©∫Âô®ËÆæËÆ°‰∏≠ÂèëÊå•‰∫ÜÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÈÄöËøáÁ©∫Ê∞îÂä®ÂäõÂ≠¶„ÄÅÊµÅ‰ΩìÂäõÂ≠¶ÂíåÁªìÊûÑËÆæËÆ°ÁöÑËøõÊ≠•„ÄÇËøô‰∫õÊ¶ÇÂøµ‰øÉËøõ‰∫ÜÁ©∫Ê∞îÂä®ÂäõÂ≠¶Êú∫ÁøºË°®Èù¢„ÄÅËäÇËÉΩÂñ∑Ê∞îÂèëÂä®Êú∫ÂíåËΩªÈáè„ÄÅÂùöÂõ∫ÁöÑÈ£ûÊú∫ÁªìÊûÑÁöÑÂèëÂ±ï„ÄÇÂ¶Ç‰ªäÔºåÁ†îÁ©∂Ê≠£Âú®ËøõË°å‰∏≠Ôºå‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÈ£ûÊú∫ÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºåÊó®Âú®ÂàõÈÄ†Âá∫ÈÄüÂ∫¶ÊúÄÂø´„ÄÅÊúÄÂèØÊåÅÁª≠‰∏îÊúÄÂÆâÂÖ®ÁöÑ‰∫§ÈÄöÊñπÂºè„ÄÇ\"\r\n",
      "        ]\r\n",
      "    ],\r\n",
      "    \"sort\": [\r\n",
      "        1,\r\n",
      "        0\r\n",
      "    ]\r\n",
      "}\r\n",
      "{\r\n",
      "    \"src\": [\r\n",
      "        \"Part 1. Definition\\nÂú®Ëøô‰∏™‰ªªÂä°‰∏≠Ôºå‰Ω†‰ºöÂæóÂà∞‰∏Ä‰∫õÊé®ÁâπÂ∏ñÂ≠ê„ÄÇ‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†áËÆ∞Â∏ñÂ≠ê‰∏≠Áî®Êà∑Ë°®ËææÁöÑÊÉÖÊÑüÔºåÂ¶ÇÊÇ≤‰º§„ÄÅÂñúÊÇ¶„ÄÅÁà±„ÄÅÊÑ§ÊÄí„ÄÅÊÅêÊÉßÊàñÊÉäËÆ∂„ÄÇ\\nPart 2. Example\\nim feeling quite sad and sorry for myself but ill snap out of it soon\\nÁ≠îÊ°àÔºöÊÇ≤‰º§\\nËß£ÈáäÔºöÂ∏ñÂ≠ê‰∏≠ÁöÑÊÉÖÊÑüÊòØÊÇ≤‰º§ÔºåÂõ†‰∏∫Áî®Êà∑ÊèêÂà∞‚ÄúÊÑüËßâÁõ∏ÂΩìÊÇ≤‰º§Âíå‰∏∫Ëá™Â∑±ÊÑüÂà∞Êä±Ê≠â‚Äù„ÄÇÂõ†Ê≠§ÔºåÊ†áÁ≠æÊòØ‚ÄúÊÇ≤‰º§‚Äù„ÄÇ\\nPart 3. Exercise\\nim feeling morose as i tend to do when im awake and writing here at almost am\\nÁ≠îÊ°àÔºö\"\r\n",
      "    ],\r\n",
      "    \"tgt\": [],\r\n",
      "    \"response\": [\r\n",
      "        [\r\n",
      "            \"ÊÇ≤‰º§\\nËß£ÈáäÔºöËøôÁØáÂ∏ñÂ≠êÁöÑÊÉÖÁª™ÊòØÊÇ≤‰º§ÔºåÂõ†‰∏∫Áî®Êà∑ÊèêÂà∞‚ÄúÊÑüÂà∞ÂøßÈÉÅ‚ÄùÔºåËøôÊòØÊÇ≤‰º§Êàñ‰∏çÂø´‰πêÁöÑÂêå‰πâËØç„ÄÇ‰ªñ‰ª¨Âú®Ê∏ÖÈÜíÁöÑÊó∂ÂÄôÂÜô‰ΩúÂπ∂Ë°®ËææËøôÁßçÊÉÖÁª™ÔºåËøôË°®Êòé‰∫Ü‰∏ÄÁßçÊ∂àÊûÅÁöÑÊÉÖÁª™Áä∂ÊÄÅ„ÄÇÂõ†Ê≠§ÔºåÊ†áÁ≠æÊòØ‚ÄúÊÇ≤‰º§‚Äù„ÄÇ\"\r\n",
      "        ],\r\n",
      "        [\r\n",
      "            \"ÊÇ≤‰º§\\n\\nËß£ÈáäÔºöËøôÁØáÂ∏ñÂ≠êÁöÑÊÉÖÁª™ÊòØÊÇ≤‰º§ÔºåÂõ†‰∏∫Áî®Êà∑ËØ¥Ëá™Â∑±‚ÄúÊÑüÂà∞ÈÉÅÈó∑‚ÄùÂíå‚ÄúÂá†‰πéÂáåÊô®ËøòÊ∏ÖÈÜíÁùÄÂú®ËøôÈáåÂÜô‚ÄùÔºåËøôË°®Êòé‰ªñ‰ª¨Ê≠£Âú®ÁªèÂéÜÊÇ≤‰º§„ÄÇ\"\r\n",
      "        ]\r\n",
      "    ],\r\n",
      "    \"sort\": [\r\n",
      "        1,\r\n",
      "        0\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming you are in the root directory of the ERNIE project\n",
    "dpo_train_file = \"./examples/data/dpo-train.jsonl\"\n",
    "\n",
    "with open(dpo_train_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 2:\n",
    "            sample = json.loads(line.strip())\n",
    "            print(json.dumps(sample, indent=4, ensure_ascii=False))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print out the first two samples in the dataset, helping us understand its specific structure and content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start DPO Training\n",
    "\n",
    "Now that we have prepared the environment, model, and data, it is time to dive into the core of DPO training. This chapter will first analyze DPO in depth from a theoretical perspective, and then provide detailed guidance on how to use the powerful features of ERNIE Kit to configure and start training tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 In-depth Analysis of DPO Principles\n",
    "\n",
    "Understanding how DPO works helps us better adjust parameters and analyze results. The core idea of DPO is to **directly convert human preferences into an optimizable loss function**, thereby bypassing the complex reward model training and reinforcement learning process in RLHF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 From Preference to Policy: The Core Idea of DPO\n",
    "\n",
    "1.  **Implicit Reward Model**: DPO cleverly assumes that there is an unknown, ideal reward model `r(x, y)` that can score any ‚Äúprompt-response‚Äù pair `(x, y)`. Human preference data (i.e., which response is better) reflects the outcomes of this reward model. Specifically, if for prompt `x`, response `y_w` (chosen) is preferred over `y_l` (rejected), then we assume `r(x, y_w) > r(x, y_l)`.\n",
    "\n",
    "2.  **Bradley-Terry model**: DPO borrows the Bradley-Terry model to model this preference probability. The model states that the probability `p*(y_w > y_l | x)` that humans prefer `y_w` over `y_l` can be expressed as:\n",
    "\n",
    "    $p^*(y_w > y_l | x) = \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_w)) + \\exp(r(x, y_l))} = \\sigma(r(x, y_w) - r(x, y_l))$\n",
    "   \n",
    "\n",
    "    where `œÉ` is the Sigmoid function. This formula intuitively represents that the greater the difference in reward scores between two answers, the closer the probability of human preference for one of them approaches 1.\n",
    "\n",
    "3.  **Connecting the language model with the reward**: This is the most ingenious step in DPO. It establishes an analytical relationship between the language model policy `œÄ_Œ∏` (i.e., the model we are training) and the aforementioned implicit reward model `r`. Through a series of derivations, it can be proven that the reward function can be expressed as:\n",
    "\n",
    "    $r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$\n",
    "\n",
    "    Here:\n",
    "    * `œÄ_Œ∏(y|x)` is the probability that the current policy model generates the response `y`.\n",
    "    * `œÄ_ref(y|x)` is the probability that the reference model (typically the model after SFT) generates the response `y`.\n",
    "    * `Œ≤` is a hyperparameter that controls the divergence (degree of difference) between the policy model and the reference model. It balances the two objectives of ‚Äúaccommodating preferences‚Äù and ‚Äúmaintaining language capability.‚Äù\n",
    "\n",
    "#### 4.1.2 DPO Loss Function: Intuitive Understanding and Mathematical Expression\n",
    "\n",
    "By substituting the above reward function expression into the Bradley-Terry model, we can represent human preference probabilities using language model probabilities. Then, by maximizing the log-likelihood of these preference probabilities, we ultimately obtain the DPO loss function:\n",
    "\n",
    "$-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left( \\beta \\log \\frac{\\pi_\\theta\\left(y_w|x\\right)}{\\pi_{\\text{ref}}\\left(y_w|x\\right)} - \\beta \\log \\frac{\\pi_\\theta\\left(y_l|x\\right)}{\\pi_{\\text{ref}}\\left(y_l|x\\right)} \\right) \\right] = \\mathcal{L}_{\\text{DPO}}\\left(\\pi_\\theta; \\pi_{\\text{ref}}\\right)$\n",
    "\n",
    "\n",
    "**Intuitive understanding of this complex formula:**\n",
    "\n",
    "*   **Core objective**: The objective of the loss function is to maximize the difference between the model's preference for the `chosen` answer and its preference for the `rejected` answer.\n",
    "*   **How preference is measured**: The model's ‚Äúpreference‚Äù for a response is measured by the term `log(œÄ_Œ∏ / œÄ_ref)`. This indicates how much the current model's probability for this response has increased relative to the reference model.\n",
    "* **Training Process**: At each training step, the model adjusts the parameter `Œ∏` to maximize the value of `log(œÄ_Œ∏(y_w|x) / œÄ_ref(y_w|x))` while minimizing the value of `log(œÄ_Œ∏(y_l|x) / œÄ_ref(y_l|x))`. As a result, the difference between the two increases, and the value of `log œÉ(...)` also increases, ultimately reducing the overall loss `L_DPO`.\n",
    "\n",
    "**DPO vs. RLHF: Why is DPO superior?**\n",
    "\n",
    "*   **Simple and stable**: DPO merges the complex two-stage RLHF process (reward modeling + reinforcement learning) into a simple classification task with a new loss function. It does not require training an independent reward model and avoids the sampling and training instability issues in reinforcement learning.\n",
    "*   **Data-efficient**: DPO directly utilizes preference data, eliminating the need for extensive sampling from the policy model during training, as in PPO. This results in lower computational costs and higher data utilization.\n",
    "*   **Equivalent or better performance**: Multiple studies have shown that DPO can achieve performance equivalent to or even surpassing RLHF while being simpler and more cost-effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Starting DPO Training\n",
    "\n",
    "Next, we use the `erniekit` tool to start DPO training. `erniekit` is a convenient command-line toolkit provided by the ERNIE project that helps us easily manage tasks such as model training, evaluation, and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single card training command:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the DPO training task using the `--stage DPO` parameter. At the same time, we also configure core parameters such as `--model_name_or_path` (pre-trained model path), `--train_dataset_path` (training dataset), `--eval_dataset_path` (evaluation dataset), and `--output_dir` (model output path). Furthermore, hyperparameters such as `--max_seq_len` (maximum sequence length), `--learning_rate` (learning rate), and `--max_steps` (maximum training steps) are also set to precisely control the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note:\n",
    "\n",
    "According to the call stack, ernie/modeling_moe.py passes a tuple containing six elements to ernie/loss/dpo.py, which expects five elements. The extra parameter score_deltas causes an error. Therefore, we need to manually delete this parameter\n",
    "\n",
    "- In /home/aistudio/ERNIE-develop/ernie/modeling_moe.py, around line 1709\n",
    "- For a quick demonstration, I will reduce the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T04:04:33.297297Z",
     "iopub.status.busy": "2025-07-18T04:04:33.296967Z",
     "iopub.status.idle": "2025-07-18T04:06:41.292412Z",
     "shell.execute_reply": "2025-07-18T04:06:41.291650Z",
     "shell.execute_reply.started": "2025-07-18T04:04:33.297276Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 -----------  Configuration  ----------------------\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 auto_cluster_config: 0\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 auto_parallel_config: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 auto_tuner_json: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 devices: 0\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 elastic_level: -1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 elastic_timeout: 30\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 enable_gpu_log: True\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 gloo_port: 6767\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 host: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 ips: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 job_id: default\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 legacy: False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 log_dir: erniekit_dist_log\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 log_level: INFO\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 log_overwrite: False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 master: 127.0.0.1:8080\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 max_restart: 3\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 nnodes: 1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 nproc_per_node: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,160 rank: -1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 run_mode: collective\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 server_num: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 servers: \r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 sort_ip: False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 start_port: 6070\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 trainer_num: None\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 trainers: \r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 training_script: /home/aistudio/ERNIE-develop/erniekit/launcher.py\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 training_script_args: ['train', '--stage', 'DPO', '--model_name_or_path', '../data/models/30656/ERNIE-4.5-0.3B-Paddle', '--train_dataset_path', './examples/data/dpo-train.jsonl', '--eval_dataset_path', './examples/data/dpo-eval.jsonl', '--output_dir', './output/dpo_tutorial_checkpoint', '--max_seq_len', '8192', '--learning_rate', '5.0e-7', '--warmup_steps', '5', '--max_steps', '10', '--save_steps', '10', '--logging_steps', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '36', '--bf16', 'True', '--do_train', '--fp16_opt_level', 'O2']\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 with_gloo: 1\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 --------------------------------------------------\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,161 Job: default, mode collective, replicas 1[1:1], elastic False\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,163 Run Pod: bhnrxg, replicas 1, status ready\r\n",
      "LAUNCH INFO 2025-07-18 12:04:39,177 Watching Pod: bhnrxg, replicas 1, status running\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "\u001b[32m[2025-07-18 12:04:41,399] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,432] [ WARNING]\u001b[0m - Dpo training requires use_sparse_head_and_loss_fn=True. Set use_sparse_head_and_loss_fn to True\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,433] [ WARNING]\u001b[0m - LoRA does not support ref_model_update_steps. Set ref_model_update_steps to -1.\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m -      Model Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - add_tail_layers               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - continue_training             : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fine_tuning                   : LoRA\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_gate_detach_matmul       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_linear                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_rms_norm                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_rope                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_softmax_mask             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - fuse_swiglu                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_alpha                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_path                     : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_plus_scale               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - lora_rank                     : 8\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,433] [   DEBUG]\u001b[0m - model_name_or_path            : ../data/models/30656/ERNIE-4.5-0.3B-Paddle\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_aux_loss_lambda           : 1e-05\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_group                     : dummy\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_group_experts             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_multimodal_dispatch_use_allgather: v2-alltoall-unpad\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_orthogonal_loss_lambda    : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_use_aux_free              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_use_hard_gate             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - moe_z_loss_lambda             : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - no_recompute_layers           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - offload_recompute_inputs      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - pp_seg_method                 : layer:Ernie4_5_DecoderLayer|EmptyLayer\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - recompute_granularity         : full\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - recompute_use_reentrant       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - rslora                        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - rslora_plus                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - stage                         : DPO\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - tensor_parallel_output        : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_attn_mask_start_row_indices: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_flash_attention           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_fused_head_and_loss_fn    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_recompute_moe             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_sparse_flash_attn         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,434] [   DEBUG]\u001b[0m - use_sparse_head_and_loss_fn   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - virtual_pp_degree             : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m -       Data Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - buffer_size                   : 500\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - dataset_type                  : iterable\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - eval_dataset_path             : ./examples/data/dpo-eval.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - eval_dataset_prob             : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - eval_dataset_type             : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - greedy_intokens               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - in_tokens_batching            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - mask_out_eos_token            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - max_prompt_len                : 2048\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - max_seq_len                   : 8192\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - num_comparisons               : 6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - num_samples_each_epoch        : 6000000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - offline_dataset_path          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - random_shuffle                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - train_dataset_path            : ./examples/data/dpo-train.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,435] [   DEBUG]\u001b[0m - train_dataset_prob            : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - train_dataset_type            : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - use_cls                       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m -    DPOConfig Configuration Arguments    \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - _no_sync_in_gradient_accumulation: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - actscale_moving_rate          : 0.01\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - adam_beta1                    : 0.9\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - adam_beta2                    : 0.999\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - adam_epsilon                  : 1e-08\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - amp_custom_black_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - amp_custom_white_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - amp_master_grad               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - apply_hadamard                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - apply_online_actscale_step    : 200\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - attention_probs_dropout_prob  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - auto_parallel_resume_form_hybrid_parallel: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,436] [   DEBUG]\u001b[0m - batch_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - beta                          : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - bf16                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - bf16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - ckpt_quant_stage              : O0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - compute_type                  : bf16\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - context_parallel_degree       : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - count_trained_tokens          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - current_device                : gpu:0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - data_parallel_config          : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - data_parallel_degree          : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - data_parallel_rank            : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataloader_drop_last          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataloader_num_workers        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataloader_shuffle            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataset_rank                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - dataset_world_size            : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - ddp_find_unused_parameters    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - decay_steps                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - device                        : gpu\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - disable_ckpt_quant            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - disable_tqdm                  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,437] [   DEBUG]\u001b[0m - distributed_dataloader        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_eval                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_export                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_predict                    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - do_train                      : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - dpo_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - dpop_lambda                   : 50\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - dropout_warmup_steps          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - enable_auto_parallel          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - enable_zero_cost_checkpoint   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - eval_accumulation_steps       : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - eval_batch_size               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - eval_steps                    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - evaluation_strategy           : IntervalStrategy.NO\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_max_capacity           : 4294967296\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_min_capacity           : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - expert_tensor_parallel_degree : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - flash_device_save_steps       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - flatten_param_grads           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - force_reshard_pp              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - fp16                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,438] [   DEBUG]\u001b[0m - fp16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - fp16_opt_level                : O2\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - fp8_format_type               : hybrid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - fuse_sequence_parallel_allreduce: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - gradient_accumulation_steps   : 36\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - greater_is_better             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - hadamard_block_size           : 32\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - hidden_dropout_prob           : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - hybrid_parallel_topo_order    : pp_first\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - ignore_data_skip              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - ignore_load_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - ignore_save_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - label_names                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - label_smoothing               : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - layerwise_lr_decay_bound      : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - lazy_data_processing          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - learning_rate                 : 5e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - load_best_model_at_end        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - load_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - local_process_index           : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - local_rank                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - log_level                     : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - log_level_replica             : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,439] [   DEBUG]\u001b[0m - log_on_each_node              : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_dir                   : ./output/dpo_tutorial_checkpoint/runs/Jul18_12-04-41_jupyter-2553954-9384723\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_first_step            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_steps                 : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logging_strategy              : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - logical_process_index         : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - loss_type                     : sigmoid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - lr_end                        : 1e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - lr_scheduler_type             : SchedulerType.LINEAR\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_estimate_samples          : 100000.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_evaluate_steps            : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_grad_norm                 : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - max_steps                     : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - metric_for_best_model         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - metrics_output_path           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - min_lr                        : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - minimum_eval_times            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - multi_token_pred_lambda       : 0.3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - no_cuda                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - normalize_logps               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_cycles                    : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_nextn_predict_layers      : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_of_gpus                   : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,440] [   DEBUG]\u001b[0m - num_train_epochs              : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - offload_optim                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - offset_alpha                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - optim                         : OptimizerNames.ADAMW\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - optim_shard_num               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - optimizer_name_suffix         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - ordered_save_group_size       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - output_dir                    : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - output_signal_dir             : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - overwrite_output_dir          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pad_token_id                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - past_index                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pdc_download_ckpt             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pdc_download_timeout          : 300\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - per_device_eval_batch_size    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - per_device_train_batch_size   : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pipeline_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pipeline_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pipeline_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - power                         : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - prediction_loss_only          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - pref_loss_ratio               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,441] [   DEBUG]\u001b[0m - process_index                 : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - quant_input_grad              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - quant_weight_grad             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - recompute                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - ref_model_update_steps        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - reference_free                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - refined_recompute             : {}\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - release_grads                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - remove_unused_columns         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - report_to                     : ['visualdl']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - resume_from_checkpoint        : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - run_name                      : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_on_each_node             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_rng_states               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_sharding_stage1_model_include_freeze_params: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_steps                    : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_strategy                 : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_tokenizer                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - save_total_limit              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - scale_loss                    : 32768\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - seed                          : 42\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - sep_parallel_degree           : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,442] [   DEBUG]\u001b[0m - sequence_parallel             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sequence_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - server_tp_degree              : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sft_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sft_loss_ratio                : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding                      : []\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_comm_buffer_size_MB  : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_degree               : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_mesh_dimension: dp\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - sharding_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_load_dataset           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_load_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_log                    : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save_model_state       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save_model_with_tensor_fusion: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - should_save_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - simpo_gamma                   : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - skip_data_intervals           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - skip_memory_metrics           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,443] [   DEBUG]\u001b[0m - skip_profile_timer            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - split_inputs_sequence_dim     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - split_norm_comm               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensor_parallel_config        : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensor_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensor_parallel_rank          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - tensorwise_offload_optimizer  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - to_static                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - train_batch_size              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - unified_checkpoint            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - unified_checkpoint_config     : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_async_save                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_expert_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_hybrid_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_lowprecision_moment       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_recompute_mtp             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - use_sp_callback               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - wandb_api_key                 : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - wandb_http_proxy              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - warmup_ratio                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - warmup_steps                  : 5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - weight_decay                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,444] [   DEBUG]\u001b[0m - weight_name_suffix            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - weight_quantize_algo          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - world_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_ema_interval              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_pipeline_hooks_capacity_usage: 0.6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_save_ema_coef             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - zcc_workers_num               : 3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:41,445] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,445] [    INFO]\u001b[0m - The global seed is set to 42, local seed is set to 43 and random seed is set to 42.\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,445] [ WARNING]\u001b[0m - Process rank: -1, device: gpu, world_size: 1, distributed training: False, 16-bits training: True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,445] [    INFO]\u001b[0m - Start to load model ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,449] [    INFO]\u001b[0m - Loading configuration file ../data/models/30656/ERNIE-4.5-0.3B-Paddle/config.json\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 12:04:41,452] [ WARNING]\u001b[0m - You are using a model of type ernie4_5 to instantiate a model of type ernie4_5_moe. This is not supported for all configurations of models and can yield errors.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,454] [    INFO]\u001b[0m - Loading weights file ../data/models/30656/ERNIE-4.5-0.3B-Paddle/model.safetensors\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,748] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,749] [    INFO]\u001b[0m - change initializer-range from 0.02 to 0.018041293779826325\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,749] [    INFO]\u001b[0m - using moe-group: dummy\u001b[0m\r\n",
      "W0718 12:04:41.931790 259232 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.8, Runtime API Version: 12.6\r\n",
      "\u001b[32m[2025-07-18 12:04:41,934] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,937] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,939] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,942] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,944] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,946] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,949] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,951] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,953] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,955] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,957] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,960] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,962] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,964] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,966] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,969] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,971] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,974] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,978] [    INFO]\u001b[0m - output-weight:[103424, 1024] config.tie_word_embeddings=True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:41,978] [    INFO]\u001b[0m - Use fusedRMSNorm\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,068] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing Ernie4_5_MoeForCausalLM.\r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,068] [    INFO]\u001b[0m - All the weights of Ernie4_5_MoeForCausalLM were initialized from the model checkpoint at ../data/models/30656/ERNIE-4.5-0.3B-Paddle.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Ernie4_5_MoeForCausalLM for predictions without further training.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,070] [    INFO]\u001b[0m - Loading configuration file ../data/models/30656/ERNIE-4.5-0.3B-Paddle/generation_config.json\u001b[0m\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:250: UserWarning: using greedy search strategy. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:255: UserWarning: using greedy search strategy. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-18 12:04:42,082] [    INFO]\u001b[0m - Start to wrap model with LoRA config ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,092] [    INFO]\u001b[0m - Mark only lora and trainable_module as trainable.\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:42,095] [   DEBUG]\u001b[0m - Frozen parameters: 4.67e+08 || Trainable parameters:5.16e+05 || Total parameters:4.67e+08|| Trainable:0.11%\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:42,095] [    INFO]\u001b[0m - Wraping model with LoRA config successfully !\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,243] [    INFO]\u001b[0m - Loading model & tokenizer successfully !\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,243] [    INFO]\u001b[0m - Start to create dataset ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,248] [    INFO]\u001b[0m - ./examples/data/dpo-train.jsonl: task prob: 1.0, ori number of examples: 200, target_num_each_epoch: 6000000\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,248] [    INFO]\u001b[0m - Creating dataset successfully ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,248] [    INFO]\u001b[0m - The global seed is set to 42, local seed is set to 43 and random seed is set to 42.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,283] [    INFO]\u001b[0m - max_steps is given, it will override any value given in num_train_epochs\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,283] [    INFO]\u001b[0m - Using half precision\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m -     Training Configuration Arguments    \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - _no_sync_in_gradient_accumulation: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - actscale_moving_rate          : 0.01\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - adam_beta1                    : 0.9\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - adam_beta2                    : 0.999\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - adam_epsilon                  : 1e-08\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,298] [   DEBUG]\u001b[0m - amp_custom_black_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - amp_custom_white_list         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - amp_master_grad               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - apply_hadamard                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - apply_online_actscale_step    : 200\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - attention_probs_dropout_prob  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - auto_parallel_resume_form_hybrid_parallel: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - batch_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - beta                          : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - bf16                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - bf16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - ckpt_quant_stage              : O0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - compute_type                  : bf16\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - context_parallel_degree       : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - count_trained_tokens          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - current_device                : gpu:0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - data_parallel_config          : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - data_parallel_degree          : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - data_parallel_rank            : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataloader_drop_last          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataloader_num_workers        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataloader_shuffle            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,299] [   DEBUG]\u001b[0m - dataset_rank                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dataset_world_size            : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - ddp_find_unused_parameters    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - decay_steps                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - device                        : gpu\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - disable_ckpt_quant            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - disable_tqdm                  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - distributed_dataloader        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_eval                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_export                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_predict                    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - do_train                      : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dpo_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dpop_lambda                   : 50\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - dropout_warmup_steps          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - enable_auto_parallel          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - enable_zero_cost_checkpoint   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - eval_accumulation_steps       : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - eval_batch_size               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - eval_steps                    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - evaluation_strategy           : IntervalStrategy.NO\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - expert_max_capacity           : 4294967296\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - expert_min_capacity           : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,300] [   DEBUG]\u001b[0m - expert_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - expert_tensor_parallel_degree : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - flash_device_save_steps       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - flatten_param_grads           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - force_reshard_pp              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp16                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp16_opt_level                : O2\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fp8_format_type               : hybrid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - fuse_sequence_parallel_allreduce: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - gradient_accumulation_steps   : 36\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - greater_is_better             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - hadamard_block_size           : 32\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - hidden_dropout_prob           : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - hybrid_parallel_topo_order    : pp_first\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - ignore_data_skip              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - ignore_load_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - ignore_save_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - label_names                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - label_smoothing               : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - layerwise_lr_decay_bound      : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - lazy_data_processing          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,301] [   DEBUG]\u001b[0m - learning_rate                 : 5e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - load_best_model_at_end        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - load_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - local_process_index           : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - local_rank                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - log_level                     : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - log_level_replica             : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - log_on_each_node              : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_dir                   : ./output/dpo_tutorial_checkpoint/runs/Jul18_12-04-41_jupyter-2553954-9384723\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_first_step            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_steps                 : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logging_strategy              : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - logical_process_index         : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - loss_type                     : sigmoid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - lr_end                        : 1e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - lr_scheduler_type             : SchedulerType.LINEAR\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_estimate_samples          : 100000.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_evaluate_steps            : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_grad_norm                 : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - max_steps                     : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - metric_for_best_model         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - metrics_output_path           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - min_lr                        : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,302] [   DEBUG]\u001b[0m - minimum_eval_times            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - multi_token_pred_lambda       : 0.3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - no_cuda                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - normalize_logps               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_cycles                    : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_nextn_predict_layers      : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_of_gpus                   : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - num_train_epochs              : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - offload_optim                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - offset_alpha                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - optim                         : OptimizerNames.ADAMW\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - optim_shard_num               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - optimizer_name_suffix         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - ordered_save_group_size       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - output_dir                    : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - output_signal_dir             : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - overwrite_output_dir          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pad_token_id                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - past_index                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pdc_download_ckpt             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pdc_download_timeout          : 300\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - per_device_eval_batch_size    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - per_device_train_batch_size   : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,303] [   DEBUG]\u001b[0m - pipeline_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - pipeline_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - pipeline_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - power                         : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - prediction_loss_only          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - pref_loss_ratio               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - process_index                 : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - quant_input_grad              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - quant_weight_grad             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - recompute                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - ref_model_update_steps        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - reference_free                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - refined_recompute             : {}\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - release_grads                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - remove_unused_columns         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - report_to                     : ['visualdl']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - resume_from_checkpoint        : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - run_name                      : ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_on_each_node             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_rng_states               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_sharding_stage1_model_include_freeze_params: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_steps                    : 10\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,304] [   DEBUG]\u001b[0m - save_strategy                 : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - save_tokenizer                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - save_total_limit              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - scale_loss                    : 32768\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - seed                          : 42\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sep_parallel_degree           : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sequence_parallel             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sequence_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - server_tp_degree              : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sft_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sft_loss_ratio                : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding                      : []\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_comm_buffer_size_MB  : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_degree               : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_mesh_dimension: dp\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - sharding_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_load_dataset           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_load_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_log                    : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_save                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,305] [   DEBUG]\u001b[0m - should_save_model_state       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - should_save_model_with_tensor_fusion: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - should_save_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - simpo_gamma                   : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - skip_data_intervals           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - skip_memory_metrics           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - skip_profile_timer            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - split_inputs_sequence_dim     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - split_norm_comm               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensor_parallel_config        : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensor_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensor_parallel_rank          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - tensorwise_offload_optimizer  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - to_static                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - train_batch_size              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - unified_checkpoint            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - unified_checkpoint_config     : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_async_save                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_expert_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_hybrid_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_lowprecision_moment       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_recompute_mtp             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - use_sp_callback               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,306] [   DEBUG]\u001b[0m - wandb_api_key                 : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - wandb_http_proxy              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - warmup_ratio                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - warmup_steps                  : 5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - weight_decay                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - weight_name_suffix            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - weight_quantize_algo          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - world_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_ema_interval              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_pipeline_hooks_capacity_usage: 0.6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_save_ema_coef             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - zcc_workers_num               : 3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,307] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,307] [    INFO]\u001b[0m - Starting training from resume_from_checkpoint : None\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m - [timelog] checkpoint loading time: 0.00s (2025-07-18 12:04:43) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m - ***** Running training *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Num examples = 360\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Num Epochs = 9223372036854775807\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Instantaneous batch size per device = 1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Total train batch size (w. parallel, distributed & accumulation) = 36\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Gradient Accumulation steps = 36\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Total optimization steps = 10\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:04:43,309] [    INFO]\u001b[0m -   Total num train samples = 360\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 12:04:43,310] [   DEBUG]\u001b[0m -   Number of trainable parameters = 516,096 (per device)\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:   0%|          | 0/10 [00:00<?, ?it/s]W0718 12:04:50.665277 259232 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.\r\n",
      "\r\n",
      "TrainProcess:  10%|‚ñà         | 1/10 [00:18<02:44, 18.24s/it]\u001b[32m[2025-07-18 12:05:01,567] [    INFO]\u001b[0m - loss: 0.69314718, learning_rate: 1e-07, global_step: 1, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 55.92091178894043, max_memory_allocated: 11.65785264968872, max_memory_reserved: 55.92091178894043, interval_runtime: 18.2493, interval_samples_per_second: 1.9727, interval_steps_per_second: 0.0548, rewards/chosen: 0.0, rewards/rejected: 0.0, rewards/accuracies: 0.0, rewards/margins: 0.0, logps/rejected: -593.4622802734375, logps/chosen: -558.8125610351562, sigmoid_loss: 0.6931471824645996, sft_loss: 0.0, progress_or_epoch: 0.1\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  20%|‚ñà‚ñà        | 2/10 [00:28<01:49, 13.72s/it]\u001b[32m[2025-07-18 12:05:12,115] [    INFO]\u001b[0m - loss: 0.69314718, learning_rate: 2e-07, global_step: 2, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 55.92091178894043, max_memory_allocated: 11.65785264968872, max_memory_reserved: 55.92091178894043, interval_runtime: 10.5483, interval_samples_per_second: 3.4129, interval_steps_per_second: 0.0948, rewards/chosen: 0.0, rewards/rejected: 0.0, rewards/accuracies: 0.0, rewards/margins: 0.0, logps/rejected: -561.4170532226562, logps/chosen: -552.60888671875, sigmoid_loss: 0.6931471824645996, sft_loss: 0.0, progress_or_epoch: 0.2\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  30%|‚ñà‚ñà‚ñà       | 3/10 [00:39<01:27, 12.43s/it]\u001b[32m[2025-07-18 12:05:23,022] [    INFO]\u001b[0m - loss: 0.69692159, learning_rate: 3e-07, global_step: 3, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 55.92091178894043, max_memory_allocated: 11.65785264968872, max_memory_reserved: 55.92091178894043, interval_runtime: 10.9074, interval_samples_per_second: 3.3005, interval_steps_per_second: 0.0917, rewards/chosen: 0.0004406546941027045, rewards/rejected: -0.000714117253664881, rewards/accuracies: 0.5167573690414429, rewards/margins: 0.0011547724716365337, logps/rejected: -565.6019897460938, logps/chosen: -536.885986328125, sigmoid_loss: 0.6969215273857117, sft_loss: 0.0, progress_or_epoch: 0.3\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:50<01:09, 11.65s/it]\u001b[32m[2025-07-18 12:05:33,480] [    INFO]\u001b[0m - loss: 0.69113737, learning_rate: 4e-07, global_step: 4, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.4572, interval_samples_per_second: 3.4426, interval_steps_per_second: 0.0956, rewards/chosen: 0.006543044466525316, rewards/rejected: -0.004613205324858427, rewards/accuracies: 0.5185551643371582, rewards/margins: 0.011156250722706318, logps/rejected: -548.4920654296875, logps/chosen: -541.0109252929688, sigmoid_loss: 0.6911373734474182, sft_loss: 0.0, progress_or_epoch: 0.4\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [01:00<00:56, 11.32s/it]\u001b[32m[2025-07-18 12:05:44,199] [    INFO]\u001b[0m - loss: 0.69389129, learning_rate: 5e-07, global_step: 5, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.7196, interval_samples_per_second: 3.3583, interval_steps_per_second: 0.0933, rewards/chosen: 0.005858992226421833, rewards/rejected: -0.002096320502460003, rewards/accuracies: 0.5291945338249207, rewards/margins: 0.007955312728881836, logps/rejected: -571.0379638671875, logps/chosen: -564.6283569335938, sigmoid_loss: 0.6938911080360413, sft_loss: 0.0, progress_or_epoch: 0.5\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:11<00:44, 11.06s/it]\u001b[32m[2025-07-18 12:05:54,770] [    INFO]\u001b[0m - loss: 0.69585663, learning_rate: 4e-07, global_step: 6, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.5706, interval_samples_per_second: 3.4057, interval_steps_per_second: 0.0946, rewards/chosen: 0.002569297095760703, rewards/rejected: -0.001821780577301979, rewards/accuracies: 0.5273837447166443, rewards/margins: 0.004391078371554613, logps/rejected: -561.6737060546875, logps/chosen: -544.4690551757812, sigmoid_loss: 0.6958566308021545, sft_loss: 0.0, progress_or_epoch: 0.6\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [01:22<00:32, 10.90s/it]\u001b[32m[2025-07-18 12:06:05,324] [    INFO]\u001b[0m - loss: 0.70019507, learning_rate: 3e-07, global_step: 7, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.5534, interval_samples_per_second: 3.4112, interval_steps_per_second: 0.0948, rewards/chosen: -0.006452502217143774, rewards/rejected: -0.0021812929771840572, rewards/accuracies: 0.5088156461715698, rewards/margins: -0.0042712087742984295, logps/rejected: -511.5072937011719, logps/chosen: -512.0642700195312, sigmoid_loss: 0.7001951336860657, sft_loss: 0.0, progress_or_epoch: 0.7\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [01:32<00:21, 10.85s/it]\u001b[32m[2025-07-18 12:06:16,074] [    INFO]\u001b[0m - loss: 0.70007676, learning_rate: 2e-07, global_step: 8, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.751, interval_samples_per_second: 3.3485, interval_steps_per_second: 0.093, rewards/chosen: 0.003911859355866909, rewards/rejected: 0.009731447324156761, rewards/accuracies: 0.49071377515792847, rewards/margins: -0.005819589830935001, logps/rejected: -575.8965454101562, logps/chosen: -557.0797729492188, sigmoid_loss: 0.7000767588615417, sft_loss: 0.0, progress_or_epoch: 0.8\u001b[0m\r\n",
      "\r\n",
      "TrainProcess:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [01:43<00:10, 10.82s/it]\u001b[32m[2025-07-18 12:06:26,816] [    INFO]\u001b[0m - loss: 0.70192665, learning_rate: 1e-07, global_step: 9, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.7402, interval_samples_per_second: 3.3519, interval_steps_per_second: 0.0931, rewards/chosen: 0.0030782907269895077, rewards/rejected: 0.013875698670744896, rewards/accuracies: 0.4840790331363678, rewards/margins: -0.010797406546771526, logps/rejected: -542.80224609375, logps/chosen: -535.5034790039062, sigmoid_loss: 0.701926589012146, sft_loss: 0.0, progress_or_epoch: 0.9\u001b[0m\r\n",
      "\r\n",
      "TrainProcess: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 10.81s/it]\u001b[32m[2025-07-18 12:06:37,607] [    INFO]\u001b[0m - loss: 0.69396645, learning_rate: 0.0, global_step: 10, current_memory_allocated: 0.6796383857727051, current_memory_reserved: 61.84350395202637, max_memory_allocated: 11.707652807235718, max_memory_reserved: 61.84350395202637, interval_runtime: 10.7926, interval_samples_per_second: 3.3356, interval_steps_per_second: 0.0927, rewards/chosen: -0.0035317859146744013, rewards/rejected: -0.01574733480811119, rewards/accuracies: 0.5068547129631042, rewards/margins: 0.012215547263622284, logps/rejected: -582.034912109375, logps/chosen: -574.9534912109375, sigmoid_loss: 0.6939664483070374, sft_loss: 0.0, progress_or_epoch: 1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,608] [    INFO]\u001b[0m - Saving model checkpoint to ./output/dpo_tutorial_checkpoint/checkpoint-10\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,611] [    INFO]\u001b[0m - tokenizer config file saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/tokenizer_config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,612] [    INFO]\u001b[0m - Special tokens file saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/special_tokens_map.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,612] [    INFO]\u001b[0m - added tokens file saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/added_tokens.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,625] [    INFO]\u001b[0m - Configuration saved in ./output/dpo_tutorial_checkpoint/checkpoint-10/config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,628] [    INFO]\u001b[0m - Saving optimizer files.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,646] [    INFO]\u001b[0m - [timelog] checkpoint saving time: 0.04s (2025-07-18 12:06:37) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,647] [    INFO]\u001b[0m - \r\n",
      "Training completed. \r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,647] [    INFO]\u001b[0m - train_runtime: 114.3367, train_samples_per_second: 3.1486, train_steps_per_second: 0.0875, train_loss: 0.6960266172885895, progress_or_epoch: 1.0\u001b[0m\r\n",
      "\r\n",
      "TrainProcess: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:54<00:00, 11.43s/it]\r\n",
      "\u001b[32m[2025-07-18 12:06:37,647] [    INFO]\u001b[0m - Saving model checkpoint to ./output/dpo_tutorial_checkpoint\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,651] [    INFO]\u001b[0m - tokenizer config file saved in ./output/dpo_tutorial_checkpoint/tokenizer_config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,652] [    INFO]\u001b[0m - Special tokens file saved in ./output/dpo_tutorial_checkpoint/special_tokens_map.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,653] [    INFO]\u001b[0m - added tokens file saved in ./output/dpo_tutorial_checkpoint/added_tokens.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m - Configuration saved in ./output/dpo_tutorial_checkpoint/config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m - ***** train metrics *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   progress_or_epoch        =        1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_loss               =      0.696\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_runtime            = 0:01:54.33\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_samples_per_second =     3.1486\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 12:06:37,664] [    INFO]\u001b[0m -   train_steps_per_second   =     0.0875\u001b[0m\r\n",
      "LAUNCH INFO 2025-07-18 12:06:40,304 Pod completed\r\n",
      "LAUNCH INFO 2025-07-18 12:06:40,304 Exit code 0\r\n"
     ]
    }
   ],
   "source": [
    "!erniekit train \\\n",
    "    --stage DPO \\\n",
    "    --model_name_or_path ../data/models/30656/ERNIE-4.5-0.3B-Paddle \\\n",
    "    --train_dataset_path ./examples/data/dpo-train.jsonl \\\n",
    "    --eval_dataset_path ./examples/data/dpo-eval.jsonl \\\n",
    "    --output_dir ./output/dpo_tutorial_checkpoint \\\n",
    "    --max_seq_len 8192 \\\n",
    "    --learning_rate 5.0e-7 \\\n",
    "    --warmup_steps 5 \\\n",
    "    --max_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "    --logging_steps 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 36 \\\n",
    "    --bf16 True \\\n",
    "    --do_train \\\n",
    "    --fp16_opt_level O2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitoring the training process:**\n",
    "\n",
    "Once training begins, you will see a large amount of log output in the terminal. Pay attention to the following key information:\n",
    "\n",
    "*   **`loss`**: This is the DPO loss value. It should steadily decrease as training progresses.\n",
    "*   **`rewards/chosen` and `rewards/rejected`**: These are the average implicit reward scores for chosen and rejected answers. You should observe the value of `rewards/chosen` gradually increasing, while the value of `rewards/rejected` gradually decreases or remains at a low level.\n",
    "*   **`rewards/accuracies`**: This indicates the proportion of samples in a batch where the model correctly assigns higher rewards to `chosen` responses than to `rejected` responses. This value should tend toward 1.0.\n",
    "* **`rewards/margins`**: This is the average difference between `rewards/chosen` and `rewards/rejected`. The larger this value, the stronger the model's ability to distinguish between good and bad answers.\n",
    "* **`eval_loss`**: At each evaluation step (`eval_steps`), the loss is calculated on the validation set. Observing this value can help you determine if the model is overfitting.\n",
    "\n",
    "After training is complete, the final model weights and configuration file will be saved in the directory specified by `output_dir` (e.g., `./output/ERNIE-4.5-0.3B-dpo`). The model in this directory is the final product of our successful DPO, which now better understands human preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Inference\n",
    "\n",
    "After DPO training, our model theoretically understands human preferences better. But ‚Äúthe proof is in the pudding.‚Äù This chapter will guide you on how to scientifically evaluate the model's effectiveness and deploy it to actual interactive inference tasks to intuitively experience its performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 DPO Model Evaluation\n",
    "\n",
    "DPO evaluation differs from traditional metrics such as accuracy and F1 score, as it focuses more on measuring **generation quality and preference alignment**. The core of the evaluation is to determine whether the content generated by the model is more in line with our expectations (e.g., safer, more detailed, more creative, etc.) than that generated by the SFT model when faced with the same prompt.\n",
    "\n",
    "#### 5.1.1 Evaluation Strategy\n",
    "\n",
    "1.  **Construct a high-quality evaluation set**:\n",
    "*   Designing a specialized evaluation set containing various challenging prompts is crucial. These prompts should be able to stimulate the model's capabilities in different dimensions, such as:\n",
    "*   **Safety**: Include some edge cases that may induce the model to generate unsafe or biased responses.\n",
    "        *   **Instruction Compliance**: Design prompts with complex, multi-step, or constrained instructions.\n",
    "*   **Creativity and Open-Ended Questions**: Pose questions that require the model to exercise imagination or engage in deep thinking.\n",
    "*   **Factual and Knowledge-Based**: Include questions that require accurate knowledge reserves to answer.\n",
    "*   This evaluation set should not appear in the training or validation data to ensure the fairness of the evaluation.\n",
    "\n",
    "2.  **Comparative Evaluation (A/B Test)**:\n",
    "*   The most effective evaluation method is to conduct a **head-to-head** comparison. For each prompt in the evaluation set, generate responses using both the **SFT model** and the **DPO model**.\n",
    "    * Present the responses generated by the two models (anonymized and in random order) to human evaluators, who then judge which response is better, or if they are equivalent/both poor.  \n",
    "    * Collect a large number of evaluation results, calculate the win rate, draw rate, and loss rate of the DPO model, thereby quantifying its improvement relative to the SFT model.\n",
    "\n",
    "3.  **Automated evaluation (using a stronger model as a judge)**:  \n",
    "    *   When there are insufficient human resources for large-scale manual evaluation, stronger models (such as GPT-4, ERNIE-4.0) can be used as ‚Äújudges.‚Äù\n",
    "    * Design a referee prompt template to input user prompts, SFT model responses, and DPO model responses into the referee model, which then determines which is better and provides a rationale.  \n",
    "    * While automated evaluation cannot fully replace human assessment, it offers a scalable, cost-effective evaluation method that quickly provides an initial impression of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Outlook\n",
    "\n",
    "Congratulations! You have now completed this tutorial on using the ERNIE Kit to perform direct preference optimization (DPO) on the ERNIE-4.5-0.3B model. Let's review this journey and look ahead to future possibilities.\n",
    "\n",
    "### 6.1 Review of the Core Content of This Tutorial\n",
    "\n",
    "In this tutorial, we systematically learned and practiced the following core content:\n",
    "\n",
    "1.  **Core Principles of DPO**: We gained a deep understanding of how DPO cleverly transforms human preference data into a simple classification loss, thereby bypassing the complex reward modeling and reinforcement learning processes of RLHF, achieving more efficient and stable model alignment.\n",
    "\n",
    "2. **Environment and Code Preparation**: We set up a development environment based on PaddlePaddle and ERNIE Kit, and learned how to install ERNIE Kit from source code to access the latest features and maximum flexibility.\n",
    "\n",
    "3.  **Data Preparation and Analysis**: We mastered the `(prompt, chosen, rejected)` triplet data format required by DPO and used the `UltraFeedback Binarized` dataset as an example to download, unzip, and view data samples through code, gaining a deep understanding of the key elements of high-quality preference data.\n",
    "\n",
    "4.  **DPO Training Configuration and Execution**: We thoroughly reviewed the DPO configuration file in ERNIE Kit, particularly focusing on the understanding and setup of key hyperparameters such as `model_name_or_path` (must be an SFT model), `beta`, and `learning_rate`. We also learned how to initiate single-GPU and multi-GPU training via the command line and how to interpret key metrics in the training logs to monitor the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Limitations and Future Prospects of DPO\n",
    "\n",
    "Although DPO is powerful, it is not a silver bullet and has its limitations:\n",
    "\n",
    "*   **Dependence on preference data quality**: The effectiveness of DPO is highly dependent on the quality and consistency of preference data. Ambiguous, biased, or inconsistent data can seriously mislead the model's learning.\n",
    "*   **Sensitivity to Beta Values**: The choice of the hyperparameter `beta` significantly impacts the final results and requires careful tuning through experimentation.\n",
    "*   **Risk of Model Collapse**: Although more stable than RLHF, DPO may still lead to models sacrificing diversity and fluency in generation to overly accommodate preferences in certain scenarios.\n",
    "\n",
    "**Cutting-Edge DPO Variants and Future Directions:**\n",
    "\n",
    "Academia and industry are continuously exploring alignment algorithms superior to DPO. Once you have a deep understanding of DPO, consider the following cutting-edge directions:\n",
    "\n",
    "*   **IPO (Identity Preference Optimization)**: A variant of DPO that claims to better prevent model overfitting on preference data by modifying the loss function. ERNIE Kit already supports this loss.\n",
    "*   **KTO (Kahneman-Tversky Optimization)**: Inspired by human decision theory, KTO allows alignment using single samples labeled as ‚Äúgood‚Äù or ‚Äúbad,‚Äù eliminating the strict requirement for paired preference data and significantly lowering the data annotation threshold.\n",
    "*   **SimPO (Simple Preference Optimization)**: A more concise loss function design aimed at improving training efficiency and final performance.\n",
    "\n",
    "These new algorithms are continuously driving the development of large-scale model alignment technology, and they also foreshadow that we will have more and better tools in the future to enable AI to better serve humanity.\n",
    "\n",
    "### 6.3 Conclusion\n",
    "\n",
    "This tutorial has opened the door to the world of large-scale model preference alignment. True learning begins with practice. We strongly encourage you to:\n",
    "\n",
    "*   **Experiment with different hyperparameters**: Adjust parameters such as `beta` and `learning_rate` to observe their impact on model behavior.\n",
    "*   **Build your own dataset**: Attempt to create a small preference dataset tailored to your own business context or area of interest, and train it using the methods outlined in this tutorial.\n",
    "*   **Explore other models**: Apply the methods from this tutorial to other models supported by ERNIE Kit.\n",
    "\n",
    "Thank you for following this tutorial. We hope you continue to create smarter, more reliable AI that aligns with human values as you explore large models. Enjoy your exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback/Contact me: WeChat: G_Fuji"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
