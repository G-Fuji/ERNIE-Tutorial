{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIE-4.5-0.3B 中文情感分析实战教程\n",
    "\n",
    "## 1. 引言\n",
    "\n",
    "欢迎来到 ERNIE-4.5-0.3B 实战教程系列！在之前的学习中，我们已经掌握了预训练（PT）、监督微调（SFT）和直接偏好优化（DPO）等核心技术，这些构成了大模型开发的基础。现在，是时候将理论付诸实践，解决一个真实世界的问题了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 为什么选择情感分析作为第一个实战项目？\n",
    "\n",
    "理论学习好比学习内功心法，而实战项目则是修炼具体的武功招式。情感分析任务，就是我们选择的第一招。为什么是它？\n",
    "\n",
    "*   **承上启下，巩固SFT技能**：情感分析本质上是一个文本分类任务，是监督微调（SFT）最直接、最经典的应用场景。通过这个项目，我们可以将学到的SFT知识融会贯通，深刻理解如何将一个通用的预训练模型，调教成特定领域的专家。\n",
    "*   **需求广泛，商业价值巨大**：从分析用户评论、监控品牌口碑，到理解社会舆情，情感分析是无数AI应用不可或缺的基石。掌握它，就等于掌握了一项能创造巨大商业价值的核心技能。\n",
    "*   **简单直观，易于上手**：相比于复杂的生成任务或对话系统，情感分析的目标明确（判断正面/负面），结果易于评估（准确率），非常适合作为从理论到实践的“第一座桥梁”。\n",
    "\n",
    "因此，本教程将以中文情感分析为靶，带领您完整地走一遍“数据准备 -> 模型微调 -> 评估预测”的实战流程，真正将ERNIE-4.5-0.3B的强大能力，应用到解决实际问题中来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 什么是情感分析？\n",
    "\n",
    "情感分析，又称为意见挖掘 (Opinion Mining)，是利用自然语言处理、文本分析和计算语言学等方法，对带有情感色彩的主观性文本进行提取、分析、归纳和推理的过程。简单来说，它的目标就是**识别和判断一段文本所表达的情感是积极的（正面）、消极的（负面），还是中性的**。\n",
    "\n",
    "例如，给定一条用户评价：“这家餐厅的烤鸭味道绝了，环境也很棒！”，情感分析系统应该能判断出这是一条**积极**的评价。而对于“等了半个多小时才上菜，味道也很一般”，则应判断为**消极**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 情感分析的应用场景和项目意义\n",
    "\n",
    "情感分析技术在商业和研究领域都有着巨大的应用价值，是许多智能应用的基础模块。\n",
    "\n",
    "| 应用领域       | 具体场景                                                     | 项目意义                                       |\n",
    "| -------------- | ------------------------------------------------------------ | ---------------------------------------------- |\n",
    "| **商业智能**   | 舆情监控、品牌声誉管理、产品评价分析                         | 实时了解公众情绪，优化产品，进行危机公关         |\n",
    "| **客户关系**   | 分析客服对话、服务工单、用户满意度问卷                       | 自动评估客户满意度，发现服务痛点，提升服务质量 |\n",
    "| **金融科技**   | 分析财经新闻、社交媒体情绪、股评                           | 辅助量化交易和投资决策                         |\n",
    "| **社会科学**   | 分析公众对特定社会事件或政策的情感倾向                       | 为社会学、传播学等领域的研究提供数据参考         |\n",
    "\n",
    "本项目旨在通过一个完整的实战案例，让您掌握如何利用最新的ERNIE-4.5-0.3B模型解决真实场景下的文本分类问题，为您在自己的业务或研究中应用NLP技术打下坚实的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 为什么选择 ERNIE-4.5-0.3B 模型？\n",
    "\n",
    "ERNIE 4.5 并非单一模型，而是一个包含多种规模和针对不同场景优化的模型家族。为了帮助您更好地理解我们为什么选择 ERNIE-4.5-0.3B，下表依据官方 `ERNIE-develop` 仓库文档，对系列内的主要模型进行了对比：\n",
    "\n",
    "| 模型名称 | 支持的微调方法 | 支持的序列长度 | 最低资源需求 (SFT) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **ERNIE-4.5-0.3B** | SFT, SFT-LoRA, DPO, DPO-LoRA | 8k, 32k, 128k | **1x 80G A/H GPU** |\n",
    "| ERNIE-4.5-21B-A3B | SFT, SFT-LoRA, DPO, DPO-LoRA | 8k, 32k, 128k | 8x 80G A/H GPUs |\n",
    "| ERNIE-4.5-300B-A47B | SFT, SFT(FP8), SFT-LoRA, DPO, DPO-LoRA | 8k, 32k | 96x 80G A/H GPUs (SFT) <br> 16x 80G H GPUs (SFT with FP8) |\n",
    "\n",
    "*表格信息来源：ERNIE-develop/docs/erniekit.md* \n",
    "\n",
    "<br>\n",
    "\n",
    "**为什么选择 ERNIE-4.5-0.3B？**\n",
    "\n",
    "对于本次情感分析实战教程，ERNIE-4.5-0.3B 是一个理想的选择，原因如下：\n",
    "\n",
    "*   **资源友好，触手可及**：从上表可以明确看到，`ERNIE-4.5-0.3B` 的全参数微调（SFT）仅需**单张80G的A/H系列GPU**即可完成，这极大地降低了学习和实践大模型的硬件门槛。相比之下，其他更大规模的模型动辄需要数十甚至上百张GPU，对于个人开发者和大多数学习场景来说是不现实的。\n",
    "*   **功能完整，体验全面**：尽管模型规模较小，但 `ERNIE-4.5-0.3B` 支持包括SFT、LoRA、DPO在内的全套微调技术，让我们可以用最经济的方式，完整地体验和学习大模型开发的整个流程。\n",
    "*   **性能优异，足以胜任**：对于情感分析这类相对成熟的NLP任务，`ERNIE-4.5-0.3B` 继承了ERNIE系列强大的语义理解能力，其性能完全能够满足高精度分类的需求。我们无需动用“杀牛刀”，即可高效地解决问题。\n",
    "\n",
    "ERNIE-4.5-0.3B 在**学习成本、功能完整性和任务性能**之间取得了绝佳的平衡，是入门和实践大模型情感分析任务的**官方推荐和最佳选择**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 本教程的目标与内容\n",
    "\n",
    "本教程面向对NLP和深度学习有基本了解，并希望动手实践解决真实问题的开发者和学习者。通过本教程，您将收获满满：\n",
    "\n",
    "| 学习目标 | 核心技能点 | 您将掌握 | \n",
    "|:---:|:---|:---| \n",
    "| 🎯 **理解任务** | 情感分析基本流程 | 能够清晰地描述情感分析任务的输入、输出和评估指标 | \n",
    "| 🛠️ **掌握工具** | ERNIEkit的使用 | 熟练使用ERNIEkit进行数据处理、模型加载和配置 | \n",
    "| 🔥 **核心实战** | ERNIE-4.5-0.3B微调 | 从零开始，训练一个高精度的中文情感分析模型 | \n",
    "| 📊 **学会评估** | 模型性能评估与预测 | 能够科学地评估模型效果，并使用模型进行推理预测 | \n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ERNIEkit简介：一站式大模型开发利器\n",
    "\n",
    "在正式开始动手之前，我们有必要先认识一下本次实战的“主角”之一——ERNIEkit。简单来说，**ERNIEkit 是一个为ERNIE系列大模型量身打造的全流程开发工具包**。\n",
    "\n",
    "如果您之前有过使用 `PaddleNLP` 或 `Hugging Face Transformers` 的经验，您可以将 `ERNIEkit` 理解为一个更加聚焦和高效的替代品。它将大模型开发的复杂流程，如数据处理、模型训练、性能优化、评估和部署等，都封装成了简洁的命令行工具和配置文件，旨在为开发者提供“开箱即用”的极致体验。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ERNIEkit的核心设计理念\n",
    "\n",
    "ERNIEkit的设计哲学可以概括为“**约定优于配置**” (Convention over Configuration)。它为大模型的不同应用场景（如SFT、DPO、预训练等）提供了最佳实践范本和默认配置。开发者不再需要编写冗长的训练脚本，只需：\n",
    "\n",
    "1.  **准备数据**：按照约定格式准备好您的数据。\n",
    "2.  **修改配置**：在一个`yaml`配置文件中，像填空一样修改几个核心参数（如模型路径、数据路径、学习率等）。\n",
    "3.  **执行命令**：运行一条简单的命令（如 `erniekit train`），即可启动一个经过深度优化的训练流程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ERNIEkit的核心优势\n",
    "\n",
    "| 优势特性 | 具体体现 | 为开发者带来的价值 |\n",
    "|:---:|:---|:---|\n",
    "| 🚀 **极致易用** | `配置文件` + `命令行` 的开发模式 | 大幅降低大模型开发门槛，无需编写复杂代码 |\n",
    "| ⚡ **性能卓越** | 内置多种并行策略、混合精度、算子融合等优化 | 无需手动优化，即可获得高性能的训练和推理速度 |\n",
    "| 📦 **功能全面** | 覆盖数据处理、训练、评估、推理、部署全流程 | 提供一站式解决方案，简化开发流程 |\n",
    "| 🔧 **灵活扩展** | 支持自定义模型、数据、训练流程 | 既能开箱即用，也为高阶用户保留了足够的灵活性 |\n",
    "\n",
    "通过本教程，您将亲身体验到使用ERNIEkit进行大模型开发的便捷与高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 环境准备\n",
    "\n",
    "首先，我们需要确保安装了必需的库。本教程主要依赖 ERNIEkit 和 aistudio-sdk。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 安装最新版本的ERNIEkit和aistudio-sdk\r\n",
    "!pip install --upgrade aistudio-sdk\r\n",
    "\r\n",
    "# 下载ERNIE-4.5-0.3B模型\r\n",
    "!aistudio download --model PaddlePaddle/ERNIE-4.5-0.3B-Paddle --local_dir baidu/ERNIE-4.5-0.3B-Paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装完成后，我们可以导入并检查版本信息，以确保环境配置正确。  Ai studio运行的同学不需要运行这代码块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T07:16:41.583798Z",
     "iopub.status.busy": "2025-07-18T07:16:41.583484Z",
     "iopub.status.idle": "2025-07-18T07:16:43.037416Z",
     "shell.execute_reply": "2025-07-18T07:16:43.036888Z",
     "shell.execute_reply.started": "2025-07-18T07:16:41.583779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddlePaddle is compiled with CUDA and GPU is available.\r\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU是否可用\r\n",
    "import paddle\r\n",
    "if paddle.is_compiled_with_cuda():\r\n",
    "    print(\"PaddlePaddle is compiled with CUDA and GPU is available.\")\r\n",
    "else:\r\n",
    "    print(\"PaddlePaddle is not compiled with CUDA. It will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境准备非常简单。接下来，我们将加载用于本次任务的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据准备\n",
    "\n",
    "对于情感分析任务，我们需要一个**已标注**的数据集，其中每条文本都对应一个情感标签（如\"积极\"或\"消极\"）。\n",
    "\n",
    "### 3.1 数据集格式说明\n",
    "\n",
    "ERNIEkit 使用 `jsonl` 格式作为标准的数据输入格式。每一行都是一个独立的JSON对象，包含了训练所需的数据。对于情感分析任务，我们需要准备以下格式的数据：\n",
    "\n",
    "```json\n",
    "{\"text\": \"这家餐厅的烤鸭味道绝了，环境也很棒！\", \"label\": 1}\n",
    "{\"text\": \"等了半个多小时才上菜，味道也很一般\", \"label\": 0}\n",
    "```\n",
    "\n",
    "其中：\n",
    "- `text`: 表示输入的文本内容。\n",
    "- `label`: 表示情感标签，`1` 代表积极，`0` 代表消极。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 创建示例数据\n",
    "\n",
    "为了方便演示，我们手动创建两个`jsonl`文件，分别作为训练集和评估集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T07:43:59.478943Z",
     "iopub.status.busy": "2025-07-18T07:43:59.478622Z",
     "iopub.status.idle": "2025-07-18T07:43:59.485958Z",
     "shell.execute_reply": "2025-07-18T07:43:59.485388Z",
     "shell.execute_reply.started": "2025-07-18T07:43:59.478923Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "符合ERNIEkit格式的示例数据创建成功！sft-train.jsonl 和 sft-eval.jsonl 已生成。\r\n"
     ]
    }
   ],
   "source": [
    "import json\r\n",
    "\r\n",
    "# 定义标签映射\r\n",
    "label_map = {1: \"正面情感\", 0: \"负面情感\"}\r\n",
    "\r\n",
    "# 创建符合ERNIEkit SFT格式的训练数据\r\n",
    "train_data = [\r\n",
    "    # 正面评价\r\n",
    "    {\"src\": [\"这家餐厅的烤鸭味道绝了，环境也很棒！\"], \"tgt\": [label_map[1]]},\r\n",
    "    {\"src\": [\"服务员态度很好，下次还会再来\"], \"tgt\": [label_map[1]]},\r\n",
    "    {\"src\": [\"电影特效非常震撼，故事情节也很吸引人。\"], \"tgt\": [label_map[1]]},\r\n",
    "    {\"src\": [\"这款手机拍照效果真不错，电池也耐用。\"], \"tgt\": [label_map[1]]},\r\n",
    "    \r\n",
    "    # 负面评价\r\n",
    "    {\"src\": [\"等了半个多小时才上菜，味道也很一般\"], \"tgt\": [label_map[0]]},\r\n",
    "    {\"src\": [\"房间隔音太差了，晚上根本睡不着\"], \"tgt\": [label_map[0]]},\r\n",
    "    {\"src\": [\"新买的鞋子穿了一天就开胶了，质量太差了。\"], \"tgt\": [label_map[0]]},\r\n",
    "    {\"src\": [\"客服电话一直打不通，问题解决不了，体验极差。\"], \"tgt\": [label_map[0]]}\r\n",
    "]\r\n",
    "\r\n",
    "with open('sft-train.jsonl', 'w', encoding='utf-8') as f:\r\n",
    "    for item in train_data:\r\n",
    "        # ERNIEkit要求src和tgt是列表\r\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\r\n",
    "\r\n",
    "# 创建符合ERNIEkit SFT格式的评估数据\r\n",
    "eval_data = [\r\n",
    "    {\"src\": [\"风景不错，就是人太多了\"], \"tgt\": [label_map[1]]},\r\n",
    "    {\"src\": [\"酒店位置很难找，性价比不高\"], \"tgt\": [label_map[0]]}\r\n",
    "]\r\n",
    "\r\n",
    "with open('sft-eval.jsonl', 'w', encoding='utf-8') as f:\r\n",
    "    for item in eval_data:\r\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\r\n",
    "\r\n",
    "print(\"符合ERNIEkit格式的示例数据创建成功！sft-train.jsonl 和 sft-eval.jsonl 已生成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 配置数据路径\n",
    "\n",
    "在ERNIEkit中，我们通过修改配置文件来指定数据集的路径。打开 `examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml` 文件，找到以下部分并修改：\n",
    "\n",
    "```yaml\n",
    "### data\n",
    "train_dataset_path: \"./sft-train.jsonl\"\n",
    "eval_dataset_path: \"./sft-eval.jsonl\"\n",
    "```\n",
    "\n",
    "这样，ERNIEkit在训练时就会自动加载我们准备好的数据。\n",
    "\n",
    "至此，数据准备的全部工作已经完成。我们拥有了可以随时送入模型进行训练的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型与微调原理\n",
    "\n",
    "在数据准备就绪后，我们需要理解我们将要使用的模型以及如何让它适应我们的情感分析任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ERNIE-4.5-0.3B 模型简介：一位懂知识的语言大师\n",
    "\n",
    "正如引言中所说，ERNIE-4.5-0.3B 是一个更\"懂\"中文的模型。它的核心优势在于其独特的**知识整合预训练任务**。\n",
    "\n",
    "*   **一个更深入的比喻**：\n",
    "    想象一个学生（其他模型）在学习一句话：\"中国四大发明之一的活字印刷术是由毕昇发明的。\"\n",
    "    这个学生可能通过\"单字填空\"学会了\"（活）字印刷术\"和\"毕（昇）\"这些字。\n",
    "    \n",
    "    而ERNIE这位\"学霸\"则不同，老师会直接将\"**活字印刷术**\"和\"**毕昇**\"这两个完整的知识单元盖住，让他去预测。为了能答对，ERNIE不仅仅要学习语言的流畅性，更被迫去理解\"活字印刷术\"是一个完整的技术概念，\"毕昇\"是一个人名，并且这两个知识单元之间存在\"发明\"的关联。\n",
    "    \n",
    "    通过在海量文本上进行这种\"知识点挖掘\"式的学习，ERNIE构建了更深层次的语义理解能力，而不仅仅是表面的文字组合。\n",
    "\n",
    "这就是为什么ERNIE在需要理解句子深层含义的任务（如情感分析）上通常表现得更好的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 微调 (Fine-tuning) 原理：为语言大师配上\"情感分析眼镜\"\n",
    "\n",
    "我们加载的 `ERNIE-4.5-0.3B` 模型是一位博学的\"通才\"，它懂得语言的各种知识，但默认情况下，它并不知道自己需要做\"情感分析\"这项具体工作。**微调 (Fine-tuning)** 的过程，就好比是为这位语言大师配上一副特制的\"情感分析眼镜\"，并指导他如何使用。\n",
    "\n",
    "这个过程分为几步：\n",
    "\n",
    "1.  **获取句子的\"灵魂\"——[CLS]输出**：\n",
    "    *   我们将一条评论文本（例如\"这家店的拉面真好吃！\"）送入ERNIE模型。在送入前，我们在文本最前面加上一个特殊的标志 `[CLS]` (Classification)。\n",
    "    *   经过ERNIE内部复杂的计算（多层Transformer），每个字词都会得到一个富含上下文信息的向量。而这个特殊的 `[CLS]` 标志，在经过整个模型后，它对应的输出向量 (`pooled_output`) 就被设计为凝聚了整个句子的核心语义。我们可以把它比作ERNIE读完一句话后，在脑海中形成的对这句话的\"整体印象\"或\"灵魂\"向量。\n",
    "\n",
    "2.  **安装\"情感分析镜片\"——添加分类头**：\n",
    "    *   这个\"灵魂\"向量（例如一个768维的向量）本身还只是一串数字，不直接代表\"积极\"或\"消极\"。\n",
    "    *   我们在这串数字后面接上一个非常简单的\"镜片\"——一个**全连接层 (Linear Layer)**。这个全连接层就是\"分类头\"，它的任务非常专一：接收代表句子灵魂的向量，然后将其转换为对我们任务有意义的输出。\n",
    "    *   在我们的二分类任务中，这个\"镜片\"有两个输出神经元，一个代表\"消极\"的可能性，一个代表\"积极\"的可能性。\n",
    "\n",
    "3.  **\"校准眼镜\"——训练与反向传播**：\n",
    "    *   现在，我们有了一个戴着\"情感分析眼镜\"的ERNIE模型。我们把准备好的标注数据（成千上万条已知情感的评论）喂给它。\n",
    "    *   对于每条评论，模型会通过\"眼镜\"给出一个初步的判断（例如，\"消极\"得分0.1，\"积极\"得分0.9）。\n",
    "    *   我们拿这个判断和真实标签（比如这条评论的真实标签是\"积极\"）去比较。如果判断准确，说明\"眼镜\"校准得不错；如果判断错误，就计算出一个\"误差\"（即**交叉熵损失**）。\n",
    "    *   这个\"误差\"信号会反向传播，像一个校准指令一样，不仅会微调\"镜片\"（分类头）的参数，让它判断得更准，**更重要的是，它也会微调ERNIE这位语言大师本身的参数**，让他以后在阅读文本时，能更侧重于那些与情感相关的语义信息。\n",
    "\n",
    "经过成千上万次这样的\"校准\"，ERNIE大师和他的\"情感分析眼镜\"就配合得天衣无缝了。他不仅能读懂句子的意思，还能准确地判断出其中蕴含的情感。\n",
    "\n",
    "\n",
    "我们本教程采用的是 **SFT (Supervised Fine-Tuning)**，也称为**全参数微调**。这意味着在训练过程中，我们会更新ERNIE模型的所有参数以及新添加的分类头，让整个模型都来适配我们的情感分析任务。\n",
    "\n",
    "幸运的是，ERNIEkit 已经帮我们完成了\"装配眼镜\"的工作。我们只需要加载模型，它就会自动为我们搭建好\"ERNIE大师 + 分类头\"的结构，让我们能专注于\"校准眼镜\"的训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型微调\n",
    "\n",
    "现在，我们将所有部分组合起来，开始训练我们的情感分析模型。在ERNIEkit中，我们通过配置文件来管理所有的训练参数，并通过一条简单的命令来启动训练。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 配置训练参数\n",
    "\n",
    "打开 `examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml` 文件，我们可以看到所有的训练参数。这里我们对一些核心参数进行说明：\n",
    "\n",
    "```yaml\n",
    "### model\n",
    "model_name_or_path: baidu/ERNIE-4.5-0.3B-Paddle  # 指定我们下载的模型路径\n",
    "fine_tuning: Full                                # 全参数微调\n",
    "\n",
    "### finetuning\n",
    "stage: SFT                                       # 指定训练阶段为SFT\n",
    "seed: 23                                         # 随机种子，保证实验可复现\n",
    "do_train: True                                   # 是否进行训练\n",
    "do_eval: True                                    # 是否进行评估\n",
    "batch_size: 1                                    # 每个GPU上的批大小\n",
    "num_train_epochs: 1                              # 训练轮数\n",
    "max_steps: 100                                   # 最大训练步数\n",
    "learning_rate: 1.0e-5                            # 学习率\n",
    "output_dir: ./output                             # 模型保存路径\n",
    "\n",
    "### performance\n",
    "compute_type: bf16                               # 使用bf16混合精度训练，加速训练\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 启动训练\n",
    "\n",
    "配置好参数后，我们就可以使用以下命令启动训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T07:44:07.840269Z",
     "iopub.status.busy": "2025-07-18T07:44:07.839871Z",
     "iopub.status.idle": "2025-07-18T07:46:57.138005Z",
     "shell.execute_reply": "2025-07-18T07:46:57.137361Z",
     "shell.execute_reply.started": "2025-07-18T07:44:07.840248Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 -----------  Configuration  ----------------------\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 auto_cluster_config: 0\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 auto_parallel_config: None\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 auto_tuner_json: None\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 devices: 0\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 elastic_level: -1\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 elastic_timeout: 30\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 enable_gpu_log: True\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 gloo_port: 6767\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 host: None\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 ips: None\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 job_id: default\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 legacy: False\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 log_dir: erniekit_dist_log\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 log_level: INFO\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 log_overwrite: False\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 master: 127.0.0.1:8080\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 max_restart: 3\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 nnodes: 1\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 nproc_per_node: None\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 rank: -1\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 run_mode: collective\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 server_num: None\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 servers: \r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 sort_ip: False\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 start_port: 6070\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 trainer_num: None\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 trainers: \r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 training_script: /home/ERNIEKit/erniekit/launcher.py\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 training_script_args: ['train', 'ERNIE-develop/examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml']\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 with_gloo: 1\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,200 --------------------------------------------------\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,201 Job: default, mode collective, replicas 1[1:1], elastic False\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,202 Run Pod: xiidtm, replicas 1, status ready\r\n",
      "LAUNCH INFO 2025-07-18 15:44:13,217 Watching Pod: xiidtm, replicas 1, status running\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "\u001b[32m[2025-07-18 15:44:15,281] [    INFO]\u001b[0m - user has defined resume_from_checkpoint: None\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,281] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,341] [    INFO]\u001b[0m - Tensor_parallel_degree = 1. Set sequence_parallel to False.\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,341] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,341] [   DEBUG]\u001b[0m -      Model Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - add_tail_layers               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - continue_training             : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - fine_tuning                   : Full\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - fuse_gate_detach_matmul       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - fuse_linear                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - fuse_rms_norm                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - fuse_rope                     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - fuse_softmax_mask             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - fuse_swiglu                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - lora                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - lora_alpha                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - lora_path                     : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - lora_plus_scale               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - lora_rank                     : 8\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - model_name_or_path            : ./data/models/30656/ERNIE-4.5-0.3B-Paddle\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - moe_aux_loss_lambda           : 1e-05\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - moe_group                     : dummy\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - moe_group_experts             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,342] [   DEBUG]\u001b[0m - moe_multimodal_dispatch_use_allgather: v2-alltoall-unpad\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - moe_orthogonal_loss_lambda    : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - moe_use_aux_free              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - moe_use_hard_gate             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - moe_z_loss_lambda             : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - no_recompute_layers           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - offload_recompute_inputs      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - pp_seg_method                 : layer:Ernie4_5_DecoderLayer|EmptyLayer\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - recompute_granularity         : full\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - recompute_use_reentrant       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - rslora                        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - rslora_plus                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - stage                         : SFT\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - tensor_parallel_output        : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - use_attn_mask_start_row_indices: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - use_flash_attention           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - use_fused_head_and_loss_fn    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - use_recompute_moe             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - use_sparse_flash_attn         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - use_sparse_head_and_loss_fn   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - virtual_pp_degree             : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,343] [   DEBUG]\u001b[0m -       Data Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - buffer_size                   : 500\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - dataset_type                  : iterable\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - eval_dataset_path             : ./sft-eval.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - eval_dataset_prob             : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - eval_dataset_type             : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - greedy_intokens               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - in_tokens_batching            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - mask_out_eos_token            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - max_prompt_len                : 2048\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - max_seq_len                   : 8192\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - num_comparisons               : 6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - num_samples_each_epoch        : 6000000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - offline_dataset_path          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - random_shuffle                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - train_dataset_path            : ./sft-train.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - train_dataset_prob            : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - train_dataset_type            : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - use_cls                       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:15,344] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,345] [    INFO]\u001b[0m - The global seed is set to 23, local seed is set to 24 and random seed is set to 23.\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 15:44:15,345] [ WARNING]\u001b[0m - Process rank: -1, device: gpu, world_size: 1, distributed training: False, 16-bits training: True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,345] [    INFO]\u001b[0m - Start to load model ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,494] [    INFO]\u001b[0m - Loading configuration file ./data/models/30656/ERNIE-4.5-0.3B-Paddle/config.json\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 15:44:15,496] [ WARNING]\u001b[0m - You are using a model of type ernie4_5 to instantiate a model of type ernie4_5_moe. This is not supported for all configurations of models and can yield errors.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,497] [    INFO]\u001b[0m - Loading weights file ./data/models/30656/ERNIE-4.5-0.3B-Paddle/model.safetensors\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,845] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,846] [    INFO]\u001b[0m - change initializer-range from 0.02 to 0.018041293779826325\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,846] [    INFO]\u001b[0m - using moe-group: dummy\u001b[0m\r\n",
      "W0718 15:44:15.853309 93184 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.8, Runtime API Version: 12.6\r\n",
      "\u001b[32m[2025-07-18 15:44:15,855] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,859] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,861] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,863] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,866] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,868] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,870] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,872] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,875] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,877] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,879] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,881] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,884] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,886] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,889] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,950] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,953] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,955] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,959] [    INFO]\u001b[0m - output-weight:[103424, 1024] config.tie_word_embeddings=True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:15,960] [    INFO]\u001b[0m - Use fusedRMSNorm\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,039] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing Ernie4_5_MoeForCausalLM.\r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,040] [    INFO]\u001b[0m - All the weights of Ernie4_5_MoeForCausalLM were initialized from the model checkpoint at ./data/models/30656/ERNIE-4.5-0.3B-Paddle.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Ernie4_5_MoeForCausalLM for predictions without further training.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,041] [    INFO]\u001b[0m - Loading configuration file ./data/models/30656/ERNIE-4.5-0.3B-Paddle/generation_config.json\u001b[0m\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:250: UserWarning: using greedy search strategy. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:255: UserWarning: using greedy search strategy. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-18 15:44:16,052] [    INFO]\u001b[0m - Loading model successfully !\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:16,054] [   DEBUG]\u001b[0m - Model config: Ernie4_5_MoeConfig {\r\n",
      "  \"add_tail_layers\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"Ernie4_5_ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.0,\r\n",
      "  \"bos_token_id\": 1,\r\n",
      "  \"cachekv_quant\": false,\r\n",
      "  \"compression_ratio\": 1.0,\r\n",
      "  \"dpo_config\": null,\r\n",
      "  \"dtype\": \"bfloat16\",\r\n",
      "  \"enable_delay_scale_loss\": true,\r\n",
      "  \"enable_mtp_magic_send\": false,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"fuse_gate_detach_matmul\": true,\r\n",
      "  \"fuse_linear\": false,\r\n",
      "  \"fuse_ln\": false,\r\n",
      "  \"fuse_rms_norm\": true,\r\n",
      "  \"fuse_rope\": true,\r\n",
      "  \"fuse_softmax_mask\": false,\r\n",
      "  \"fuse_swiglu\": true,\r\n",
      "  \"global_aux_loss\": false,\r\n",
      "  \"head_dim\": 128,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_dropout_prob\": 0.0,\r\n",
      "  \"hidden_size\": 1024,\r\n",
      "  \"ignored_index\": -100,\r\n",
      "  \"initializer_range\": 0.018041293779826325,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"max_position_embeddings\": 131072,\r\n",
      "  \"max_sequence_length\": 8192,\r\n",
      "  \"micro_batch_size\": -1,\r\n",
      "  \"model_type\": \"ernie4_5_moe\",\r\n",
      "  \"moe_all_to_all_dropout\": 0.0,\r\n",
      "  \"moe_aux_loss_lambda\": 1e-05,\r\n",
      "  \"moe_capacity\": [],\r\n",
      "  \"moe_dense_experts_token_type_id\": 3,\r\n",
      "  \"moe_dropout_prob\": 0.0,\r\n",
      "  \"moe_gate\": \"topk\",\r\n",
      "  \"moe_gate_act\": \"softmax\",\r\n",
      "  \"moe_group_experts\": false,\r\n",
      "  \"moe_group_origin\": \"dummy\",\r\n",
      "  \"moe_group_orthogonal_loss\": true,\r\n",
      "  \"moe_intermediate_size\": 0,\r\n",
      "  \"moe_k\": 2,\r\n",
      "  \"moe_layer_end_index\": 17,\r\n",
      "  \"moe_layer_interval\": 2,\r\n",
      "  \"moe_layer_start_index\": 0,\r\n",
      "  \"moe_multimodal_dispatch_use_allgather\": \"v2-alltoall-unpad\",\r\n",
      "  \"moe_norm_gate_logits\": true,\r\n",
      "  \"moe_num_experts\": null,\r\n",
      "  \"moe_num_shared_experts\": 0,\r\n",
      "  \"moe_orthogonal_loss_lambda\": 0.0,\r\n",
      "  \"moe_rank\": 0,\r\n",
      "  \"moe_reverse_token_drop\": false,\r\n",
      "  \"moe_use_aux_free\": false,\r\n",
      "  \"moe_use_hard_gate\": false,\r\n",
      "  \"moe_world_size\": 1,\r\n",
      "  \"moe_z_loss_lambda\": 0.0,\r\n",
      "  \"multi_token_pred_lambda\": 0.3,\r\n",
      "  \"num_acc_steps\": 8,\r\n",
      "  \"num_attention_heads\": 16,\r\n",
      "  \"num_hidden_layers\": 18,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"num_nextn_predict_layers\": 0,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"paddleformers_version\": \"0.1\",\r\n",
      "  \"pp_seg_method\": \"layer:Ernie4_5_DecoderLayer|EmptyLayer\",\r\n",
      "  \"recompute_granularity\": \"full\",\r\n",
      "  \"recompute_use_reentrant\": true,\r\n",
      "  \"refined_recompute\": {},\r\n",
      "  \"rms_norm_eps\": 1e-05,\r\n",
      "  \"rope_theta\": 500000,\r\n",
      "  \"sinkhorn_2gate\": true,\r\n",
      "  \"sinkhorn_temp\": 0.03,\r\n",
      "  \"skip_recompute_ops\": {},\r\n",
      "  \"tensor_parallel_degree\": -1,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"token_balance_loss\": false,\r\n",
      "  \"token_balance_seqlen\": false,\r\n",
      "  \"use_bias\": false,\r\n",
      "  \"use_flash_attention\": true,\r\n",
      "  \"use_fused_head_and_loss_fn\": false,\r\n",
      "  \"use_recompute_lm_head\": false,\r\n",
      "  \"use_recompute_loss_fn\": false,\r\n",
      "  \"use_recompute_moe\": false,\r\n",
      "  \"use_recompute_mtp\": false,\r\n",
      "  \"use_rmsnorm\": true,\r\n",
      "  \"use_sparse_flash_attn\": true,\r\n",
      "  \"use_sparse_head_and_loss_fn\": true,\r\n",
      "  \"use_var_len_flash_attn\": false,\r\n",
      "  \"vocab_size\": 103424,\r\n",
      "  \"weight_share_add_bias\": true\r\n",
      "}\r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,054] [    INFO]\u001b[0m - [timelog] basemodel loading time: 0.56s (2025-07-18 15:44:16) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,973] [    INFO]\u001b[0m - Start to create dataset ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,975] [    INFO]\u001b[0m - ./sft-train.jsonl: task prob: 1.0, ori number of examples: 8, target_num_each_epoch: 6000000\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,975] [    INFO]\u001b[0m - Creating dataset successfully ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:16,975] [    INFO]\u001b[0m - The global seed is set to 23, local seed is set to 24 and random seed is set to 23.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,012] [    INFO]\u001b[0m - max_steps is given, it will override any value given in num_train_epochs\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,013] [    INFO]\u001b[0m - Using half precision\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m -     Training Configuration Arguments    \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - _no_sync_in_gradient_accumulation: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - actscale_moving_rate          : 0.01\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - adam_beta1                    : 0.9\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - adam_beta2                    : 0.95\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - adam_epsilon                  : 1e-08\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - amp_custom_black_list         : ['reduce_sum', 'softmax_with_cross_entropy', 'c_softmax_with_cross_entropy', 'elementwise_div', 'sin', 'cos']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - amp_custom_white_list         : ['lookup_table', 'lookup_table_v2', 'flash_attn', 'matmul', 'matmul_v2', 'fused_gemm_epilogue']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,025] [   DEBUG]\u001b[0m - amp_master_grad               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - apply_hadamard                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - apply_online_actscale_step    : 200\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - attention_probs_dropout_prob  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - auto_parallel_resume_form_hybrid_parallel: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - batch_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - beta                          : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - bf16                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - bf16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - ckpt_quant_stage              : O0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - compute_type                  : bf16\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - context_parallel_degree       : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - count_trained_tokens          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - current_device                : gpu:0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - data_parallel_config          : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - data_parallel_degree          : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - data_parallel_rank            : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - dataloader_drop_last          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - dataloader_num_workers        : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - dataloader_shuffle            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - dataset_rank                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,026] [   DEBUG]\u001b[0m - dataset_world_size            : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - ddp_find_unused_parameters    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - decay_steps                   : 100\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - device                        : gpu\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - disable_ckpt_quant            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - disable_tqdm                  : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - distributed_dataloader        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - do_eval                       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - do_export                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - do_predict                    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - do_train                      : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - dpo_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - dpop_lambda                   : 50\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - dropout_warmup_steps          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - enable_auto_parallel          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - enable_zero_cost_checkpoint   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - eval_accumulation_steps       : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - eval_batch_size               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - eval_steps                    : 10000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - evaluation_strategy           : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - expert_max_capacity           : 4294967296\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - expert_min_capacity           : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - expert_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,027] [   DEBUG]\u001b[0m - expert_tensor_parallel_degree : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - flash_device_save_steps       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - flatten_param_grads           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - force_reshard_pp              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - fp16                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - fp16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - fp16_opt_level                : O2\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - fp8_format_type               : hybrid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - fuse_sequence_parallel_allreduce: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - gradient_accumulation_steps   : 8\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - greater_is_better             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - hadamard_block_size           : 32\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - hidden_dropout_prob           : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - hybrid_parallel_topo_order    : pp_first\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - ignore_data_skip              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - ignore_load_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - ignore_save_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - label_names                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - label_smoothing               : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - layerwise_lr_decay_bound      : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - lazy_data_processing          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - learning_rate                 : 1e-05\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - load_best_model_at_end        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,028] [   DEBUG]\u001b[0m - load_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - local_process_index           : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - local_rank                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - log_level                     : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - log_level_replica             : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - log_on_each_node              : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - logging_dir                   : ./vdl_log\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - logging_first_step            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - logging_steps                 : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - logging_strategy              : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - logical_process_index         : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - loss_type                     : sigmoid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - lr_end                        : 1e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - lr_scheduler_type             : SchedulerType.COSINE\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - max_estimate_samples          : 100000.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - max_evaluate_steps            : 10000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - max_grad_norm                 : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - max_steps                     : 100\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - metric_for_best_model         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - metrics_output_path           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - min_lr                        : 1e-06\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - minimum_eval_times            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,029] [   DEBUG]\u001b[0m - multi_token_pred_lambda       : 0.3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - no_cuda                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - normalize_logps               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - num_cycles                    : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - num_nextn_predict_layers      : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - num_of_gpus                   : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - num_train_epochs              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - offload_optim                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - offset_alpha                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - optim                         : OptimizerNames.ADAMW\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - optim_shard_num               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - optimizer_name_suffix         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - ordered_save_group_size       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - output_dir                    : ./output\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - output_signal_dir             : ./output\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - overwrite_output_dir          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - pad_token_id                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - past_index                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - pdc_download_ckpt             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - pdc_download_timeout          : 300\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - per_device_eval_batch_size    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - per_device_train_batch_size   : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - pipeline_parallel_config      : enable_delay_scale_loss enable_release_grads disable_partial_send_recv\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,030] [   DEBUG]\u001b[0m - pipeline_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - pipeline_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - power                         : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - prediction_loss_only          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - pref_loss_ratio               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - process_index                 : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - quant_input_grad              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - quant_weight_grad             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - recompute                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - ref_model_update_steps        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - reference_free                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - refined_recompute             : {}\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - release_grads                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - remove_unused_columns         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - report_to                     : ['visualdl']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - resume_from_checkpoint        : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - run_name                      : ./output\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - save_on_each_node             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - save_rng_states               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - save_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - save_sharding_stage1_model_include_freeze_params: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - save_steps                    : 10000000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - save_strategy                 : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,031] [   DEBUG]\u001b[0m - save_tokenizer                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - save_total_limit              : 5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - scale_loss                    : 32768\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - seed                          : 23\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sep_parallel_degree           : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sequence_parallel             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sequence_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - server_tp_degree              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sft_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sft_loss_ratio                : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sharding                      : []\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sharding_comm_buffer_size_MB  : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sharding_degree               : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sharding_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sharding_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sharding_parallel_mesh_dimension: dp\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - sharding_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - should_load_dataset           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - should_load_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - should_log                    : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - should_save                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - should_save_model_state       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - should_save_model_with_tensor_fusion: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,032] [   DEBUG]\u001b[0m - should_save_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - simpo_gamma                   : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - skip_data_intervals           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - skip_memory_metrics           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - skip_profile_timer            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - split_inputs_sequence_dim     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - split_norm_comm               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - tensor_parallel_config        : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - tensor_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - tensor_parallel_rank          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - tensorwise_offload_optimizer  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - to_static                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - train_batch_size              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - unified_checkpoint            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - unified_checkpoint_config     : ['async_save']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - use_async_save                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - use_expert_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - use_hybrid_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - use_lowprecision_moment       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - use_recompute_mtp             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - use_sp_callback               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - wandb_api_key                 : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - wandb_http_proxy              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,033] [   DEBUG]\u001b[0m - warmup_ratio                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - warmup_steps                  : 20\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - weight_decay                  : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - weight_name_suffix            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - weight_quantize_algo          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - world_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - zcc_ema_interval              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - zcc_pipeline_hooks_capacity_usage: 0.6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - zcc_save_ema_coef             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - zcc_workers_num               : 3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,034] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,034] [    INFO]\u001b[0m - Starting training from resume_from_checkpoint : None\u001b[0m\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:450: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m - [timelog] checkpoint loading time: 0.00s (2025-07-18 15:44:17) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m - ***** Running training *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m -   Num examples = 800\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m -   Num Epochs = 9223372036854775807\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m -   Instantaneous batch size per device = 1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m -   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m -   Gradient Accumulation steps = 8\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m -   Total optimization steps = 100\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,042] [    INFO]\u001b[0m -   Total num train samples = 800\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 15:44:17,043] [   DEBUG]\u001b[0m -   Number of trainable parameters = 360,748,032 (per device)\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:17,060] [    INFO]\u001b[0m - prepare SequenceDataset ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:20,120] [    INFO]\u001b[0m - prepare SequenceDataset done: total number of examples is 6000000\u001b[0m\r\n",
      "W0718 15:44:21.237807 93184 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.\r\n",
      "\u001b[32m[2025-07-18 15:44:24,412] [    INFO]\u001b[0m - loss: 10.39164352, learning_rate: 5e-07, global_step: 1, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 11.897777318954468, max_memory_allocated: 10.45703935623169, max_memory_reserved: 11.897777318954468, interval_runtime: 7.3676, interval_samples_per_second: 1.0858, interval_steps_per_second: 0.1357, ppl: 32586.178986822717, progress_or_epoch: 0.01\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:25,911] [    INFO]\u001b[0m - loss: 10.39015293, learning_rate: 1e-06, global_step: 2, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 11.897777318954468, max_memory_allocated: 10.45703935623169, max_memory_reserved: 11.897777318954468, interval_runtime: 1.4993, interval_samples_per_second: 5.3358, interval_steps_per_second: 0.667, ppl: 32537.642537246655, progress_or_epoch: 0.02\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:27,566] [    INFO]\u001b[0m - loss: 10.3223896, learning_rate: 1.5e-06, global_step: 3, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 12.813213109970093, max_memory_allocated: 10.462876796722412, max_memory_reserved: 12.813213109970093, interval_runtime: 1.6552, interval_samples_per_second: 4.8331, interval_steps_per_second: 0.6041, ppl: 30405.828621905155, progress_or_epoch: 0.03\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:29,042] [    INFO]\u001b[0m - loss: 10.25313473, learning_rate: 2e-06, global_step: 4, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4758, interval_samples_per_second: 5.4207, interval_steps_per_second: 0.6776, ppl: 28371.339157937142, progress_or_epoch: 0.04\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:30,509] [    INFO]\u001b[0m - loss: 9.72236824, learning_rate: 2.5e-06, global_step: 5, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4673, interval_samples_per_second: 5.4522, interval_steps_per_second: 0.6815, ppl: 16686.716120719335, progress_or_epoch: 0.05\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:31,984] [    INFO]\u001b[0m - loss: 8.23078442, learning_rate: 3e-06, global_step: 6, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4745, interval_samples_per_second: 5.4256, interval_steps_per_second: 0.6782, ppl: 3754.777920103064, progress_or_epoch: 0.06\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:33,447] [    INFO]\u001b[0m - loss: 7.72773361, learning_rate: 3.5e-06, global_step: 7, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4627, interval_samples_per_second: 5.4694, interval_steps_per_second: 0.6837, ppl: 2270.4506386450303, progress_or_epoch: 0.07\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:34,910] [    INFO]\u001b[0m - loss: 4.5312953, learning_rate: 4e-06, global_step: 8, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4636, interval_samples_per_second: 5.466, interval_steps_per_second: 0.6833, ppl: 92.87878909514416, progress_or_epoch: 0.08\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:36,378] [    INFO]\u001b[0m - loss: 3.88533902, learning_rate: 4.5e-06, global_step: 9, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4675, interval_samples_per_second: 5.4514, interval_steps_per_second: 0.6814, ppl: 48.68344432339593, progress_or_epoch: 0.09\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:38,080] [    INFO]\u001b[0m - loss: 1.43456924, learning_rate: 5e-06, global_step: 10, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.7024, interval_samples_per_second: 4.6992, interval_steps_per_second: 0.5874, ppl: 4.197836359278837, progress_or_epoch: 0.1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:39,544] [    INFO]\u001b[0m - loss: 1.06942534, learning_rate: 5.5e-06, global_step: 11, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.464, interval_samples_per_second: 5.4646, interval_steps_per_second: 0.6831, ppl: 2.913704629280229, progress_or_epoch: 0.11\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:41,009] [    INFO]\u001b[0m - loss: 0.463714, learning_rate: 6e-06, global_step: 12, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4649, interval_samples_per_second: 5.4611, interval_steps_per_second: 0.6826, ppl: 1.5899681745094114, progress_or_epoch: 0.12\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:42,480] [    INFO]\u001b[0m - loss: 0.05802114, learning_rate: 6.5e-06, global_step: 13, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4712, interval_samples_per_second: 5.4377, interval_steps_per_second: 0.6797, ppl: 1.0597373983220915, progress_or_epoch: 0.13\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:43,952] [    INFO]\u001b[0m - loss: 0.02088747, learning_rate: 7e-06, global_step: 14, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.472, interval_samples_per_second: 5.4347, interval_steps_per_second: 0.6793, ppl: 1.0211071399856833, progress_or_epoch: 0.14\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:45,416] [    INFO]\u001b[0m - loss: 0.0037058, learning_rate: 7.5e-06, global_step: 15, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.464, interval_samples_per_second: 5.4646, interval_steps_per_second: 0.6831, ppl: 1.0037126749666139, progress_or_epoch: 0.15\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:46,883] [    INFO]\u001b[0m - loss: 0.00050533, learning_rate: 8e-06, global_step: 16, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.4669, interval_samples_per_second: 5.4537, interval_steps_per_second: 0.6817, ppl: 1.000505457700714, progress_or_epoch: 0.16\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:48,493] [    INFO]\u001b[0m - loss: 0.00019829, learning_rate: 8.5e-06, global_step: 17, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 13.730960607528687, max_memory_allocated: 10.465795040130615, max_memory_reserved: 13.730960607528687, interval_runtime: 1.6096, interval_samples_per_second: 4.9701, interval_steps_per_second: 0.6213, ppl: 1.0001983096607616, progress_or_epoch: 0.17\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:49,965] [    INFO]\u001b[0m - loss: 4.877e-05, learning_rate: 9e-06, global_step: 18, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4724, interval_samples_per_second: 5.4334, interval_steps_per_second: 0.6792, ppl: 1.0000487711892758, progress_or_epoch: 0.18\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:51,428] [    INFO]\u001b[0m - loss: 9.17e-06, learning_rate: 9.5e-06, global_step: 19, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4627, interval_samples_per_second: 5.4693, interval_steps_per_second: 0.6837, ppl: 1.0000091700420446, progress_or_epoch: 0.19\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:52,885] [    INFO]\u001b[0m - loss: 3.71e-06, learning_rate: 1e-05, global_step: 20, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4569, interval_samples_per_second: 5.4911, interval_steps_per_second: 0.6864, ppl: 1.0000037100068822, progress_or_epoch: 0.2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:54,355] [    INFO]\u001b[0m - loss: 2.1e-06, learning_rate: 9.997e-06, global_step: 21, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4707, interval_samples_per_second: 5.4398, interval_steps_per_second: 0.68, ppl: 1.000002100002205, progress_or_epoch: 0.21\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:55,824] [    INFO]\u001b[0m - loss: 1.4e-06, learning_rate: 9.986e-06, global_step: 22, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4686, interval_samples_per_second: 5.4475, interval_steps_per_second: 0.6809, ppl: 1.00000140000098, progress_or_epoch: 0.22\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:57,290] [    INFO]\u001b[0m - loss: 1.01e-06, learning_rate: 9.969e-06, global_step: 23, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4658, interval_samples_per_second: 5.4579, interval_steps_per_second: 0.6822, ppl: 1.0000010100005101, progress_or_epoch: 0.23\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:44:58,758] [    INFO]\u001b[0m - loss: 7.9e-07, learning_rate: 9.945e-06, global_step: 24, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4686, interval_samples_per_second: 5.4475, interval_steps_per_second: 0.6809, ppl: 1.000000790000312, progress_or_epoch: 0.24\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:00,223] [    INFO]\u001b[0m - loss: 6.8e-07, learning_rate: 9.914e-06, global_step: 25, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4652, interval_samples_per_second: 5.4601, interval_steps_per_second: 0.6825, ppl: 1.0000006800002312, progress_or_epoch: 0.25\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:01,693] [    INFO]\u001b[0m - loss: 6e-07, learning_rate: 9.876e-06, global_step: 26, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4694, interval_samples_per_second: 5.4444, interval_steps_per_second: 0.6805, ppl: 1.00000060000018, progress_or_epoch: 0.26\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:03,159] [    INFO]\u001b[0m - loss: 6.2e-07, learning_rate: 9.831e-06, global_step: 27, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.4657, interval_samples_per_second: 5.458, interval_steps_per_second: 0.6822, ppl: 1.0000006200001923, progress_or_epoch: 0.27\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:04,623] [    INFO]\u001b[0m - loss: 6e-07, learning_rate: 9.78e-06, global_step: 28, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 14.651019811630249, max_memory_allocated: 10.465795040130615, max_memory_reserved: 14.651019811630249, interval_runtime: 1.464, interval_samples_per_second: 5.4646, interval_steps_per_second: 0.6831, ppl: 1.00000060000018, progress_or_epoch: 0.28\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:06,102] [    INFO]\u001b[0m - loss: 5.5e-07, learning_rate: 9.722e-06, global_step: 29, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4797, interval_samples_per_second: 5.4066, interval_steps_per_second: 0.6758, ppl: 1.0000005500001512, progress_or_epoch: 0.29\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:07,573] [    INFO]\u001b[0m - loss: 5.2e-07, learning_rate: 9.657e-06, global_step: 30, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.471, interval_samples_per_second: 5.4386, interval_steps_per_second: 0.6798, ppl: 1.0000005200001352, progress_or_epoch: 0.3\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:09,041] [    INFO]\u001b[0m - loss: 5e-07, learning_rate: 9.587e-06, global_step: 31, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.468, interval_samples_per_second: 5.4495, interval_steps_per_second: 0.6812, ppl: 1.000000500000125, progress_or_epoch: 0.31\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:10,671] [    INFO]\u001b[0m - loss: 4.9e-07, learning_rate: 9.51e-06, global_step: 32, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6298, interval_samples_per_second: 4.9087, interval_steps_per_second: 0.6136, ppl: 1.00000049000012, progress_or_epoch: 0.32\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:12,147] [    INFO]\u001b[0m - loss: 5e-07, learning_rate: 9.426e-06, global_step: 33, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4758, interval_samples_per_second: 5.4207, interval_steps_per_second: 0.6776, ppl: 1.000000500000125, progress_or_epoch: 0.33\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:13,621] [    INFO]\u001b[0m - loss: 4.6e-07, learning_rate: 9.337e-06, global_step: 34, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4738, interval_samples_per_second: 5.428, interval_steps_per_second: 0.6785, ppl: 1.0000004600001058, progress_or_epoch: 0.34\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:15,096] [    INFO]\u001b[0m - loss: 4.6e-07, learning_rate: 9.242e-06, global_step: 35, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4748, interval_samples_per_second: 5.4243, interval_steps_per_second: 0.678, ppl: 1.0000004600001058, progress_or_epoch: 0.35\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:16,560] [    INFO]\u001b[0m - loss: 4.5e-07, learning_rate: 9.141e-06, global_step: 36, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4646, interval_samples_per_second: 5.4623, interval_steps_per_second: 0.6828, ppl: 1.0000004500001012, progress_or_epoch: 0.36\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:18,035] [    INFO]\u001b[0m - loss: 4.4e-07, learning_rate: 9.034e-06, global_step: 37, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4752, interval_samples_per_second: 5.4231, interval_steps_per_second: 0.6779, ppl: 1.0000004400000968, progress_or_epoch: 0.37\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:19,509] [    INFO]\u001b[0m - loss: 4.3e-07, learning_rate: 8.922e-06, global_step: 38, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4741, interval_samples_per_second: 5.4272, interval_steps_per_second: 0.6784, ppl: 1.0000004300000924, progress_or_epoch: 0.38\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:21,118] [    INFO]\u001b[0m - loss: 4e-07, learning_rate: 8.804e-06, global_step: 39, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6084, interval_samples_per_second: 4.974, interval_steps_per_second: 0.6218, ppl: 1.00000040000008, progress_or_epoch: 0.39\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:22,578] [    INFO]\u001b[0m - loss: 3.8e-07, learning_rate: 8.682e-06, global_step: 40, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4604, interval_samples_per_second: 5.478, interval_steps_per_second: 0.6848, ppl: 1.0000003800000723, progress_or_epoch: 0.4\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:24,046] [    INFO]\u001b[0m - loss: 3.8e-07, learning_rate: 8.555e-06, global_step: 41, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4674, interval_samples_per_second: 5.452, interval_steps_per_second: 0.6815, ppl: 1.0000003800000723, progress_or_epoch: 0.41\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:25,527] [    INFO]\u001b[0m - loss: 3.7e-07, learning_rate: 8.423e-06, global_step: 42, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4812, interval_samples_per_second: 5.4012, interval_steps_per_second: 0.6751, ppl: 1.0000003700000684, progress_or_epoch: 0.42\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:27,006] [    INFO]\u001b[0m - loss: 3.5e-07, learning_rate: 8.286e-06, global_step: 43, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4791, interval_samples_per_second: 5.4086, interval_steps_per_second: 0.6761, ppl: 1.0000003500000612, progress_or_epoch: 0.43\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:28,469] [    INFO]\u001b[0m - loss: 3.7e-07, learning_rate: 8.145e-06, global_step: 44, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4636, interval_samples_per_second: 5.4659, interval_steps_per_second: 0.6832, ppl: 1.0000003700000684, progress_or_epoch: 0.44\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:29,938] [    INFO]\u001b[0m - loss: 3.4e-07, learning_rate: 8e-06, global_step: 45, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4683, interval_samples_per_second: 5.4486, interval_steps_per_second: 0.6811, ppl: 1.0000003400000579, progress_or_epoch: 0.45\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:31,550] [    INFO]\u001b[0m - loss: 3.2e-07, learning_rate: 7.851e-06, global_step: 46, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6125, interval_samples_per_second: 4.9611, interval_steps_per_second: 0.6201, ppl: 1.0000003200000511, progress_or_epoch: 0.46\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:33,019] [    INFO]\u001b[0m - loss: 3.2e-07, learning_rate: 7.699e-06, global_step: 47, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4691, interval_samples_per_second: 5.4453, interval_steps_per_second: 0.6807, ppl: 1.0000003200000511, progress_or_epoch: 0.47\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:34,491] [    INFO]\u001b[0m - loss: 2.8e-07, learning_rate: 7.543e-06, global_step: 48, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4718, interval_samples_per_second: 5.4355, interval_steps_per_second: 0.6794, ppl: 1.0000002800000392, progress_or_epoch: 0.48\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:35,963] [    INFO]\u001b[0m - loss: 2.7e-07, learning_rate: 7.384e-06, global_step: 49, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4715, interval_samples_per_second: 5.4368, interval_steps_per_second: 0.6796, ppl: 1.0000002700000366, progress_or_epoch: 0.49\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:37,432] [    INFO]\u001b[0m - loss: 2.6e-07, learning_rate: 7.222e-06, global_step: 50, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4693, interval_samples_per_second: 5.4447, interval_steps_per_second: 0.6806, ppl: 1.0000002600000337, progress_or_epoch: 0.5\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:38,900] [    INFO]\u001b[0m - loss: 2.4e-07, learning_rate: 7.058e-06, global_step: 51, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4684, interval_samples_per_second: 5.4483, interval_steps_per_second: 0.681, ppl: 1.0000002400000287, progress_or_epoch: 0.51\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:40,368] [    INFO]\u001b[0m - loss: 2.3e-07, learning_rate: 6.891e-06, global_step: 52, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4676, interval_samples_per_second: 5.4512, interval_steps_per_second: 0.6814, ppl: 1.0000002300000264, progress_or_epoch: 0.52\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:41,836] [    INFO]\u001b[0m - loss: 2.2e-07, learning_rate: 6.721e-06, global_step: 53, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4681, interval_samples_per_second: 5.4492, interval_steps_per_second: 0.6812, ppl: 1.0000002200000242, progress_or_epoch: 0.53\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:43,301] [    INFO]\u001b[0m - loss: 2e-07, learning_rate: 6.551e-06, global_step: 54, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4649, interval_samples_per_second: 5.461, interval_steps_per_second: 0.6826, ppl: 1.00000020000002, progress_or_epoch: 0.54\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:44,770] [    INFO]\u001b[0m - loss: 1.9e-07, learning_rate: 6.378e-06, global_step: 55, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4692, interval_samples_per_second: 5.4451, interval_steps_per_second: 0.6806, ppl: 1.000000190000018, progress_or_epoch: 0.55\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:46,237] [    INFO]\u001b[0m - loss: 1.8e-07, learning_rate: 6.204e-06, global_step: 56, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4669, interval_samples_per_second: 5.4536, interval_steps_per_second: 0.6817, ppl: 1.0000001800000162, progress_or_epoch: 0.56\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:47,705] [    INFO]\u001b[0m - loss: 1.7e-07, learning_rate: 6.029e-06, global_step: 57, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4678, interval_samples_per_second: 5.4502, interval_steps_per_second: 0.6813, ppl: 1.0000001700000145, progress_or_epoch: 0.57\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:49,173] [    INFO]\u001b[0m - loss: 1.6e-07, learning_rate: 5.853e-06, global_step: 58, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4678, interval_samples_per_second: 5.4503, interval_steps_per_second: 0.6813, ppl: 1.0000001600000128, progress_or_epoch: 0.58\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:50,651] [    INFO]\u001b[0m - loss: 1.6e-07, learning_rate: 5.677e-06, global_step: 59, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4786, interval_samples_per_second: 5.4106, interval_steps_per_second: 0.6763, ppl: 1.0000001600000128, progress_or_epoch: 0.59\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:52,118] [    INFO]\u001b[0m - loss: 1.5e-07, learning_rate: 5.5e-06, global_step: 60, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4666, interval_samples_per_second: 5.4548, interval_steps_per_second: 0.6818, ppl: 1.0000001500000113, progress_or_epoch: 0.6\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:53,744] [    INFO]\u001b[0m - loss: 1.4e-07, learning_rate: 5.323e-06, global_step: 61, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6264, interval_samples_per_second: 4.9188, interval_steps_per_second: 0.6148, ppl: 1.0000001400000098, progress_or_epoch: 0.61\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:55,217] [    INFO]\u001b[0m - loss: 1.3e-07, learning_rate: 5.147e-06, global_step: 62, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4722, interval_samples_per_second: 5.4341, interval_steps_per_second: 0.6793, ppl: 1.0000001300000085, progress_or_epoch: 0.62\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:56,688] [    INFO]\u001b[0m - loss: 1.3e-07, learning_rate: 4.971e-06, global_step: 63, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4718, interval_samples_per_second: 5.4356, interval_steps_per_second: 0.6794, ppl: 1.0000001300000085, progress_or_epoch: 0.63\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:58,160] [    INFO]\u001b[0m - loss: 1.3e-07, learning_rate: 4.796e-06, global_step: 64, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4713, interval_samples_per_second: 5.4374, interval_steps_per_second: 0.6797, ppl: 1.0000001300000085, progress_or_epoch: 0.64\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:45:59,637] [    INFO]\u001b[0m - loss: 1.2e-07, learning_rate: 4.622e-06, global_step: 65, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4777, interval_samples_per_second: 5.4139, interval_steps_per_second: 0.6767, ppl: 1.0000001200000073, progress_or_epoch: 0.65\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:01,101] [    INFO]\u001b[0m - loss: 1.1e-07, learning_rate: 4.449e-06, global_step: 66, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4638, interval_samples_per_second: 5.4653, interval_steps_per_second: 0.6832, ppl: 1.000000110000006, progress_or_epoch: 0.66\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:02,574] [    INFO]\u001b[0m - loss: 1.2e-07, learning_rate: 4.279e-06, global_step: 67, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4729, interval_samples_per_second: 5.4314, interval_steps_per_second: 0.6789, ppl: 1.0000001200000073, progress_or_epoch: 0.67\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:04,191] [    INFO]\u001b[0m - loss: 1.1e-07, learning_rate: 4.109e-06, global_step: 68, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6164, interval_samples_per_second: 4.9493, interval_steps_per_second: 0.6187, ppl: 1.000000110000006, progress_or_epoch: 0.68\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:05,656] [    INFO]\u001b[0m - loss: 1.1e-07, learning_rate: 3.942e-06, global_step: 69, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4655, interval_samples_per_second: 5.459, interval_steps_per_second: 0.6824, ppl: 1.000000110000006, progress_or_epoch: 0.69\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:07,123] [    INFO]\u001b[0m - loss: 1.1e-07, learning_rate: 3.778e-06, global_step: 70, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4669, interval_samples_per_second: 5.4535, interval_steps_per_second: 0.6817, ppl: 1.000000110000006, progress_or_epoch: 0.7\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:08,594] [    INFO]\u001b[0m - loss: 1e-07, learning_rate: 3.616e-06, global_step: 71, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.471, interval_samples_per_second: 5.4385, interval_steps_per_second: 0.6798, ppl: 1.000000100000005, progress_or_epoch: 0.71\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:10,068] [    INFO]\u001b[0m - loss: 1e-07, learning_rate: 3.457e-06, global_step: 72, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4738, interval_samples_per_second: 5.428, interval_steps_per_second: 0.6785, ppl: 1.000000100000005, progress_or_epoch: 0.72\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:11,640] [    INFO]\u001b[0m - loss: 1e-07, learning_rate: 3.301e-06, global_step: 73, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.5718, interval_samples_per_second: 5.0896, interval_steps_per_second: 0.6362, ppl: 1.000000100000005, progress_or_epoch: 0.73\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:13,103] [    INFO]\u001b[0m - loss: 1e-07, learning_rate: 3.149e-06, global_step: 74, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4632, interval_samples_per_second: 5.4675, interval_steps_per_second: 0.6834, ppl: 1.000000100000005, progress_or_epoch: 0.74\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:14,738] [    INFO]\u001b[0m - loss: 9e-08, learning_rate: 3e-06, global_step: 75, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6352, interval_samples_per_second: 4.8923, interval_steps_per_second: 0.6115, ppl: 1.0000000900000041, progress_or_epoch: 0.75\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:16,211] [    INFO]\u001b[0m - loss: 9e-08, learning_rate: 2.855e-06, global_step: 76, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4733, interval_samples_per_second: 5.43, interval_steps_per_second: 0.6788, ppl: 1.0000000900000041, progress_or_epoch: 0.76\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:17,682] [    INFO]\u001b[0m - loss: 9e-08, learning_rate: 2.714e-06, global_step: 77, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4703, interval_samples_per_second: 5.4412, interval_steps_per_second: 0.6801, ppl: 1.0000000900000041, progress_or_epoch: 0.77\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:19,144] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 2.577e-06, global_step: 78, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.462, interval_samples_per_second: 5.472, interval_steps_per_second: 0.684, ppl: 1.0000000800000033, progress_or_epoch: 0.78\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:20,608] [    INFO]\u001b[0m - loss: 9e-08, learning_rate: 2.445e-06, global_step: 79, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4647, interval_samples_per_second: 5.462, interval_steps_per_second: 0.6827, ppl: 1.0000000900000041, progress_or_epoch: 0.79\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:22,088] [    INFO]\u001b[0m - loss: 9e-08, learning_rate: 2.318e-06, global_step: 80, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4794, interval_samples_per_second: 5.4078, interval_steps_per_second: 0.676, ppl: 1.0000000900000041, progress_or_epoch: 0.8\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:23,561] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 2.196e-06, global_step: 81, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4732, interval_samples_per_second: 5.4305, interval_steps_per_second: 0.6788, ppl: 1.0000000800000033, progress_or_epoch: 0.81\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:25,039] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 2.078e-06, global_step: 82, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4782, interval_samples_per_second: 5.4119, interval_steps_per_second: 0.6765, ppl: 1.0000000800000033, progress_or_epoch: 0.82\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:26,503] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.966e-06, global_step: 83, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4644, interval_samples_per_second: 5.4631, interval_steps_per_second: 0.6829, ppl: 1.0000000800000033, progress_or_epoch: 0.83\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:27,973] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.859e-06, global_step: 84, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4695, interval_samples_per_second: 5.4441, interval_steps_per_second: 0.6805, ppl: 1.0000000800000033, progress_or_epoch: 0.84\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:29,438] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.758e-06, global_step: 85, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4652, interval_samples_per_second: 5.4599, interval_steps_per_second: 0.6825, ppl: 1.0000000800000033, progress_or_epoch: 0.85\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:30,907] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.663e-06, global_step: 86, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4687, interval_samples_per_second: 5.4471, interval_steps_per_second: 0.6809, ppl: 1.0000000800000033, progress_or_epoch: 0.86\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:32,374] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.574e-06, global_step: 87, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4668, interval_samples_per_second: 5.4542, interval_steps_per_second: 0.6818, ppl: 1.0000000800000033, progress_or_epoch: 0.87\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:33,836] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.49e-06, global_step: 88, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4624, interval_samples_per_second: 5.4704, interval_steps_per_second: 0.6838, ppl: 1.0000000800000033, progress_or_epoch: 0.88\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:35,303] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.413e-06, global_step: 89, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.467, interval_samples_per_second: 5.4532, interval_steps_per_second: 0.6816, ppl: 1.0000000800000033, progress_or_epoch: 0.89\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:36,913] [    INFO]\u001b[0m - loss: 8e-08, learning_rate: 1.343e-06, global_step: 90, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6096, interval_samples_per_second: 4.9702, interval_steps_per_second: 0.6213, ppl: 1.0000000800000033, progress_or_epoch: 0.9\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:38,379] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.278e-06, global_step: 91, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4659, interval_samples_per_second: 5.4573, interval_steps_per_second: 0.6822, ppl: 1.0000000700000025, progress_or_epoch: 0.91\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:39,853] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.22e-06, global_step: 92, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4744, interval_samples_per_second: 5.4261, interval_steps_per_second: 0.6783, ppl: 1.0000000700000025, progress_or_epoch: 0.92\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:41,323] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.169e-06, global_step: 93, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4703, interval_samples_per_second: 5.4412, interval_steps_per_second: 0.6802, ppl: 1.0000000700000025, progress_or_epoch: 0.93\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:42,789] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.124e-06, global_step: 94, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4653, interval_samples_per_second: 5.4595, interval_steps_per_second: 0.6824, ppl: 1.0000000700000025, progress_or_epoch: 0.94\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:44,256] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.086e-06, global_step: 95, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4675, interval_samples_per_second: 5.4514, interval_steps_per_second: 0.6814, ppl: 1.0000000700000025, progress_or_epoch: 0.95\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:45,716] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.055e-06, global_step: 96, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4599, interval_samples_per_second: 5.4797, interval_steps_per_second: 0.685, ppl: 1.0000000700000025, progress_or_epoch: 0.96\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:47,329] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.031e-06, global_step: 97, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.6133, interval_samples_per_second: 4.9589, interval_steps_per_second: 0.6199, ppl: 1.0000000700000025, progress_or_epoch: 0.97\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:48,791] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.014e-06, global_step: 98, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4619, interval_samples_per_second: 5.4722, interval_steps_per_second: 0.684, ppl: 1.0000000700000025, progress_or_epoch: 0.98\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:50,254] [    INFO]\u001b[0m - loss: 7e-08, learning_rate: 1.003e-06, global_step: 99, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4627, interval_samples_per_second: 5.4692, interval_steps_per_second: 0.6837, ppl: 1.0000000700000025, progress_or_epoch: 0.99\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:51,728] [    INFO]\u001b[0m - loss: 6e-08, learning_rate: 1e-06, global_step: 100, current_memory_allocated: 0.6719479560852051, current_memory_reserved: 15.57339072227478, max_memory_allocated: 10.471631526947021, max_memory_reserved: 15.57339072227478, interval_runtime: 1.4744, interval_samples_per_second: 5.4258, interval_steps_per_second: 0.6782, ppl: 1.0000000600000019, progress_or_epoch: 1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:51,729] [    INFO]\u001b[0m - \r\n",
      "Training completed. \r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:51,729] [    INFO]\u001b[0m - train_runtime: 154.6855, train_samples_per_second: 5.1718, train_steps_per_second: 0.6465, train_loss: 0.7850595176554402, progress_or_epoch: 1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:51,730] [    INFO]\u001b[0m - Saving model checkpoint to ./output\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:51,733] [    INFO]\u001b[0m - tokenizer config file saved in ./output/tokenizer_config.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:51,734] [    INFO]\u001b[0m - Special tokens file saved in ./output/special_tokens_map.json\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:51,734] [    INFO]\u001b[0m - added tokens file saved in ./output/added_tokens.json\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 15:46:51,745] [ WARNING]\u001b[0m - Asynchronous saving is not supported for single card environment currently.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,736] [    INFO]\u001b[0m - Configuration saved in ./output/config.json\u001b[0m\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:316: UserWarning: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration. This warning will be raised to an exception.\r\n",
      "\r\n",
      "Thrown during validation:\r\n",
      "using greedy search strategy. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-18 15:46:53,783] [    INFO]\u001b[0m - [timelog] model saving time: 2.05s (2025-07-18 15:46:53) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,783] [    INFO]\u001b[0m - ***** train metrics *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,783] [    INFO]\u001b[0m -   progress_or_epoch        =        1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,783] [    INFO]\u001b[0m -   train_loss               =     0.7851\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,783] [    INFO]\u001b[0m -   train_runtime            = 0:02:34.68\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,783] [    INFO]\u001b[0m -   train_samples_per_second =     5.1718\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,783] [    INFO]\u001b[0m -   train_steps_per_second   =     0.6465\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,784] [    INFO]\u001b[0m - ***** Running Evaluation *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,784] [    INFO]\u001b[0m -   Num examples: Unknown\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,784] [    INFO]\u001b[0m -   Total prediction steps = 10000\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,784] [    INFO]\u001b[0m -   Pre device batch size = 1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,784] [    INFO]\u001b[0m -   Total Batch size = 1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,786] [    INFO]\u001b[0m - prepare SequenceDataset ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,786] [    INFO]\u001b[0m - prepare SequenceDataset done: total number of examples is 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,841] [    INFO]\u001b[0m - eval_loss: 0.09614189714193344, eval_runtime: 0.0576, eval_samples_per_second: 173468.878, eval_steps_per_second: 173468.878, eval_ppl: 1.1009152696414224, progress_or_epoch: 1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m - ***** eval metrics *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m -   eval_loss               =     0.0961\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m -   eval_ppl                =     1.1009\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m -   eval_runtime            = 0:00:00.05\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m -   eval_samples_per_second = 173468.878\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m -   eval_steps_per_second   = 173468.878\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m -   progress_or_epoch       =        1.0\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 15:46:53,842] [    INFO]\u001b[0m - Saving stop info into ./vdl_log/stop_step.json\u001b[0m\r\n",
      "LAUNCH INFO 2025-07-18 15:46:56,395 Pod completed\r\n",
      "LAUNCH INFO 2025-07-18 15:46:56,396 Exit code 0\r\n"
     ]
    }
   ],
   "source": [
    "!erniekit train ERNIE-develop/examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERNIEkit会自动加载模型和数据，并根据配置文件中的参数进行训练。训练过程中，会自动打印日志，显示当前的训练步数、损失、学习率等信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 训练过程解读\n",
    "\n",
    "训练日志中，我们可以关注以下几个关键指标：\n",
    "\n",
    "- **loss**: 损失值，反映了模型预测值与真实值之间的差距，这个值越小越好。\n",
    "- **learning_rate**: 学习率，表示模型参数更新的幅度。\n",
    "- **eval_loss**: 评估集上的损失值，可以用来判断模型是否过拟合。\n",
    "- **eval_accuracy**: 评估集上的准确率，直观地反映了模型在情感分析任务上的性能。\n",
    "\n",
    "训练完成后，模型会自动保存在 `output_dir` 指定的目录中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型评估与预测\n",
    "\n",
    "训练完成后，我们需要评估模型的性能，并使用它来预测新的文本。ERNIEkit提供了方便的工具来完成这些任务。\n",
    "\n",
    "### 6.1 模型评估：定量分析\n",
    "\n",
    "在SFT（有监督微调）中，当我们将任务转化为一个生成任务后（即模型生成“正面情感”或“负面情感”文本），评估的核心变成了**衡量模型生成的文本与我们期望的标签文本是否一致**。\n",
    "我们可以使用 `erniekit eval` 命令来完成这个过程。这个命令会在评估数据集上运行模型，并生成一个包含模型输出的预测文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T08:24:53.730475Z",
     "iopub.status.busy": "2025-07-18T08:24:53.730152Z",
     "iopub.status.idle": "2025-07-18T08:25:06.501120Z",
     "shell.execute_reply": "2025-07-18T08:25:06.500536Z",
     "shell.execute_reply.started": "2025-07-18T08:24:53.730455Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 -----------  Configuration  ----------------------\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 auto_cluster_config: 0\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 auto_parallel_config: None\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 auto_tuner_json: None\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 devices: 0\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 elastic_level: -1\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 elastic_timeout: 30\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 enable_gpu_log: True\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 gloo_port: 6767\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 host: None\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 ips: None\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 job_id: default\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 legacy: False\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 log_dir: erniekit_dist_log\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 log_level: INFO\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 log_overwrite: False\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 master: 127.0.0.1:8080\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 max_restart: 3\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 nnodes: 1\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 nproc_per_node: None\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 rank: -1\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,631 run_mode: collective\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 server_num: None\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 servers: \r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 sort_ip: False\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 start_port: 6070\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 trainer_num: None\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 trainers: \r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 training_script: /home/ERNIEKit/erniekit/launcher.py\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 training_script_args: ['eval', 'ERNIE-develop/examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml']\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 with_gloo: 1\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 --------------------------------------------------\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,632 Job: default, mode collective, replicas 1[1:1], elastic False\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,634 Run Pod: dyjrny, replicas 1, status ready\r\n",
      "LAUNCH INFO 2025-07-18 16:24:59,645 Watching Pod: dyjrny, replicas 1, status running\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "\u001b[32m[2025-07-18 16:25:01,881] [    INFO]\u001b[0m - user has defined resume_from_checkpoint: None\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:01,881] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:01,882] [    INFO]\u001b[0m - Tensor_parallel_degree = 1. Set sequence_parallel to False.\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m -      Model Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - add_tail_layers               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - continue_training             : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - fine_tuning                   : Full\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - fuse_gate_detach_matmul       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - fuse_linear                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - fuse_rms_norm                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - fuse_rope                     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - fuse_softmax_mask             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - fuse_swiglu                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - lora                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - lora_alpha                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - lora_path                     : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - lora_plus_scale               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - lora_rank                     : 8\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - model_name_or_path            : ./data/models/30656/ERNIE-4.5-0.3B-Paddle\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,882] [   DEBUG]\u001b[0m - moe_aux_loss_lambda           : 1e-05\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - moe_group                     : dummy\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - moe_group_experts             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - moe_multimodal_dispatch_use_allgather: v2-alltoall-unpad\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - moe_orthogonal_loss_lambda    : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - moe_use_aux_free              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - moe_use_hard_gate             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - moe_z_loss_lambda             : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - no_recompute_layers           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - offload_recompute_inputs      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - pp_seg_method                 : layer:Ernie4_5_DecoderLayer|EmptyLayer\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - recompute_granularity         : full\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - recompute_use_reentrant       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - rslora                        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - rslora_plus                   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - stage                         : SFT\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - tensor_parallel_output        : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - use_attn_mask_start_row_indices: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - use_flash_attention           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - use_fused_head_and_loss_fn    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - use_recompute_moe             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - use_sparse_flash_attn         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,883] [   DEBUG]\u001b[0m - use_sparse_head_and_loss_fn   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - virtual_pp_degree             : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m -       Data Configuration Arguments      \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - buffer_size                   : 500\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - dataset_type                  : iterable\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - eval_dataset_path             : ./sft-eval.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - eval_dataset_prob             : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - eval_dataset_type             : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - greedy_intokens               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - in_tokens_batching            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - mask_out_eos_token            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - max_prompt_len                : 2048\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - max_seq_len                   : 8192\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - num_comparisons               : 6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - num_samples_each_epoch        : 6000000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - offline_dataset_path          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - random_shuffle                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - train_dataset_path            : ./sft-train.jsonl\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - train_dataset_prob            : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,884] [   DEBUG]\u001b[0m - train_dataset_type            : erniekit\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,885] [   DEBUG]\u001b[0m - use_cls                       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:01,885] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:01,885] [    INFO]\u001b[0m - The global seed is set to 23, local seed is set to 24 and random seed is set to 23.\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 16:25:01,885] [ WARNING]\u001b[0m - Process rank: -1, device: gpu, world_size: 1, distributed training: False, 16-bits training: True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:01,885] [    INFO]\u001b[0m - Start to load model ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,035] [    INFO]\u001b[0m - Loading configuration file ./data/models/30656/ERNIE-4.5-0.3B-Paddle/config.json\u001b[0m\r\n",
      "\u001b[33m[2025-07-18 16:25:02,038] [ WARNING]\u001b[0m - You are using a model of type ernie4_5 to instantiate a model of type ernie4_5_moe. This is not supported for all configurations of models and can yield errors.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,038] [    INFO]\u001b[0m - Loading weights file ./data/models/30656/ERNIE-4.5-0.3B-Paddle/model.safetensors\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,342] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,342] [    INFO]\u001b[0m - change initializer-range from 0.02 to 0.018041293779826325\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,342] [    INFO]\u001b[0m - using moe-group: dummy\u001b[0m\r\n",
      "W0718 16:25:02.347047 166592 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.8, Runtime API Version: 12.6\r\n",
      "\u001b[32m[2025-07-18 16:25:02,348] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,352] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,354] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,356] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,359] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,361] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,363] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,365] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,368] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,370] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,372] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,375] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,377] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,380] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,382] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,439] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,442] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,445] [    INFO]\u001b[0m - use GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,449] [    INFO]\u001b[0m - output-weight:[103424, 1024] config.tie_word_embeddings=True\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,449] [    INFO]\u001b[0m - Use fusedRMSNorm\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,528] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing Ernie4_5_MoeForCausalLM.\r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,528] [    INFO]\u001b[0m - All the weights of Ernie4_5_MoeForCausalLM were initialized from the model checkpoint at ./data/models/30656/ERNIE-4.5-0.3B-Paddle.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Ernie4_5_MoeForCausalLM for predictions without further training.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,530] [    INFO]\u001b[0m - Loading configuration file ./data/models/30656/ERNIE-4.5-0.3B-Paddle/generation_config.json\u001b[0m\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:250: UserWarning: using greedy search strategy. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddleformers/generation/configuration_utils.py:255: UserWarning: using greedy search strategy. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `decode_strategy=\"greedy_search\" ` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-18 16:25:02,540] [    INFO]\u001b[0m - Loading model successfully !\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:02,541] [   DEBUG]\u001b[0m - Model config: Ernie4_5_MoeConfig {\r\n",
      "  \"add_tail_layers\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"Ernie4_5_ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.0,\r\n",
      "  \"bos_token_id\": 1,\r\n",
      "  \"cachekv_quant\": false,\r\n",
      "  \"compression_ratio\": 1.0,\r\n",
      "  \"dpo_config\": null,\r\n",
      "  \"dtype\": \"bfloat16\",\r\n",
      "  \"enable_delay_scale_loss\": true,\r\n",
      "  \"enable_mtp_magic_send\": false,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"fuse_gate_detach_matmul\": true,\r\n",
      "  \"fuse_linear\": false,\r\n",
      "  \"fuse_ln\": false,\r\n",
      "  \"fuse_rms_norm\": true,\r\n",
      "  \"fuse_rope\": true,\r\n",
      "  \"fuse_softmax_mask\": false,\r\n",
      "  \"fuse_swiglu\": true,\r\n",
      "  \"global_aux_loss\": false,\r\n",
      "  \"head_dim\": 128,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_dropout_prob\": 0.0,\r\n",
      "  \"hidden_size\": 1024,\r\n",
      "  \"ignored_index\": -100,\r\n",
      "  \"initializer_range\": 0.018041293779826325,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"max_position_embeddings\": 131072,\r\n",
      "  \"max_sequence_length\": 8192,\r\n",
      "  \"micro_batch_size\": -1,\r\n",
      "  \"model_type\": \"ernie4_5_moe\",\r\n",
      "  \"moe_all_to_all_dropout\": 0.0,\r\n",
      "  \"moe_aux_loss_lambda\": 1e-05,\r\n",
      "  \"moe_capacity\": [],\r\n",
      "  \"moe_dense_experts_token_type_id\": 3,\r\n",
      "  \"moe_dropout_prob\": 0.0,\r\n",
      "  \"moe_gate\": \"topk\",\r\n",
      "  \"moe_gate_act\": \"softmax\",\r\n",
      "  \"moe_group_experts\": false,\r\n",
      "  \"moe_group_origin\": \"dummy\",\r\n",
      "  \"moe_group_orthogonal_loss\": true,\r\n",
      "  \"moe_intermediate_size\": 0,\r\n",
      "  \"moe_k\": 2,\r\n",
      "  \"moe_layer_end_index\": 17,\r\n",
      "  \"moe_layer_interval\": 2,\r\n",
      "  \"moe_layer_start_index\": 0,\r\n",
      "  \"moe_multimodal_dispatch_use_allgather\": \"v2-alltoall-unpad\",\r\n",
      "  \"moe_norm_gate_logits\": true,\r\n",
      "  \"moe_num_experts\": null,\r\n",
      "  \"moe_num_shared_experts\": 0,\r\n",
      "  \"moe_orthogonal_loss_lambda\": 0.0,\r\n",
      "  \"moe_rank\": 0,\r\n",
      "  \"moe_reverse_token_drop\": false,\r\n",
      "  \"moe_use_aux_free\": false,\r\n",
      "  \"moe_use_hard_gate\": false,\r\n",
      "  \"moe_world_size\": 1,\r\n",
      "  \"moe_z_loss_lambda\": 0.0,\r\n",
      "  \"multi_token_pred_lambda\": 0.3,\r\n",
      "  \"num_acc_steps\": 8,\r\n",
      "  \"num_attention_heads\": 16,\r\n",
      "  \"num_hidden_layers\": 18,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"num_nextn_predict_layers\": 0,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"paddleformers_version\": \"0.1\",\r\n",
      "  \"pp_seg_method\": \"layer:Ernie4_5_DecoderLayer|EmptyLayer\",\r\n",
      "  \"recompute_granularity\": \"full\",\r\n",
      "  \"recompute_use_reentrant\": true,\r\n",
      "  \"refined_recompute\": {},\r\n",
      "  \"rms_norm_eps\": 1e-05,\r\n",
      "  \"rope_theta\": 500000,\r\n",
      "  \"sinkhorn_2gate\": true,\r\n",
      "  \"sinkhorn_temp\": 0.03,\r\n",
      "  \"skip_recompute_ops\": {},\r\n",
      "  \"tensor_parallel_degree\": -1,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"token_balance_loss\": false,\r\n",
      "  \"token_balance_seqlen\": false,\r\n",
      "  \"use_bias\": false,\r\n",
      "  \"use_flash_attention\": true,\r\n",
      "  \"use_fused_head_and_loss_fn\": false,\r\n",
      "  \"use_recompute_lm_head\": false,\r\n",
      "  \"use_recompute_loss_fn\": false,\r\n",
      "  \"use_recompute_moe\": false,\r\n",
      "  \"use_recompute_mtp\": false,\r\n",
      "  \"use_rmsnorm\": true,\r\n",
      "  \"use_sparse_flash_attn\": true,\r\n",
      "  \"use_sparse_head_and_loss_fn\": true,\r\n",
      "  \"use_var_len_flash_attn\": false,\r\n",
      "  \"vocab_size\": 103424,\r\n",
      "  \"weight_share_add_bias\": true\r\n",
      "}\r\n",
      "\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:02,541] [    INFO]\u001b[0m - [timelog] basemodel loading time: 0.51s (2025-07-18 16:25:02) \u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,590] [    INFO]\u001b[0m - Start to create dataset ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,592] [    INFO]\u001b[0m - Creating dataset successfully ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,592] [    INFO]\u001b[0m - The global seed is set to 23, local seed is set to 24 and random seed is set to 23.\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,630] [    INFO]\u001b[0m - max_steps is given, it will override any value given in num_train_epochs\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,630] [    INFO]\u001b[0m - Using half precision\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,644] [   DEBUG]\u001b[0m - ============================================================\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m -     Training Configuration Arguments    \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - paddle commit id              : cdcd76a93d1dfe767dde768198e1d7c7488b93f6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - paddleformers commit id       : 053a5a5e2a72cd2d3b997cbcff81b14af9fe4d47\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - _no_sync_in_gradient_accumulation: True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - actscale_moving_rate          : 0.01\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - adam_beta1                    : 0.9\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - adam_beta2                    : 0.95\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - adam_epsilon                  : 1e-08\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - amp_custom_black_list         : ['reduce_sum', 'softmax_with_cross_entropy', 'c_softmax_with_cross_entropy', 'elementwise_div', 'sin', 'cos']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - amp_custom_white_list         : ['lookup_table', 'lookup_table_v2', 'flash_attn', 'matmul', 'matmul_v2', 'fused_gemm_epilogue']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - amp_master_grad               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - apply_hadamard                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - apply_online_actscale_step    : 200\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - attention_probs_dropout_prob  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - auto_parallel_resume_form_hybrid_parallel: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,645] [   DEBUG]\u001b[0m - batch_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - beta                          : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - bf16                          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - bf16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - ckpt_quant_stage              : O0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - compute_type                  : bf16\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - context_parallel_degree       : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - count_trained_tokens          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - current_device                : gpu:0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - data_parallel_config          : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - data_parallel_degree          : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - data_parallel_rank            : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - dataloader_drop_last          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - dataloader_num_workers        : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - dataloader_shuffle            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - dataset_rank                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - dataset_world_size            : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - ddp_find_unused_parameters    : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - decay_steps                   : 100\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - device                        : gpu\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - disable_ckpt_quant            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - disable_tqdm                  : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,646] [   DEBUG]\u001b[0m - distributed_dataloader        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - do_eval                       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - do_export                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - do_predict                    : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - do_train                      : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - dpo_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - dpop_lambda                   : 50\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - dropout_warmup_steps          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - enable_auto_parallel          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - enable_zero_cost_checkpoint   : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - eval_accumulation_steps       : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - eval_batch_size               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - eval_steps                    : 10000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - evaluation_strategy           : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - expert_max_capacity           : 4294967296\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - expert_min_capacity           : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - expert_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - expert_tensor_parallel_degree : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - flash_device_save_steps       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - flatten_param_grads           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - force_reshard_pp              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - fp16                          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - fp16_full_eval                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,647] [   DEBUG]\u001b[0m - fp16_opt_level                : O2\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - fp8_format_type               : hybrid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - fuse_sequence_parallel_allreduce: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - gradient_accumulation_steps   : 8\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - greater_is_better             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - hadamard_block_size           : 32\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - hidden_dropout_prob           : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - hybrid_parallel_topo_order    : pp_first\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - ignore_data_skip              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - ignore_load_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - ignore_save_lr_and_optim      : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - label_names                   : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - label_smoothing               : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - layerwise_lr_decay_bound      : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - lazy_data_processing          : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - learning_rate                 : 1e-05\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - load_best_model_at_end        : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - load_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - local_process_index           : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - local_rank                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - log_level                     : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - log_level_replica             : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,648] [   DEBUG]\u001b[0m - log_on_each_node              : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - logging_dir                   : ./vdl_log\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - logging_first_step            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - logging_steps                 : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - logging_strategy              : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - logical_process_index         : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - loss_type                     : sigmoid\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - lr_end                        : 1e-07\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - lr_scheduler_type             : SchedulerType.COSINE\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - max_estimate_samples          : 100000.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - max_evaluate_steps            : 10000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - max_grad_norm                 : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - max_steps                     : 100\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - metric_for_best_model         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - metrics_output_path           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - min_lr                        : 1e-06\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - minimum_eval_times            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - multi_token_pred_lambda       : 0.3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - no_cuda                       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - normalize_logps               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - num_cycles                    : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - num_nextn_predict_layers      : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,649] [   DEBUG]\u001b[0m - num_of_gpus                   : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - num_train_epochs              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - offload_optim                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - offset_alpha                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - optim                         : OptimizerNames.ADAMW\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - optim_shard_num               : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - optimizer_name_suffix         : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - ordered_save_group_size       : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - output_dir                    : ./output\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - output_signal_dir             : ./output\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - overwrite_output_dir          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - pad_token_id                  : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - past_index                    : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - pdc_download_ckpt             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - pdc_download_timeout          : 300\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - per_device_eval_batch_size    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - per_device_train_batch_size   : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - pipeline_parallel_config      : enable_delay_scale_loss enable_release_grads disable_partial_send_recv\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - pipeline_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - pipeline_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - power                         : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - prediction_loss_only          : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - pref_loss_ratio               : 1.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,650] [   DEBUG]\u001b[0m - process_index                 : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - quant_input_grad              : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - quant_weight_grad             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - recompute                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - ref_model_update_steps        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - reference_free                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - refined_recompute             : {}\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - release_grads                 : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - remove_unused_columns         : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - report_to                     : ['visualdl']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - resume_from_checkpoint        : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - run_name                      : ./output\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_on_each_node             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_rng_states               : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_sharded_model            : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_sharding_stage1_model_include_freeze_params: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_steps                    : 10000000\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_strategy                 : IntervalStrategy.STEPS\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_tokenizer                : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - save_total_limit              : 5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - scale_loss                    : 32768\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - seed                          : 23\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - sep_parallel_degree           : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,651] [   DEBUG]\u001b[0m - sequence_parallel             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sequence_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - server_tp_degree              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sft_benchmark                 : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sft_loss_ratio                : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sharding                      : []\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sharding_comm_buffer_size_MB  : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sharding_degree               : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sharding_parallel_config      : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sharding_parallel_degree      : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sharding_parallel_mesh_dimension: dp\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - sharding_parallel_rank        : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - should_load_dataset           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - should_load_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - should_log                    : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - should_save                   : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - should_save_model_state       : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - should_save_model_with_tensor_fusion: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - should_save_sharding_stage1_model: False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - simpo_gamma                   : 0.5\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - skip_data_intervals           : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - skip_memory_metrics           : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - skip_profile_timer            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,652] [   DEBUG]\u001b[0m - split_inputs_sequence_dim     : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - split_norm_comm               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - tensor_parallel_config        : \u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - tensor_parallel_degree        : -1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - tensor_parallel_rank          : 0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - tensorwise_offload_optimizer  : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - to_static                     : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - train_batch_size              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - unified_checkpoint            : True\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - unified_checkpoint_config     : ['async_save']\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - use_async_save                : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - use_expert_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - use_hybrid_parallel           : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - use_lowprecision_moment       : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - use_recompute_mtp             : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - use_sp_callback               : False\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - wandb_api_key                 : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - wandb_http_proxy              : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - warmup_ratio                  : 0.0\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - warmup_steps                  : 20\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - weight_decay                  : 0.1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - weight_name_suffix            : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,653] [   DEBUG]\u001b[0m - weight_quantize_algo          : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,654] [   DEBUG]\u001b[0m - world_size                    : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,654] [   DEBUG]\u001b[0m - zcc_ema_interval              : 1\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,654] [   DEBUG]\u001b[0m - zcc_pipeline_hooks_capacity_usage: 0.6\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,654] [   DEBUG]\u001b[0m - zcc_save_ema_coef             : None\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,654] [   DEBUG]\u001b[0m - zcc_workers_num               : 3\u001b[0m\r\n",
      "\u001b[35m[2025-07-18 16:25:03,654] [   DEBUG]\u001b[0m - \u001b[0m\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/distributed/parallel.py:450: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[32m[2025-07-18 16:25:03,662] [    INFO]\u001b[0m - ***** Running Evaluation *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,662] [    INFO]\u001b[0m -   Num examples: Unknown\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,662] [    INFO]\u001b[0m -   Total prediction steps = 10000\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,662] [    INFO]\u001b[0m -   Pre device batch size = 1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,663] [    INFO]\u001b[0m -   Total Batch size = 1\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,664] [    INFO]\u001b[0m - prepare SequenceDataset ...\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:03,664] [    INFO]\u001b[0m - prepare SequenceDataset done: total number of examples is 2\u001b[0m\r\n",
      "W0718 16:25:04.358070 166592 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.\r\n",
      "\u001b[32m[2025-07-18 16:25:04,663] [    INFO]\u001b[0m - eval_loss: 9.515118598937988, eval_runtime: 1.0008, eval_samples_per_second: 9992.3098, eval_steps_per_second: 9992.3098, eval_ppl: 13563.241735167847\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:04,663] [    INFO]\u001b[0m - ***** eval metrics *****\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:04,663] [    INFO]\u001b[0m -   eval_loss               =     9.5151\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:04,664] [    INFO]\u001b[0m -   eval_ppl                = 13563.2417\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:04,664] [    INFO]\u001b[0m -   eval_runtime            = 0:00:01.00\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:04,664] [    INFO]\u001b[0m -   eval_samples_per_second =  9992.3098\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:04,664] [    INFO]\u001b[0m -   eval_steps_per_second   =  9992.3098\u001b[0m\r\n",
      "\u001b[32m[2025-07-18 16:25:04,664] [    INFO]\u001b[0m - Saving stop info into ./vdl_log/stop_step.json\u001b[0m\r\n",
      "LAUNCH INFO 2025-07-18 16:25:05,654 Pod completed\r\n",
      "LAUNCH INFO 2025-07-18 16:25:05,654 Exit code 0\r\n"
     ]
    }
   ],
   "source": [
    "!erniekit eval ERNIE-develop/examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重要提示：关于 `eval` 命令的日志输出**\n",
    "\n",
    "当您运行 `erniekit eval` 命令时，您会看到终端输出了大量的日志信息，类似于您在训练时看到的内容。**这是完全正常的行为**。因为 `erniekit eval` 本质上是启动了一个分布式的评估任务，它需要加载分布式环境、解析完整的模型和数据配置、加载模型权重等，这些过程都会被详细地记录下来。\n",
    "\n",
    "在日志输出中，您需要重点关注以下内容：\n",
    "\n",
    "1. **命令执行状态**：最终是否显示 `Exit code 0`，表示评估任务成功完成\n",
    "\n",
    "2. **关键评估指标**：\n",
    "   - `eval_loss`：评估损失值，反映模型预测的准确程度，值越小越好\n",
    "   - `eval_ppl`：困惑度，用于衡量生成任务的性能，值越小表示模型表现越好\n",
    "   - `eval_runtime`：评估耗时\n",
    "   - `eval_samples_per_second`：每秒处理样本数，反映评估速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 性能优化建议\n",
    "\n",
    "如果您在实际应用中需要更好的性能，可以参考以下优化方案表格。我们从数据、训练、模型和部署四个维度，提供了详细的优化建议和具体操作方法：\n",
    "\n",
    "#### 6.2.1 优化方案全景图\n",
    "\n",
    "| 优化维度 | 具体措施 | 操作建议 | 预期效果 | 适用场景 |\n",
    "|:---:|:---|:---|:---|:---|\n",
    "| 📊 **数据优化** | • 扩充数据规模<br>• 提升数据质量<br>• 领域数据增强 | • 使用数据增强技术<br>• 引入人工审核机制<br>• 收集目标领域数据 | • 提升模型泛化能力<br>• 降低过拟合风险<br>• 提高领域适应性 | • 数据量不足<br>• 标注质量参差<br>• 领域迁移场景 |\n",
    "| ⚙️ **训练优化** | • 学习率调优<br>• 批大小优化<br>• 训练策略改进 | • 使用学习率预热<br>• 尝试梯度累积<br>• 采用早停策略 | • 加快收敛速度<br>• 提升训练稳定性<br>• 节省计算资源 | • 训练不稳定<br>• 显存受限<br>• 过拟合严重 |\n",
    "| 🧠 **模型优化** | • 模型选型<br>• 量化加速<br>• 知识蒸馏 | • 尝试更大/小模型<br>• 使用INT8/BF16量化<br>• 应用教师-学生蒸馏 | • 平衡性能和效率<br>• 加速推理速度<br>• 减小模型体积 | • 性能瓶颈<br>• 推理延迟高<br>• 资源受限 |\n",
    "| 🚀 **部署优化** | • 文本处理加速<br>• 推理性能提升<br>• 批处理优化 | • 启用FastTokenizer<br>• 开启推理优化选项<br>• 实现请求批处理 | • 降低处理延迟<br>• 提升推理速度<br>• 增加系统吞吐 | • 在线服务<br>• 高并发场景<br>• 延迟敏感 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 总结与展望\n",
    "\n",
    "恭喜您完成了本次 ERNIE-4.5-0.3B 中文情感分析实战教程！\n",
    "\n",
    "在本教程中，我们从零开始，学习了如何使用 ERNIEkit 对 ERNIE-4.5-0.3B 模型进行监督微调，并将其应用于情感分析任务。我们经历了从环境准备、数据处理、模型微调到评估预测的完整流程，并深入探讨了 ERNIE-4.5-0.3B 模型的优势和微调原理。\n",
    "\n",
    "通过本教程，您应该已经掌握了以下核心技能：\n",
    "\n",
    "- **使用 ERNIEkit 进行模型训练**：学会了通过配置文件管理训练参数，并使用简单的命令启动训练、评估和预测。\n",
    "- **处理自定义数据集**：掌握了如何将自己的数据转换为 ERNIEkit 所需的格式，并进行有效的训练。\n",
    "- **理解和应用 SFT**：深入理解了监督微调的原理，并能将其应用于实际的 NLP 任务中。\n",
    "- **评估和优化模型**：学会了如何评估模型性能，并掌握了多种优化策略来提升模型效果。\n",
    "\n",
    "### 7.1 未来展望\n",
    "\n",
    "情感分析是一个不断发展的领域，随着技术的进步，我们可以探索更多有趣的方向：\n",
    "\n",
    "- **多模态情感分析**：结合文本、图像和声音，更全面地理解用户的情感。\n",
    "- **细粒度情感分析**：不仅识别情感的极性，还能分析情感的强度、原因和对象。\n",
    "- **跨领域情感分析**：将在一个领域训练好的模型，应用于另一个新的领域。\n",
    "- **小样本情感分析**：在数据量有限的情况下，如何有效地训练情感分析模型。\n",
    "\n",
    "ERNIE-4.5-0.3B 作为一个强大的基础模型，为这些前沿研究提供了坚实的基础。我们鼓励您在本次教程的基础上，继续探索和实践，将所学知识应用于更广阔的领域。\n",
    "\n",
    "感谢您的学习！如果您有任何问题或建议，欢迎随时与我们交流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题反馈/与我联系： Wechat：G_Fuji"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
