{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bcefd1",
   "metadata": {},
   "source": [
    "# Tutorial: Deploying a Large ERNIE Model Using Ollama\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial details how to quickly deploy an ERNIE-4.5-0.3B model using Ollama, allowing you to run and use the ERNIE large language model in a local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cb175",
   "metadata": {},
   "source": [
    "Tutorial Objectives\n",
    "\n",
    "After completing this tutorial, you will be able to:\n",
    "1. Install Ollama locally\n",
    "2. Deploy the ERNIE-4.5-0.3B model with one click\n",
    "3. Communicate with the model via the command line\n",
    "4. Call the model using the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335a49d",
   "metadata": {},
   "source": [
    "## System Requirements\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "- **Memory**: At least 8GB of RAM (16GB or higher recommended)\n",
    "- **Storage**: At least 10GB of available space\n",
    "- **Processor**: A modern CPU that supports the AVX instruction set\n",
    "- **GPU** (optional): An NVIDIA GPU with CUDA support (significantly improves inference speed)\n",
    "\n",
    "### Software Requirements\n",
    "\n",
    "- **Operating System**: Windows 10/11, macOS 10.15+, or Linux\n",
    "- **Python**: 3.10+ (for model conversion and API calls)\n",
    "- **Git**: For downloading model files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb893744",
   "metadata": {},
   "source": [
    "## Step 1: Install Ollama\n",
    "\n",
    "### Windows Installation\n",
    "\n",
    "1. **Download the Installation Package**\n",
    "\n",
    "Visit the Ollama official website and download the Windows installation package:\n",
    "\n",
    "https://ollama.com/download/OllamaSetup.exe\n",
    "\n",
    "2. **Run the Installer**\n",
    "\n",
    "Double-click the downloaded `OllamaSetup.exe` file and follow the installation wizard to complete the installation.\n",
    "\n",
    "3. **Verify the Installation**\n",
    "\n",
    "Open a command prompt or PowerShell and run:\n",
    "\n",
    "bash\n",
    "ollama --version\n",
    "\n",
    "If version information is displayed, the installation was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654fc52",
   "metadata": {},
   "source": [
    "### macOS Installation\n",
    "\n",
    "1. **Download the installation package**\n",
    "\n",
    "```bash\n",
    "# Download the dmg file\n",
    "curl -O https://ollama.com/download/Ollama.dmg\n",
    "```\n",
    "\n",
    "2. **Install the application**\n",
    "\n",
    "Double-click the dmg file and drag Ollama to the Applications folder.\n",
    "\n",
    "3. **Verify the installation**\n",
    "\n",
    "```bash\n",
    "ollama --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a75d5",
   "metadata": {},
   "source": [
    "### Linux Installation\n",
    "\n",
    "1. **One-click installation script**\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "2. **Start the service**\n",
    "\n",
    "```bash\n",
    "# Start the Ollama service\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e02ff",
   "metadata": {},
   "source": [
    "## Step 2: Deploy the ERNIE-4.5-0.3B model\n",
    "\n",
    "### One-click model deployment\n",
    "\n",
    "Use the following command to deploy the ERNIE-4.5-0.3B model directly from the Ollama model repository:\n",
    "\n",
    "```bash\n",
    "ollama run dengcao/ERNIE-4.5-0.3B-PT\n",
    "```\n",
    "\n",
    "This command automatically:\n",
    "\n",
    "1. Downloads the ERNIE-4.5-0.3B model\n",
    "\n",
    "2. Installs it to the local Ollama environment\n",
    "\n",
    "3. Launches the model and enters session mode\n",
    "\n",
    "> **Note**: The model file will be downloaded during the first run, which may take several minutes depending on your network speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dfa30a",
   "metadata": {},
   "source": [
    "## Step 3: Run and Test the Model\n",
    "\n",
    "### Command Line Dialogue\n",
    "\n",
    "1. **Start Conversation Mode**\n",
    "\n",
    "```bash\n",
    "ollama run dengcao/ERNIE-4.5-0.3B-PT\n",
    "```\n",
    "\n",
    "2. **Perform Conversation Test**\n",
    "\n",
    "```\n",
    ">>> Hello, please introduce yourself.\n",
    "\n",
    "Hello! I'm ERNIE, a large language model developed by Baidu. I have powerful Chinese language understanding and generation capabilities and can help you:\n",
    "\n",
    "- Answer various questions\n",
    "- Assist with writing articles, poems, and other content\n",
    "- Conduct daily conversations\n",
    "- Provide study and work advice\n",
    "- Translation and language processing tasks\n",
    "\n",
    "Is there anything I can help you with?\n",
    "\n",
    ">>> Please write a poem about spring.\n",
    "\n",
    "The spring breeze gently caresses the willow branches,\n",
    "\n",
    "Flowers bloom in full bloom, filling the room with fragrance.\n",
    "\n",
    "Swallows return, busy carrying mud,\n",
    "\n",
    "The hills are covered in lush greenery.\n",
    "\n",
    "The stream flows eastward,\n",
    "\n",
    "Butterflies flutter among the flowers.\n",
    "Everything is reborn, afresh,\n",
    "Spring is a beautiful time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94770f",
   "metadata": {},
   "source": [
    "3. **Exit the chat**\n",
    "\n",
    "Press Ctrl+C to exit chat mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e068d1",
   "metadata": {},
   "source": [
    "### Using the REST API\n",
    "\n",
    "1. **Start the Ollama service**\n",
    "\n",
    "```bash\n",
    "# Start the service in the background\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "The service runs by default at `http://localhost:11434`\n",
    "\n",
    "2. **API Call Example**\n",
    "\n",
    "**Generate text**:\n",
    "```bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "\"model\": \"dengcao/ERNIE-4.5-0.3B-PT\",\n",
    "\"prompt\": \"Please explain the development of artificial intelligence\",\n",
    "\"stream\": false\n",
    "}'\n",
    "```\n",
    "\n",
    "**Conversation mode**:\n",
    "```bash\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "\"model\": \"dengcao/ERNIE-4.5-0.3B-PT\",\n",
    "\"messages\": [\n",
    "{\n",
    "\"role\": \"user\",\n",
    "\"content\": \"What is machine learning?\"\n",
    "\n",
    "    }\n",
    "]\n",
    "}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd0d4e",
   "metadata": {},
   "source": [
    "### Using the Python SDK\n",
    "\n",
    "1. Installing the Python Client\n",
    "   \n",
    "   ```bash\n",
    "   pip install ollama\n",
    "   ```\n",
    "\n",
    "2. **Python代码示例**\n",
    "   \n",
    "   ```python\n",
    "   import ollama\n",
    "   \n",
    "   # Simple conversation\n",
    "   response = ollama.chat(model='dengcao/ERNIE-4.5-0.3B-PT', messages=[\n",
    "     {\n",
    "       'role': 'user',\n",
    "       'content': 'Please introduce the basic concepts of deep learning',\n",
    "     },\n",
    "   ])\n",
    "   print(response['message']['content'])\n",
    "   \n",
    "   Streaming Conversation\n",
    "   stream = ollama.chat(\n",
    "       model='dengcao/ERNIE-4.5-0.3B-PT',\n",
    "       messages=[{'role': 'user', 'content': 'Write a Python function to calculate the Fibonacci sequence'}],\n",
    "       stream=True,\n",
    "   )\n",
    "   \n",
    "   for chunk in stream:\n",
    "     print(chunk['message']['content'], end='', flush=True)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f5b7d",
   "metadata": {},
   "source": [
    "## Step 4: Integration and Application\n",
    "\n",
    "### Application Development Example\n",
    "\n",
    "1. **Simple Chatbot**\n",
    "   \n",
    "   ```python\n",
    "   import ollama\n",
    "   import streamlit as st\n",
    "   \n",
    "   st.title(\"ERNIE聊天机器人\")\n",
    "   \n",
    "   # Initialize chat history\n",
    "   if \"messages\" not in st.session_state:\n",
    "       st.session_state.messages = []\n",
    "   \n",
    "   # Display chat history\n",
    "   for message in st.session_state.messages:\n",
    "       with st.chat_message(message[\"role\"]):\n",
    "           st.markdown(message[\"content\"])\n",
    "   \n",
    "   # User input\n",
    "   if prompt := st.chat_input(\"Please enter your question\"):\n",
    "       # Add User Message\n",
    "       st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "       with st.chat_message(\"user\"):\n",
    "           st.markdown(prompt)\n",
    "   \n",
    "       # Generate a reply\n",
    "       with st.chat_message(\"assistant\"):\n",
    "           response = ollama.chat(\n",
    "               model='dengcao/ERNIE-4.5-0.3B-PT',\n",
    "               messages=st.session_state.messages\n",
    "           )\n",
    "           reply = response['message']['content']\n",
    "           st.markdown(reply)\n",
    "           st.session_state.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "   ```\n",
    "2. **Run the chatbot**\n",
    "   \n",
    "   ```bash\n",
    "   # Install dependencies\n",
    "   pip install streamlit\n",
    "   \n",
    "   # Run the application\n",
    "   streamlit run chatbot.py\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422a3f4",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Problems and Solutions\n",
    "\n",
    "1. **Model Load Failure**\n",
    "\n",
    "**Problem**: `Error: model not found`\n",
    "\n",
    "**Solution**:\n",
    "```bash\n",
    "# Check if the model exists\n",
    "ollama list\n",
    "\n",
    "# Re-download the model\n",
    "ollama run dengcao/ERNIE-4.5-0.3B-PT\n",
    "```\n",
    "\n",
    "2. **API Connection Failure**\n",
    "\n",
    "**Problem**: `Connection refused`\n",
    "\n",
    "**Solution**:\n",
    "```bash\n",
    "# Ensure the service is running\n",
    "ollama serve\n",
    "\n",
    "# Check if the port is occupied\n",
    "netstat -an | grep 11434\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb17d98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "Through this tutorial, you have learned:\n",
    "\n",
    "1. ✅ **Installing and Configuring Ollama**: Installing and configuring Ollama on different operating systems\n",
    "2. ✅ **Model Preparation**: Downloading the ERNIE-4.5-0.3B model\n",
    "3. ✅ **Application Integration**: Integrating the model into real-world applications\n",
    "4. ✅ **Troubleshooting**: Resolving common issues and debugging techniques\n",
    "\n",
    "### Contact Us\n",
    "\n",
    "If you encounter any issues or have any suggestions, please contact us through the following channels:\n",
    "\n",
    "- **GitHub Issues**: Submit issues in the project repository\n",
    "- **WeChat**: G_Fuji\n",
    "- **Community Forum**: Participate in technical discussions\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
