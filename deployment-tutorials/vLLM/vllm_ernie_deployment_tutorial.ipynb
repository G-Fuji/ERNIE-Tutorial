{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afb70ed",
   "metadata": {},
   "source": [
    "# vLLM Deployment Tutorial for ERNIE-4.5-0.3B\n",
    "\n",
    "This tutorial details how to use vLLM to deploy the Baidu ERNIE-4.5-0.3B model and achieve high-performance inference services. ERNIE-4.5 is Baidu's next-generation large language model, featuring excellent Chinese language understanding and generation capabilities. vLLM is a high-performance large language model inference engine designed for production environments, supporting high-throughput batch inference and online serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8937ef",
   "metadata": {},
   "source": [
    "## Environment Preparation\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "- **GPU**: RTX 4090 24GB recommended (the ERNIE-4.5-0.3B model actually only requires 2-4GB of video memory, but the RTX 4090 provides better inference performance)\n",
    "- **Memory**: At least 16GB of RAM (32GB recommended for better performance)\n",
    "- **Storage**: At least 10GB of free space (for model files and cache)\n",
    "- **Operating System**: Linux (Ubuntu 20.04+ recommended)\n",
    "\n",
    "### Software Requirements\n",
    "\n",
    "- **Python**: 3.9-3.12 (3.10 recommended)\n",
    "- **CUDA**: 11.8+ (if using a GPU)\n",
    "- **PyTorch**: 2.0+\n",
    "- **Network**: A stable internet connection (model download required for the first run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9e593-6af5-4a2e-89ee-dfa815f783c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "# Check Python version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45337a24-8d4c-40d9-99f3-9be2cba4edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 21 15:25:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 572.83         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "|  0%   46C    P8             20W /  450W |    2125MiB /  24564MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85395e3c-292d-405d-a5d7-1e4d81ba74b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.free [MiB]\n",
      "24564 MiB, 22029 MiB\n"
     ]
    }
   ],
   "source": [
    "# Check available video memory\n",
    "!nvidia-smi --query-gpu=memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a04e10-8505-4101-917d-cd6f47da4d68",
   "metadata": {},
   "source": [
    "## vLLM Installation\n",
    "\n",
    "When installing vLLM, pay special attention to version compatibility, as support for the ERNIE-4.5 model was added in a later version.\n",
    "\n",
    "### Creating a Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470817ff-a060-46da-8231-00ed3c80adb5",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Create a conda environment (Python 3.10 is recommended)\n",
    "conda create -n vllm-ernie python=3.10 -y\n",
    "conda activate vllm-ernie\n",
    "\n",
    "# Update pip to the latest version\n",
    "pip install --upgrade pip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdcc55-9694-4dc4-9711-396550550167",
   "metadata": {},
   "source": [
    "### Install vLLM\n",
    "\n",
    "```bash\n",
    "# Install vLLM (make sure to use the latest version to support the ERNIE model)\n",
    "pip install vllm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb4d3f-73c9-43c0-9474-e3ee770b619e",
   "metadata": {},
   "source": [
    "### Verify Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb15882-e531-4d95-982f-15d1e92da48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM version: 0.10.1\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "print(f\"vLLM version: {vllm.__version__}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7f671-bb08-4b20-8827-3997ba143866",
   "metadata": {},
   "source": [
    "### Common Installation Issues\n",
    "\n",
    "1. **CUDA Version Mismatch**: Ensure your CUDA version is compatible with your PyTorch version.\n",
    "2. **Insufficient Memory**: The installation process may require a significant amount of memory; it is recommended that you close other programs.\n",
    "3. **Network Issue**: If downloading is slow, you can use a domestic mirror.\n",
    "\n",
    "## SDK Call Examples\n",
    "\n",
    "### Basic Text Generation\n",
    "\n",
    "The following example demonstrates how to use the vLLM SDK for basic text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192a193-d9ba-441b-8a31-0502adbf6d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 15:44:09 [__init__.py:241] Automatically detected platform cuda.\n",
      "Loading ERNIE-4.5-0.3B model...\n",
      "INFO 08-21 15:44:10 [utils.py:326] non-default args: {'model': 'baidu/ERNIE-4.5-0.3B-PT', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 2048, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6c0df78643431b8f539e525bf497ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 15:44:16 [__init__.py:711] Resolved architecture: Ernie4_5ForCausalLM\n",
      "WARNING 08-21 15:44:16 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-21 15:44:16 [__init__.py:1750] Using max model len 2048\n",
      "INFO 08-21 15:44:18 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa7f0caf7d941eeb12a414689ea7c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81f7d8a005045a4b757409a8a0da4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf318d5f1ce44580b86b8006064270f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d532038238d45da9492ef0e8e4df5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffbe970f70243a4a1129f4791b6df3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704dea51d6304446ba7318efaa8dfae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab38842408a4adda319b828eefe470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/226 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:30 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:30 [core.py:74] Initializing a V1 LLM engine (v0.10.1) with config: model='baidu/ERNIE-4.5-0.3B-PT', speculative_config=None, tokenizer='baidu/ERNIE-4.5-0.3B-PT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=baidu/ERNIE-4.5-0.3B-PT, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m WARNING 08-21 15:44:33 [interface.py:389] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m WARNING 08-21 15:44:33 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [gpu_model_runner.py:1953] Starting to load model baidu/ERNIE-4.5-0.3B-PT...\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b68c93e50cc4b848617e387bcb888d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/722M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:05 [weight_utils.py:312] Time spent downloading weights for baidu/ERNIE-4.5-0.3B-PT: 587.481343 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:06 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b55637f98ae487f9233167488feeafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:06 [default_loader.py:262] Loading weights took 0.57 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:07 [gpu_model_runner.py:2007] Model loading took 0.7042 GiB and 589.396855 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:10 [backends.py:548] Using cache directory: /home/xigan/.cache/vllm/torch_compile_cache/5074a8da94/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:10 [backends.py:559] Dynamo bytecode transform time: 3.32 s\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:12 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:21 [backends.py:215] Compiling a graph for dynamic shape takes 10.89 s\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:26 [monitor.py:34] torch.compile takes 14.21 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:27 [gpu_worker.py:276] Available KV cache memory: 17.51 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:27 [kv_cache_utils.py:849] GPU KV cache size: 1,019,776 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:27 [kv_cache_utils.py:853] Maximum concurrency for 2,048 tokens per request: 497.94x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████| 67/67 [00:01<00:00, 64.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:29 [gpu_model_runner.py:2708] Graph capturing finished in 1 secs, took 0.81 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:29 [core.py:214] init engine (profile, create kv cache, warmup model) took 21.78 seconds\n",
      "INFO 08-21 15:54:30 [llm.py:298] Supported_tasks: ['generate']\n",
      "Model loading completed, took: 620.29 seconds\n",
      "\n",
      "Input prompt word: Please introduce the development history of artificial intelligence\n",
      "Generating reply...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad3ccbccf844ffabd5feb43cca952ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9486eb31baf949629debc5fa2b867489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                      | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated result: .\n",
      "<|SpecialToken:PPL|>\n",
      "**Phase 1: Embryonic Period (1950s-1970s)**\n",
      "- **Starting Stage**: In 1956, the US Navy's \"flying experimental aircraft,\" the Dornier 224 prototype, successfully completed its first aerial refueling mission. This event marked the transition of AI technology from theory to practice, laying the foundation for subsequent development.\n",
      "- **Early Applications**: In 1958, IBM developed the first logic-based computer program, \"Intelligent System,\" which analyzed vast amounts of data and performed complex algorithms to solve problems. This opened up the field of AI applications.\n",
      "- **Technical Breakthrough**: In 1963, \"The Bell Laboratories\" proposed the \"Rule-Based System\" theory, which explained the essence of AI as simulating human intelligent behavior patterns, laying the theoretical foundation for later deep learning models.\n",
      "**Phase 2: Explosive Period (1980s-2000s)**\n",
      "- **Machine Learning Emerges**: In the late 1970s, Stanford University professors Richard S. Miller and Paul S. Rosenblatt proposed the \"Bayesian Network\" theory, which combined probability theory and statistical methods, enabling machine learning to become a reality.\n",
      "- **Natural Language Processing (NLP)**: In 1970, Alan Turing published \"Computing Machinery and Intelligence,\" which proposed the Turing Test as a measure of an AI's ability to understand and respond to human language. This led to the development of NLP technologies.\n",
      "- **Large-scale Data Processing**: In 1986, IBM released the world's first large-scale data processing tool, the \"Batch Processing System\" (Rational Software Package), which enabled the rapid analysis and processing of massive data.\n",
      "- **Deep Learning Emerges**: In 1997, Deep Learning was formally incorporated into the standard specification, marking a significant change in the AI field, enabling machines to think and learn like humans.\n",
      "**Phase 3: Mature Period (2000s-present)**\n",
      "- **Reinforcement Learning (RL) enters practical use**: In the late 1990s, AlphaGo defeated the world chess champion, proving the practicality of reinforcement learning in the field of robotics. This breakthrough marked the actual application of AI in various complex scenarios, achieving new heights.\n",
      "- **Brain Science and Neuroscience Deepen Fusion**: Over the past few years, with the development of brain imaging technology, researchers have discovered that the brain has similar cognitive functions as humans, such as image recognition, language understanding, etc., opening up a new cross-disciplinary field of AI and neuroscience.\n",
      "- **Smart Home Popularization**: Smart home devices have started to be integrated into daily life, driving the combination of AI and Internet of Things (IoT) technologies.\n",
      "Generation time: 1.69 seconds\n",
      "Generated length: 923 characters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "print(\"Loading ERNIE-4.5-0.3B model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"baidu/ERNIE-4.5-0.3B-PT\",\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=2048,\n",
    "    dtype=\"float16\"\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Model loading completed, took: {load_time:.2f} seconds\")\n",
    "\n",
    "# Set sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,      # Controls the randomness of the generated data, 0.0-2.0, the higher the value, the more random it is\n",
    "    top_p=0.9,           # Controls the diversity of the generated data, 0.0-1.0, the higher the value, the more diverse it is\n",
    "    max_tokens=512,      # Controls the maximum length of the generated data\n",
    "    repetition_penalty=1.1  # Repeated penalties to avoid repeated generation\n",
    ")\n",
    "\n",
    "# Single prompt word generation\n",
    "prompt = \"Please introduce the development history of artificial intelligence\"\n",
    "print(f\"\\nInput prompt word: {prompt}\")\n",
    "print(\"Generating reply...\")\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "generate_time = time.time() - start_time\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(f\"\\nGenerated result: {generated_text}\")\n",
    "print(f\"Generation time: {generate_time:.2f} seconds\")\n",
    "print(f\"Generated length: {len(generated_text)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c4e5a-1a97-4504-9487-bc0df08cb50f",
   "metadata": {},
   "source": [
    "### Conversation mode generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a783b89-c193-44e2-9f7e-6c82fe7c2e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation prompt word:\n",
      "User: Hello, please introduce yourself\n",
      "Assistant: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbfdf2198a94a3e9c797e12f5b483d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1cb5f6d1f2498c9845ed1e50dc7a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                      | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation reply: Hello! I am a Baidu R&D AI intelligent assistant, named Wenxin Yiyan. My name has the words \"smart\" and \"language\" hidden in it, which means I have both language capabilities and a deep understanding of knowledge, information, and social issues. I am glad to help you.\n"
     ]
    }
   ],
   "source": [
    "# Conversation mode example\n",
    "def format_conversation(messages):\n",
    "    \"\"\"Format conversation messages into a format that the model can understand\"\"\"\n",
    "    conversation = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"user\":\n",
    "            conversation += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            conversation += f\"Assistant: {content}\\n\"\n",
    "        elif role == \"system\":\n",
    "            conversation += f\"System: {content}\\n\"\n",
    "    conversation += \"Assistant: \"  # Prompt the model to start replying\n",
    "    return conversation\n",
    "\n",
    "# Conversation example\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful AI assistant, please answer questions in Chinese.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, please introduce yourself\"}\n",
    "]\n",
    "\n",
    "conversation_prompt = format_conversation(messages)\n",
    "print(f\"Conversation prompt word:\\n{conversation_prompt}\")\n",
    "\n",
    "# Generate conversation reply\n",
    "outputs = llm.generate([conversation_prompt], sampling_params)\n",
    "response = outputs[0].outputs[0].text\n",
    "\n",
    "print(f\"\\nConversation reply: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2144e58-48e6-43d7-9593-9d6d79d20ee4",
   "metadata": {},
   "source": [
    "## Start vLLM service\n",
    "\n",
    "### Basic service startup\n",
    "\n",
    "```bash\n",
    "# Start vLLM server (basic configuration)\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code\n",
    "```\n",
    "\n",
    "### Command line operation\n",
    "\n",
    "#### 1. Check service status\n",
    "\n",
    "After starting the service, you can check if the service is running normally using the following command:\n",
    "\n",
    "```bash\n",
    "# Check model list\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "#### 2. Use Completions API\n",
    "\n",
    "```bash\n",
    "# Basic text completion\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"baidu/ERNIE-4.5-0.3B-PT\",\n",
    "        \"prompt\": \"The future development trends of artificial intelligence are\",\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.7\n",
    "    }'\n",
    "```\n",
    "\n",
    "#### 3. Use Chat Completions API\n",
    "\n",
    "```bash\n",
    "# Chat completions API\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"baidu/ERNIE-4.5-0.3B-PT\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Please introduce the Python programming language\"}\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.7\n",
    "    }'\n",
    "```\n",
    "\n",
    "#### 4. Advanced service startup options\n",
    "\n",
    "```bash\n",
    "# Start server (with API key authentication)\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code \\\n",
    "    --api-key your-api-key-here\n",
    "\n",
    "# Start server (specify GPU and memory configuration)\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code \\\n",
    "    --gpu-memory-utilization 0.8 \\\n",
    "    --max-model-len 4096\n",
    "\n",
    "# Start server (use ModelScope to download model)\n",
    "export VLLM_USE_MODELSCOPE=True\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code\n",
    "```\n",
    "\n",
    "#### 5. Service management commands\n",
    "\n",
    "```bash\n",
    "# View vLLM help information\n",
    "vllm serve --help\n",
    "\n",
    "# Check vLLM version\n",
    "vllm --version\n",
    "\n",
    "# Stop service (if running in foreground, use Ctrl+C)\n",
    "# If running in background, use the following command to find and stop the process:\n",
    "ps aux | grep vllm\n",
    "kill <process_id>\n",
    "```\n",
    "\n",
    "#### 6. Test connection\n",
    "\n",
    "```bash\n",
    "# Simple health check\n",
    "curl -f http://localhost:8000/health || echo \"Service not started\"\n",
    "\n",
    "# Test basic response\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"baidu/ERNIE-4.5-0.3B-PT\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"你好\"}],\n",
    "        \"max_tokens\": 50\n",
    "    }' | jq '.choices[0].message.content'\n",
    "```\n",
    "\n",
    "> **Note**:\n",
    "> - Ensure vLLM is installed: `pip install vllm`\n",
    "> - Service startup may take a few minutes to load the model\n",
    "> - It is recommended to use process management tools (e.g., systemd, supervisor) in production environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682a152-152a-45e0-971a-b7972f4c7020",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [vLLM official documentation](https://docs.vllm.ai/) <mcreference link=\"https://docs.vllm.ai/\" index=\"1\">1</mcreference>\n",
    "- [ERNIE-4.5-0.3B model page](https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT) <mcreference link=\"https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT\" index=\"2\">2</mcreference>\n",
    "- [ERNIE-4.5 official blog](https://yiyan.baidu.com/blog/posts/ernie4.5/) <mcreference link=\"https://yiyan.baidu.com/blog/posts/ernie4.5/\" index=\"1\">1</mcreference>\n",
    "- [vLLM ERNIE support PR](https://github.com/vllm-project/vllm/pull/20220) <mcreference link=\"https://github.com/vllm-project/vllm/pull/20220\" index=\"4\">4</mcreference>\n",
    "- [OpenAI API documentation](https://platform.openai.com/docs/api-reference)\n",
    "\n",
    "---\n",
    "\n",
    "*Note: vLLM support for ERNIE-4.5 models may require the latest version of vLLM. If you encounter compatibility issues, please check the vLLM version or refer to the official documentation for the latest information.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (paddle)",
   "language": "python",
   "name": "paddle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
