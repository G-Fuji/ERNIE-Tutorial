{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afb70ed",
   "metadata": {},
   "source": [
    "# vLLM部署ERNIE-4.5-0.3B教程\n",
    "\n",
    "本教程将详细介绍如何使用vLLM部署百度ERNIE-4.5-0.3B模型，实现高性能的推理服务。ERNIE-4.5是百度发布的新一代大语言模型，具有优秀的中文理解和生成能力。vLLM是一个高性能的大语言模型推理引擎，专为生产环境设计，支持高吞吐量的批量推理和在线服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8937ef",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "### 硬件要求\n",
    "\n",
    "- **GPU**: 推荐RTX 4090 24G（ERNIE-4.5-0.3B模型实际只需要2-4GB显存，但RTX 4090可以提供更好的推理性能）\n",
    "- **内存**: 至少16GB RAM（推荐32GB以获得更好的性能）\n",
    "- **存储**: 至少10GB可用空间（用于模型文件和缓存）\n",
    "- **操作系统**: Linux（推荐Ubuntu 20.04+）\n",
    "\n",
    "### 软件要求\n",
    "\n",
    "- **Python**: 3.9-3.12（推荐3.10）\n",
    "- **CUDA**: 11.8+（如果使用GPU）\n",
    "- **PyTorch**: 2.0+\n",
    "- **网络**: 稳定的网络连接（首次运行时需要下载模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce9e593-6af5-4a2e-89ee-dfa815f783c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "# 检查Python版本\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45337a24-8d4c-40d9-99f3-9be2cba4edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 21 15:25:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 572.83         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "|  0%   46C    P8             20W /  450W |    2125MiB /  24564MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 检查CUDA版本\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85395e3c-292d-405d-a5d7-1e4d81ba74b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.free [MiB]\n",
      "24564 MiB, 22029 MiB\n"
     ]
    }
   ],
   "source": [
    "# 检查可用显存\n",
    "!nvidia-smi --query-gpu=memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a04e10-8505-4101-917d-cd6f47da4d68",
   "metadata": {},
   "source": [
    "## vLLM安装\n",
    "\n",
    "vLLM的安装需要特别注意版本兼容性，因为ERNIE-4.5模型支持是在较新版本中添加的。\n",
    "\n",
    "### 创建虚拟环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470817ff-a060-46da-8231-00ed3c80adb5",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 创建conda环境（推荐使用Python 3.10）\n",
    "conda create -n vllm-ernie python=3.10 -y\n",
    "conda activate vllm-ernie\n",
    "\n",
    "# 更新pip到最新版本\n",
    "pip install --upgrade pip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdcc55-9694-4dc4-9711-396550550167",
   "metadata": {},
   "source": [
    "### 安装vLLM\n",
    "\n",
    "```bash\n",
    "# 安装vLLM（确保使用最新版本以支持ERNIE模型）\n",
    "pip install vllm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb4d3f-73c9-43c0-9474-e3ee770b619e",
   "metadata": {},
   "source": [
    "### 验证安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb15882-e531-4d95-982f-15d1e92da48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM version: 0.10.1\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "print(f\"vLLM version: {vllm.__version__}\")\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7f671-bb08-4b20-8827-3997ba143866",
   "metadata": {},
   "source": [
    "### 常见安装问题\n",
    "\n",
    "1. **CUDA版本不匹配**: 确保CUDA版本与PyTorch版本兼容\n",
    "2. **内存不足**: 安装过程中可能需要较多内存，建议关闭其他程序\n",
    "3. **网络问题**: 如果下载缓慢，可以使用国内镜像源\n",
    "\n",
    "## SDK调用示例\n",
    "\n",
    "### 基础文本生成\n",
    "\n",
    "以下示例展示了如何使用vLLM SDK进行基础的文本生成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7192a193-d9ba-441b-8a31-0502adbf6d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 15:44:09 [__init__.py:241] Automatically detected platform cuda.\n",
      "正在加载ERNIE-4.5-0.3B模型...\n",
      "INFO 08-21 15:44:10 [utils.py:326] non-default args: {'model': 'baidu/ERNIE-4.5-0.3B-PT', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 2048, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6c0df78643431b8f539e525bf497ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-21 15:44:16 [__init__.py:711] Resolved architecture: Ernie4_5ForCausalLM\n",
      "WARNING 08-21 15:44:16 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-21 15:44:16 [__init__.py:1750] Using max model len 2048\n",
      "INFO 08-21 15:44:18 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa7f0caf7d941eeb12a414689ea7c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81f7d8a005045a4b757409a8a0da4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf318d5f1ce44580b86b8006064270f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d532038238d45da9492ef0e8e4df5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffbe970f70243a4a1129f4791b6df3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704dea51d6304446ba7318efaa8dfae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab38842408a4adda319b828eefe470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/226 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:30 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:30 [core.py:74] Initializing a V1 LLM engine (v0.10.1) with config: model='baidu/ERNIE-4.5-0.3B-PT', speculative_config=None, tokenizer='baidu/ERNIE-4.5-0.3B-PT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=baidu/ERNIE-4.5-0.3B-PT, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m WARNING 08-21 15:44:33 [interface.py:389] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m WARNING 08-21 15:44:33 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [gpu_model_runner.py:1953] Starting to load model baidu/ERNIE-4.5-0.3B-PT...\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:33 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:44:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b68c93e50cc4b848617e387bcb888d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/722M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:05 [weight_utils.py:312] Time spent downloading weights for baidu/ERNIE-4.5-0.3B-PT: 587.481343 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:06 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b55637f98ae487f9233167488feeafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:06 [default_loader.py:262] Loading weights took 0.57 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:07 [gpu_model_runner.py:2007] Model loading took 0.7042 GiB and 589.396855 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:10 [backends.py:548] Using cache directory: /home/xigan/.cache/vllm/torch_compile_cache/5074a8da94/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:10 [backends.py:559] Dynamo bytecode transform time: 3.32 s\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:12 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:21 [backends.py:215] Compiling a graph for dynamic shape takes 10.89 s\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:26 [monitor.py:34] torch.compile takes 14.21 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:27 [gpu_worker.py:276] Available KV cache memory: 17.51 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:27 [kv_cache_utils.py:849] GPU KV cache size: 1,019,776 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:27 [kv_cache_utils.py:853] Maximum concurrency for 2,048 tokens per request: 497.94x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████| 67/67 [00:01<00:00, 64.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:29 [gpu_model_runner.py:2708] Graph capturing finished in 1 secs, took 0.81 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=2440)\u001b[0;0m INFO 08-21 15:54:29 [core.py:214] init engine (profile, create kv cache, warmup model) took 21.78 seconds\n",
      "INFO 08-21 15:54:30 [llm.py:298] Supported_tasks: ['generate']\n",
      "模型加载完成，耗时: 620.29秒\n",
      "\n",
      "输入提示词: 请介绍一下人工智能的发展历程\n",
      "正在生成回复...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad3ccbccf844ffabd5feb43cca952ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9486eb31baf949629debc5fa2b867489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                      | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "生成结果: 。\n",
      "<|SpecialToken:PPL|>\n",
      "**第一阶段：萌芽期（1950s-1970s）**\n",
      "- **起步阶段**：1956年，美国海军的“飞行实验机”Dornier 224原型机成功完成首次空中加油任务。这一事件标志着AI技术从理论走向实践，为后续发展奠定了基础。\n",
      "- **早期应用**：1958年，IBM公司开发出了首个基于逻辑的计算机程序——“智能系统”（IBM Intelligent System），它通过分析大量数据和执行复杂的算法来解决问题，开创了AI的应用领域。\n",
      "- **技术突破**：1963年，“贝尔实验室”的罗杰·格里菲斯提出“规则基达”理论，解释AI的本质是模拟人类智能的行为模式，为后来的深度学习等模型奠定了理论基础。\n",
      "**第二阶段：爆发期（1980s-2000s）**\n",
      "- **机器学习兴起**：1970年代末，斯坦福大学教授理查德·米勒等人提出了著名的“贝叶斯网络”，将概率论与统计方法相结合，使机器学习成为可能。\n",
      "- **自然语言处理（NLP）诞生**：1970年，艾伦·图灵发表《论计算机可理解性》，引发了对AI理解能力的巨大挑战，推动了NLP技术的快速发展。\n",
      "- **大规模数据处理**：1986年，IBM发布了世界上第一款大规模数据处理工具——“批处理系统”——IBM Rational Software Package，实现了海量数据的快速分析和处理。\n",
      "- **深度学习崛起**：1997年，Deep Learning被正式纳入标准规范，标志着AI领域的又一次重大变革，使得机器可以像人一样思考和学习。\n",
      "**第三阶段：成熟期（2000s至今）**\n",
      "- **强化学习（RL）进入实用化阶段**：20世纪90年代后期，阿尔法狗战胜了国际象棋冠军，证明了强化学习技术在机器人领域的应用前景。这一突破标志着AI在各种复杂场景中的实际应用达到了新的高度。\n",
      "- **脑科学与神经科学深入融合**：近年来，随着脑成像技术的发展，研究人员发现大脑具有类似于人类的认知功能，如图像识别、语言理解等，开启了AI与神经科学的深度交叉研究。\n",
      "- **智能家居普及**：智能家居设备开始融入日常生活，推动了AI与物联网技术的结合\n",
      "生成耗时: 1.69秒\n",
      "生成长度: 923字符\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# huggingface 镜像地址\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "print(\"正在加载ERNIE-4.5-0.3B模型...\")\n",
    "start_time = time.time()\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"baidu/ERNIE-4.5-0.3B-PT\",  # 使用PyTorch版本的ERNIE模型\n",
    "    trust_remote_code=True,           # 必须设置，允许执行自定义代码\n",
    "    gpu_memory_utilization=0.8,      # GPU内存使用率\n",
    "    max_model_len=2048,               # 最大序列长度\n",
    "    dtype=\"float16\"                   # 使用半精度以节省显存\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"模型加载完成，耗时: {load_time:.2f}秒\")\n",
    "\n",
    "# 设置采样参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,      # 控制生成的随机性，0.0-2.0，值越高越随机\n",
    "    top_p=0.9,           # 核采样，保留概率质量前90%的词汇\n",
    "    max_tokens=512,      # 最大生成长度\n",
    "    repetition_penalty=1.1  # 重复惩罚，避免重复生成\n",
    ")\n",
    "\n",
    "# 单个提示词生成\n",
    "prompt = \"请介绍一下人工智能的发展历程\"\n",
    "print(f\"\\n输入提示词: {prompt}\")\n",
    "print(\"正在生成回复...\")\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "generate_time = time.time() - start_time\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(f\"\\n生成结果: {generated_text}\")\n",
    "print(f\"生成耗时: {generate_time:.2f}秒\")\n",
    "print(f\"生成长度: {len(generated_text)}字符\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c4e5a-1a97-4504-9487-bc0df08cb50f",
   "metadata": {},
   "source": [
    "### 对话模式生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a783b89-c193-44e2-9f7e-6c82fe7c2e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对话提示词:\n",
      "用户: 你好，请介绍一下自己\n",
      "助手: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbfdf2198a94a3e9c797e12f5b483d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1cb5f6d1f2498c9845ed1e50dc7a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                      | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "对话回复: 您好！我是百度研发AI的智能助手，名叫文心一言。我的名字里藏着“智”和“语”，既代表语言能力又暗示着我对知识、信息和社会问题的深刻理解。很高兴能为您提供帮助。\n"
     ]
    }
   ],
   "source": [
    "# 对话模式示例\n",
    "def format_conversation(messages):\n",
    "    \"\"\"格式化对话消息为模型可理解的格式\"\"\"\n",
    "    conversation = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"user\":\n",
    "            conversation += f\"用户: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            conversation += f\"助手: {content}\\n\"\n",
    "        elif role == \"system\":\n",
    "            conversation += f\"系统: {content}\\n\"\n",
    "    conversation += \"助手: \"  # 提示模型开始回复\n",
    "    return conversation\n",
    "\n",
    "# 对话示例\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"你是一个有用的AI助手，请用中文回答问题。\"},\n",
    "    {\"role\": \"user\", \"content\": \"你好，请介绍一下自己\"}\n",
    "]\n",
    "\n",
    "conversation_prompt = format_conversation(messages)\n",
    "print(f\"对话提示词:\\n{conversation_prompt}\")\n",
    "\n",
    "# 生成对话回复\n",
    "outputs = llm.generate([conversation_prompt], sampling_params)\n",
    "response = outputs[0].outputs[0].text\n",
    "\n",
    "print(f\"\\n对话回复: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2144e58-48e6-43d7-9593-9d6d79d20ee4",
   "metadata": {},
   "source": [
    "## 启动vLLM服务\n",
    "\n",
    "### 基础服务启动\n",
    "\n",
    "```bash\n",
    "# 启动vLLM服务器（基础配置）\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code\n",
    "```\n",
    "\n",
    "### 命令行操作\n",
    "\n",
    "#### 1. 检查服务状态\n",
    "\n",
    "启动服务后，可以通过以下命令检查服务是否正常运行：\n",
    "\n",
    "```bash\n",
    "# 检查模型列表\n",
    "curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "#### 2. 使用Completions API\n",
    "\n",
    "```bash\n",
    "# 基础文本补全\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"baidu/ERNIE-4.5-0.3B-PT\",\n",
    "        \"prompt\": \"人工智能的未来发展趋势是\",\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.7\n",
    "    }'\n",
    "```\n",
    "\n",
    "#### 3. 使用Chat Completions API\n",
    "\n",
    "```bash\n",
    "# 聊天对话API\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"baidu/ERNIE-4.5-0.3B-PT\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个有用的AI助手。\"},\n",
    "            {\"role\": \"user\", \"content\": \"请介绍一下Python编程语言\"}\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.7\n",
    "    }'\n",
    "```\n",
    "\n",
    "#### 4. 高级服务启动选项\n",
    "\n",
    "```bash\n",
    "# 启动服务器（包含API密钥验证）\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code \\\n",
    "    --api-key your-api-key-here\n",
    "\n",
    "# 启动服务器（指定GPU和内存配置）\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code \\\n",
    "    --gpu-memory-utilization 0.8 \\\n",
    "    --max-model-len 4096\n",
    "\n",
    "# 启动服务器（使用ModelScope下载模型）\n",
    "export VLLM_USE_MODELSCOPE=True\n",
    "vllm serve baidu/ERNIE-4.5-0.3B-PT \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code\n",
    "```\n",
    "\n",
    "#### 5. 服务管理命令\n",
    "\n",
    "```bash\n",
    "# 查看vLLM帮助信息\n",
    "vllm serve --help\n",
    "\n",
    "# 检查vLLM版本\n",
    "vllm --version\n",
    "\n",
    "# 停止服务（如果在前台运行，使用Ctrl+C）\n",
    "# 如果在后台运行，可以使用以下命令查找并停止进程\n",
    "ps aux | grep vllm\n",
    "kill <process_id>\n",
    "```\n",
    "\n",
    "#### 6. 测试连接\n",
    "\n",
    "```bash\n",
    "# 简单的健康检查\n",
    "curl -f http://localhost:8000/health || echo \"服务未启动\"\n",
    "\n",
    "# 测试基础响应\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"baidu/ERNIE-4.5-0.3B-PT\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"你好\"}],\n",
    "        \"max_tokens\": 50\n",
    "    }' | jq '.choices[0].message.content'\n",
    "```\n",
    "\n",
    "> **注意**：\n",
    "> - 确保已安装vLLM：`pip install vllm`\n",
    "> - 服务启动可能需要几分钟时间来加载模型\n",
    "> - 建议在生产环境中使用进程管理工具（如systemd、supervisor等）来管理vLLM服务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682a152-152a-45e0-971a-b7972f4c7020",
   "metadata": {},
   "source": [
    "## 参考资源\n",
    "\n",
    "- [vLLM官方文档](https://docs.vllm.ai/) <mcreference link=\"https://docs.vllm.ai/\" index=\"1\">1</mcreference>\n",
    "- [ERNIE-4.5-0.3B模型页面](https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT) <mcreference link=\"https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT\" index=\"2\">2</mcreference>\n",
    "- [ERNIE-4.5官方博客](https://yiyan.baidu.com/blog/posts/ernie4.5/) <mcreference link=\"https://yiyan.baidu.com/blog/posts/ernie4.5/\" index=\"1\">1</mcreference>\n",
    "- [vLLM ERNIE支持PR](https://github.com/vllm-project/vllm/pull/20220) <mcreference link=\"https://github.com/vllm-project/vllm/pull/20220\" index=\"4\">4</mcreference>\n",
    "- [OpenAI API文档](https://platform.openai.com/docs/api-reference)\n",
    "\n",
    "---\n",
    "\n",
    "*注意：ERNIE-4.5模型的vLLM支持可能需要最新版本的vLLM。如果遇到兼容性问题，请检查vLLM版本或参考官方文档获取最新信息。*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (paddle)",
   "language": "python",
   "name": "paddle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
